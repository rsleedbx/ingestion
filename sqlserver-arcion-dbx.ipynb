{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a3e62d-a594-4d8e-b979-55f5d7162955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "Welcome to a demo of snapshot and real time replication to Databricks.\n",
    "\n",
    "Use this notebook customized schema, data, workload, and **legacy** Arcion.\n",
    "\n",
    "**NOTE**: **Databricks Personal Access Token** and **Arcion License** are required. \n",
    "\n",
    "- Initial Setup\n",
    "  - Open `Table of Contents` (Outline)\n",
    "  - Enter `Arcion License`\n",
    "  - Enter `Databricks Personal Access Token`\n",
    "  - Click `Run All`\n",
    "  - Click `View` -> `Results Only`\n",
    "  - Click `View` -> `Web Terminal`, \n",
    "    - enter `tmux attach`.  \n",
    "      - If fails with `session not found`, then wait a bit retry.\n",
    "    - In the `tmux`'s console window, `htop` will be displayed during the setup.\n",
    "    - Once the setup is complete, Arcion snapshot summary will be displayed.\n",
    "    - Wait for the setup to finish and the snapshot to complete. \n",
    "    - Takes about 5 minutes in for the setup to finish.\n",
    "- Iterate with the following:\n",
    "  - Configure Schema and Data\n",
    "  - Configure Workload\n",
    "  - Configure Arcion\n",
    "\n",
    "## Where is Data in Databricks\n",
    "  - Spark (Delta Lake) uses **Hive Meta Store** catalog: \n",
    "    - Open new tab Catalog -> hive_metastore -> <your username>\n",
    "    - find ycsbdense and ycsbsparse tables \n",
    "  - Lakehouse uses **Unity Catalog** catalog: \n",
    "    - Open new tab Catalog -> <your username> \n",
    "    - find ycsbdense and ycsbsparse tables \n",
    "\n",
    "## Frequent Demo Configurations\n",
    "- Step 1\n",
    "  - Click Real-Time\n",
    "  - Run just Arcion\n",
    "  - Change YCSB Size\n",
    "  - Watch real-time performance\n",
    "- Step 2\n",
    "  - Click Unity Catalog target\n",
    "  - Select full replication mode\n",
    "  - Run just Arcion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8484e2-ba04-4128-aa71-02f4fb01c5f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Personal Compute Cluster\n",
    "\n",
    "Choose at least 16GB of RAM for a demo.\n",
    "\n",
    "Processes use RAM.  The following is the minimum RAM usage.  The server needs enough RAM to avoid swapping.\n",
    "- Databricks: 5GB \n",
    "- SQL Server: 2GB\n",
    "- Arcion: 10% of server RAM.\n",
    "\n",
    "Note:\n",
    "- `vmstat 5`.  any non zero metrics under the `si` and `so` columns (swap in and swap out) indicate RAM shortage. \n",
    "- DBR 13 does not print output of subprocess.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2746520-15e7-48c2-9521-d397d1c993d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: file-read-backwards in /home/rslee/.local/lib/python3.10/site-packages (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deepdiff in /home/rslee/.local/lib/python3.10/site-packages (6.7.1)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /home/rslee/.local/lib/python3.10/site-packages (from deepdiff) (4.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install file-read-backwards\n",
    "%pip install deepdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cca325-b2f8-4be4-aaf6-862be36fe237",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prep python env\n",
    "import subprocess\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import requests\n",
    "import deepdiff\n",
    "from ipywidgets import HBox, VBox, Label\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "# all exp parameters \n",
    "def exp_params():\n",
    "    all_params={\n",
    "    # arcion\n",
    "    \"arcion_download_url\": arcion_download_url.value,\n",
    "    \"srcdb_arc_user\": src_username.value,\n",
    "    \"repl_type\": repl_mode.value,\n",
    "    \"replicant_memory_percentage\": ram_percent.value,\n",
    "    \"srcdb_snapshot_threads\": snapshot_threads.value,\n",
    "    \"srcdb_realtime_threads\": realtime_threads.value, \n",
    "    \"srcdb_delta\": delta_threads.value,\n",
    "    \"dstdb_type\": dbx_destinations.value,\n",
    "    \"dstdb_stage\": dbx_staging.value,\n",
    "    \"dbx_spark_url\": dbx_spark_url.value,\n",
    "    \"dbx_databricks_url\": dbx_databricks_url.value,\n",
    "    \"dbx_hostname\": dbx_hostname.value,\n",
    "    \"dbx_dbfs_root\": dbx_username.value,\n",
    "    \"dbx_username\": dbx_username.value,\n",
    "\n",
    "    # schema and data\n",
    "    \"sparse_cntstart\": sparse_cntstart.value,\n",
    "    \"sparse_cnt\": sparse_cnt.value , \n",
    "    \"sparse_fieldcount\": sparse_fieldcount.value, \n",
    "    \"sparse_fieldlength\": sparse_fieldlength.value, \n",
    "    \"sparse_recordcount\": sparse_recordcount.value, \n",
    "    \"sparse_fillpct_start\": sparse_fillpct.value[0],\n",
    "    \"sparse_fillpct_end\": sparse_fillpct.value[1],\n",
    "    \"dense_cntstart\": dense_cntstart.value, \n",
    "    \"dense_cnt\": dense_cnt.value, \n",
    "    \"dense_fieldcount\": dense_fieldcount.value, \n",
    "    \"dense_fieldlength\": dense_fieldlength.value, \n",
    "    \"dense_recordcount\": dense_recordcount.value, \n",
    "    \"dense_fillpct_start\": dense_fillpct.value[0],\n",
    "    \"dense_fillpct_end\": dense_fillpct.value[1],\n",
    "\n",
    "    # workload\n",
    "    \"sparse_tps\": sparse_tps.value,\n",
    "    \"dense_tps\": dense_tps.value,\n",
    "    \"sparse_threads\": sparse_threads.value,\n",
    "    \"dense_threads\": dense_threads.value,\n",
    "    \"sparse_multiUpdateSize\": sparse_multiupdatesize.value,\n",
    "    \"sparse_multiInsertSize\": sparse_multiinsertsize.value,\n",
    "    \"sparse_multiDeleteSize\": sparse_multideletesize.value,\n",
    "    \"dense_multiUpdateSize\": dense_multiupdatesize.value,\n",
    "    \"dense_multiInsertSize\": dense_multiinsertsize.value,\n",
    "    \"dense_multiDeleteSize\": dense_multideletesize.value,\n",
    "    }\n",
    "\n",
    "    # cluster\n",
    "    try:\n",
    "        all_params[\"spark.databricks.clusterUsageTags.clusterNodeType\"] = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\")\n",
    "        all_params[\"spark.databricks.clusterUsageTags.cloudProvider\"]  =  spark.conf.get(\"spark.databricks.clusterUsageTags.cloudProvider\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return(all_params)\n",
    "\n",
    "# used to start new MLFlow when parameters changes \n",
    "try:\n",
    "    mlflow_proc_state\n",
    "except:\n",
    "    mlflow_proc_state={}\n",
    "\n",
    "try:\n",
    "    previous_exp_params\n",
    "except:\n",
    "    previous_exp_params={}\n",
    "try:\n",
    "    current_exp_params\n",
    "except:\n",
    "    current_exp_params={}\n",
    "\n",
    "try:\n",
    "    ycsb_logfile_positions\n",
    "except:\n",
    "    ycsb_logfile_positions={}\n",
    "try:\n",
    "    ycsb_metrics\n",
    "except:\n",
    "    ycsb_metrics={}\n",
    "try:\n",
    "    previous_log_time\n",
    "except:\n",
    "    previous_log_time=None    \n",
    "\n",
    "# arcion statistics CSV\n",
    "arcion_stats_csv_header_lines=\"catalog_name,schema_name,table_name,snapshot_start_range,snapshot_end_range,start_time,end_time,insert_count,update_count,upsert_count,delete_count,elapsed_time_sec,replicant_lag,total_lag\"\n",
    "arcion_key_index={'insert_count':7,'update_count':8,'upsert_count':9,'delete_count':10,'elapsed_time_sec':11,'replicant_lag':12,'total_lag':13}\n",
    "\n",
    "try:\n",
    "    arcion_stats_csv_positions\n",
    "except:\n",
    "    arcion_stats_csv_positions={}\n",
    "\n",
    "# setup GUI elements\n",
    "\n",
    "repl_mode = widgets.Dropdown(options=['snapshot', 'real-time', 'full'],value='snapshot',\n",
    "    description='Replication:',\n",
    ")\n",
    "cdc_mode = widgets.Dropdown(options=['change', 'cdc'],value='change',\n",
    "    description='CDC Method:',\n",
    ")\n",
    "ram_percent = widgets.BoundedIntText(value=10,min=10,max=80,\n",
    "    description='RAM %:',\n",
    ")\n",
    "\n",
    "snapshot_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Snapshot Threads:',\n",
    ")\n",
    "\n",
    "realtime_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Real Time Threads:',\n",
    ")    \n",
    "\n",
    "delta_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Delta Snapshot Threads:',\n",
    ")    \n",
    "\n",
    "dbx_destinations = widgets.Dropdown(options=['null', 'deltalake', 'unitycatalog'],value='null',\n",
    "    description='Destinations:',\n",
    ")\n",
    "dbx_staging = widgets.Dropdown(options=['dbfs'],value='dbfs',\n",
    "    description='Staging:',\n",
    ")\n",
    "\n",
    "sparse_cnt = widgets.BoundedIntText(value=4,min=1,max=1000,\n",
    "    description='Tbl End:',\n",
    ")\n",
    "sparse_cntstart = widgets.BoundedIntText(value=1,min=1,max=1000,\n",
    "    description='Tbl Start:',\n",
    ")\n",
    "\n",
    "sparse_fieldcount = widgets.BoundedIntText(value=50,min=0,max=9000,\n",
    "    description='# of Fields:',\n",
    ")\n",
    "sparse_fieldlength = widgets.BoundedIntText(value=10,min=1,max=1000,\n",
    "    description='Field Len:',\n",
    ")\n",
    "\n",
    "sparse_tps = widgets.BoundedIntText(value=1,min=0,max=1000,\n",
    "    description='TPS:',\n",
    ")\n",
    "sparse_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Threads:',\n",
    ")\n",
    "sparse_recordcount = widgets.Text(value=\"2K\",\n",
    "    description='Rec Cnt:',\n",
    ")\n",
    "\n",
    "sparse_fillpct = widgets.IntRangeSlider(value=[0,0],min=0,max=100,step=1,\n",
    "    description='Fill Range:', orientation='horizontal', readout=False\n",
    ")\n",
    "\n",
    "dense_cnt = widgets.BoundedIntText(value=2,min=1,max=1000,\n",
    "    description='Tbl End:',\n",
    ")\n",
    "dense_cntstart = widgets.BoundedIntText(value=1,min=1,max=1000,\n",
    "    description='Tbl Start:',\n",
    ")\n",
    "\n",
    "dense_fieldcount = widgets.BoundedIntText(value=10,min=0,max=9000,\n",
    "    description='# of Fields:',\n",
    ")\n",
    "dense_fieldlength = widgets.BoundedIntText(value=100,min=1,max=1000,\n",
    "    description='Field Len:',\n",
    ")\n",
    "dense_recordcount = widgets.Text(value=\"1K\",\n",
    "    description='Rec Cnt:',\n",
    ")\n",
    "\n",
    "dense_tps = widgets.BoundedIntText(value=1,min=0,max=1000,\n",
    "    description='TPS:',\n",
    ")\n",
    "dense_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Threads:',\n",
    ")\n",
    "\n",
    "delupdins_proportion = widgets.IntRangeSlider(value=[0,100],min=0,max=100,step=1,\n",
    "    description='Del Upd Ind:', orientation='horizontal', readout=True\n",
    ")\n",
    "\n",
    "dense_multiupdatesize = widgets.BoundedIntText(value=0,min=0,max=10240, description='Upd Size:')\n",
    "dense_multiinsertsize = widgets.BoundedIntText(value=0,min=0,max=10240, description='Ins Size:')\n",
    "dense_multideletesize = widgets.BoundedIntText(value=0,min=0,max=10240, description='Del Size:')\n",
    "\n",
    "sparse_multiupdatesize = widgets.BoundedIntText(value=0,min=0,max=10240, description='Upd Size:')\n",
    "sparse_multiinsertsize = widgets.BoundedIntText(value=0,min=0,max=10240, description='Ins Size:')\n",
    "sparse_multideletesize = widgets.BoundedIntText(value=0,min=0,max=10240, description='Del Size:')\n",
    "\n",
    "dense_fillpct = widgets.IntRangeSlider(value=[1,99],min=0,max=100,step=1,\n",
    "    description='Fill Range:', orientation='horizontal', readout=False\n",
    ")\n",
    "\n",
    "dbx_spark_url = widgets.Textarea(value='',\n",
    "    description='Spark URL:',\n",
    ")\n",
    "\n",
    "dbx_databricks_url = widgets.Textarea(value='',\n",
    "    description='Databricks URL:',\n",
    ")\n",
    "\n",
    "dbx_hostname = widgets.Textarea(value='',\n",
    "    description='Hostname:',\n",
    ")\n",
    "\n",
    "src_username = widgets.Textarea(value='',\n",
    "    description='SRC User:',\n",
    ")\n",
    "\n",
    "dbx_username = widgets.Textarea(value='',\n",
    "    description='DST User:',\n",
    ")\n",
    "\n",
    "arcion_license = widgets.Textarea(value='',\n",
    "    description='Lic',\n",
    ")\n",
    "\n",
    "arcion_download_url = widgets.Textarea(value='https://arcion-releases.s3.us-west-1.amazonaws.com/general/replicant/replicant-cli-24.01.25.7.zip',\n",
    "    description='Download URL',\n",
    ")\n",
    "\n",
    "dbx_access_token = widgets.Password(value='',\n",
    "    description='Access Token',\n",
    ")\n",
    "\n",
    "dbx_default_catalog = widgets.Textarea(value='',\n",
    "    description='HMS Catalog',\n",
    ")\n",
    "\n",
    "\n",
    "# cluster where the notebook is running to auto populate the destinations\n",
    "spark_url=\"\"\n",
    "databricks_url=\"\"\n",
    "workspaceUrl=\"\"\n",
    "username=\"\"\n",
    "try:\n",
    "    cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "    workspace_id =spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n",
    "\n",
    "    # clusterName = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\")\n",
    "\n",
    "    workspaceUrl = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['tags']['browserHostName']\n",
    "\n",
    "    # below does not work on GCP\n",
    "    # sc.getConf().getAll() to see what is avail\n",
    "    # workspaceUrl = spark.conf.get(\"spark.databricks.workspaceUrl\") # host name\n",
    "\n",
    "    http_path = f\"sql/protocolv1/o/{workspace_id}/{cluster_id}\"\n",
    "\n",
    "    spark_url=f\"jdbc:spark://{workspaceUrl}:443/default;transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;\"\n",
    "    databricks_url=f\"jdbc:databricks://{workspaceUrl}:443/default;transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;\"\n",
    "\n",
    "except:\n",
    "    pass\n",
    "dbx_spark_url.value = spark_url\n",
    "dbx_databricks_url.value = databricks_url\n",
    "dbx_hostname.value = workspaceUrl\n",
    "\n",
    "try:\n",
    "    username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "    dbx_username.value = re.sub('[.@]','_',username)\n",
    "    src_username.value = re.sub('[.@]','_',username)\n",
    "except:\n",
    "    src_username.value='arcsrc'\n",
    "    dbx_username.value='arcdst'\n",
    "\n",
    "try:\n",
    "    dbx_default_catalog.value=spark.conf.get(\"spark.databricks.sql.initial.catalog.name\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check arcion license via os env\n",
    "try:\n",
    "    arclicenv=os.environ[\"ARCION_LICENSE\"]\n",
    "    if arclicenv != \"\": \n",
    "        arcion_license.value=arclicenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check arcion license via dbx widget\n",
    "try:\n",
    "    arclicwidget=dbutils.widgets.get(\"Arcion License\")\n",
    "    if arclicwidget != \"\": \n",
    "        arcion_license.value=arclicwidget\n",
    "        arcion_license.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check access token via dbx widget\n",
    "try:\n",
    "    acctokwidget=dbutils.widgets.get(\"Access Token\")\n",
    "    if acctokwidget != \"\": \n",
    "        dbx_access_token.value=acctokwidget\n",
    "        dbx_access_token.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check if os env has ARCION_LICENSE\n",
    "try:\n",
    "    arclicenv=os.getenv('ARCION_LICENSE')\n",
    "    if arclicenv != \"\": \n",
    "        arcion_license.value=arclicenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# gcp does not change cwd to notebook path\n",
    "pwd_result= subprocess.run(f\"\"\"pwd\"\"\",capture_output = True, text = True )\n",
    "if (pwd_result.stdout == \"/databricks/driver\\n\"):\n",
    "    notebookpath=\"/Workspace\" + str(pathlib.Path(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()).parent)\n",
    "else:\n",
    "    notebookpath = None\n",
    "\n",
    "# optional MLflow\n",
    "experiment_id=None\n",
    "try:\n",
    "    import mlflow\n",
    "    experiment_id=dbutils.widgets.get(\"Experiment ID\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872e49ec-d202-4d22-8a13-7f4f6ce38b62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup\n",
    "  - Enter `Arcion License`\n",
    "  - Enter `Personal Access Token` (generate **One Day** and delete afterwards)\n",
    "  - Click **Menu Bar** ->  Run -> Run All Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995664f9-a450-4da8-980d-e54a19214565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a746ba2-57a0-4a80-b8e0-4e8fdd0254ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c268fb1f9f054180a5cb07e603947d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Arcion'), Textarea(value='', description='Lic'), Textarea(value='ht…"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enter license and DBX personal access token\n",
    "VBox([HBox([Label('Arcion'), arcion_license, arcion_download_url]),\n",
    "      HBox([Label('DBX'), dbx_access_token, dbx_default_catalog]),\n",
    "      HBox([Label('Username'), src_username, dbx_username]),\n",
    "      HBox([Label('Workspace'), dbx_spark_url, dbx_databricks_url, dbx_hostname, ]),\n",
    "       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c848f2d-7c81-4bd2-a56e-ababeed2962c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a208371c-d54e-4158-81be-16483107efa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmux session ready. session arcdst already exists\n",
      "apt-utils already installed\n",
      "mssql-server already installed\n",
      "mssql-tools18 already installed\n",
      "unixodbc-dev alrady installed\n",
      "sqlserver already started\n",
      "deltalake /opt/stage/libs/SparkJDBC42.jar found\n",
      "lakehouse  /opt/stage/libs/DatabricksJDBC42.jar found\n",
      "postgres  /opt/stage/libs/postgresql-42.7.1.jar found\n",
      "mariadb  /opt/stage/libs/mariadb-java-client-3.3.2.jar found\n",
      "oracle /opt/stage/libs/ojdbc8.jar found\n",
      "log4j /opt/stage/libs/log4j-1.2.17.jar found\n",
      "sqlserver /opt/stage/libs/mssql-jdbc-12.6.1.jre8.jar found\n",
      "arcion  /opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/bin/replicant found\n",
      "checking jar(s) in /opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/lib for updates\n",
      "checking jar(s) in /opt/stage/arcion/replicant-cli-24.01.25.1/replicant-cli/lib for updates\n",
      "Arcion license found\n",
      "YCSB  /opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT  found\n",
      "numfmt found\n",
      "checking jar(s) in /opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT/lib for updates\n",
      "checking jar(s) in /opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT-old/lib for updates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "open terminal failed: not a terminal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/bin/replicant 24.01.25.7 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/bin/replicant 24.01.25.7 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "user arcsrc already exists.  skipping\n",
      "/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/bin/replicant 24.01.25.7 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "Configuration option 'show advanced options' changed from 1 to 1. Run the RECONFIGURE statement to install.\n",
      "Configuration option 'max server memory (MB)' changed from 1024 to 1024. Run the RECONFIGURE statement to install.\n",
      "prometheus already downloaded\n",
      "prometheus node_exporter already downloaded\n",
      "prometheus sql_exporter already downloaded\n",
      "started /opt/stage/prom/sql_exporter-0.14.0.linux-amd64/sql_exporter.  log at /var/tmp/arcsrc/sqlserver/logs/sql_exporter.log\n",
      "started /opt/stage/prom/node_exporter-1.7.0.linux-amd64/node_exporter.  log at /var/tmp/arcsrc/sqlserver/logs/node_exporter.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='export SRCDB_ARC_USER=arcsrc; bin/install-prometheus.sh', returncode=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup tmux, arcion, ycsb, sql server\n",
    "subprocess.run(f\"\"\". ./bin/setup-tmux.sh; setup_tmux '{dbx_username.value}'\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/install-sqlserver.sh\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/download-jars.sh\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"ARCION_LICENSE='{arcion_license.value}' ARCION_DOWNLOAD_URL='{arcion_download_url.value}' bin/install-arcion.sh\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/install-ycsb.sh\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; ping_sql_cli;\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; create_user;\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; set_sqlserver_ram '{dbx_username.value}';\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; bin/install-prometheus.sh\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427dc8c9-96f3-4ac6-bf41-4efd371f3b04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Schema and Data\n",
    "\n",
    "Existing tables will be appended with additional rows if the `Fill Range` is the same.  \n",
    "Increase the `Table Count` to create additional tables.  \n",
    "\n",
    "The following options are available:\n",
    "- Table count (Table Cnt): The number of tables to create.  \n",
    "  - Table names are `ycsbdense`, `ycsbdense2`, `ycsbdense3`, ... and `ycssparse`, `ycsbdense2`, and `ycsbdense3` ...\n",
    "- Number of Fields (# of Fields): The number of fields per table.  \n",
    "  - The field names are `FIELD0`, `FIELD1`, `FIELD2`, ...\n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Field Length (Field Len): The length of random character data populated per field.  \n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Record Count (Rec Cnt): The number of records per table generated.\n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Fill Range: The relative start and end range of fields that are populated with data.  Be default: \n",
    "    - sparse tables are all NULLs by having the fill range be 0% to 0% ranges\n",
    "    - dense tables have all fields populated by having the fill range be 0% to 100% of ranges \n",
    "\n",
    "```sql\n",
    "[localhost][arcsrc] 1> \\describe ycsbsparse\n",
    "+-------------+-------------+-----------+-------------+----------------+-------------+\n",
    "| TABLE_SCHEM | COLUMN_NAME | TYPE_NAME | COLUMN_SIZE | DECIMAL_DIGITS | IS_NULLABLE |\n",
    "+-------------+-------------+-----------+-------------+----------------+-------------+\n",
    "| dbo         | YCSB_KEY    | int       |          10 |              0 | NO          |\n",
    "| dbo         | FIELD0      | text      |  2147483647 |         [NULL] | YES         |\n",
    "| dbo         | FIELD1      | text      |  2147483647 |         [NULL] | YES         |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd09cab-c63e-4195-9364-09011fd2b559",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "Make changes below and click `Run All Below`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c315fc-624c-442c-b712-518336f5bfd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd49327b054646dca3109b1d69f02e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Sparse'), BoundedIntText(value=1, description='Tbl Start:', max=100…"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show YCSB Data Controls\n",
    "VBox([HBox([Label('Sparse'), sparse_cntstart,sparse_cnt, sparse_fieldcount, sparse_fieldlength, sparse_recordcount, sparse_fillpct]),\n",
    "    HBox([Label('Dense'),  dense_cntstart, dense_cnt, dense_fieldcount, dense_fieldlength, dense_recordcount, dense_fillpct])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370b278c-17fe-4f68-ac8f-aa0ee8c18568",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7f7c0f-9f1d-40e3-a4f8-a02d6a42bf0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/bin/replicant 24.01.25.7 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "starting sparse load. /var/tmp/arcsrc/sqlserver/logs/ycsb/ycssparse.load.log 1\n",
      "starting dense load. /var/tmp/arcsrc/sqlserver/logs/ycsb/ycsbdense.load.log 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "schema dump at /var/tmp/arcsrc/sqlserver/config/schema_dump.csv\n",
      "table count at /var/tmp/arcsrc/sqlserver/config/list_table_counts.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table name</th>\n",
       "      <th>min key</th>\n",
       "      <th>max key</th>\n",
       "      <th>field count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YCSBDENSE</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YCSBSPARSE</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   table name  min key  max key  field count\n",
       "0   YCSBDENSE        0      999           10\n",
       "1  YCSBSPARSE        0     1999           50"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run load_sparse_data_cnt and load_dense_data_cnt \n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; \n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    y_fieldcount={sparse_fieldcount.value} \n",
    "    y_fieldlength={sparse_fieldlength.value}  \n",
    "    y_recordcount={sparse_recordcount.value} \n",
    "    y_fillstart={math.ceil((sparse_fillpct.value[0] * sparse_fieldcount.value) / 100)}      \n",
    "    y_fillend={int((sparse_fillpct.value[1] * sparse_fieldcount.value) / 100)}      \n",
    "    load_sparse_data_cnt {sparse_cnt.value} {sparse_cntstart.value};\n",
    "    y_fieldcount={dense_fieldcount.value} \n",
    "    y_fieldlength={dense_fieldlength.value} \n",
    "    y_recordcount={dense_recordcount.value} \n",
    "    y_fillstart={math.ceil((dense_fillpct.value[0] * dense_fieldcount.value) / 100)}      \n",
    "    y_fillend={int((dense_fillpct.value[1] * dense_fieldcount.value) / 100)}      \n",
    "    load_dense_data_cnt {dense_cnt.value} {dense_cntstart.value};\n",
    "    dump_schema;\n",
    "    list_table_counts\"\"\",\n",
    "    shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath) \n",
    "# show tables\n",
    "pd.read_csv (f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\",header=None, names= ['table name','min key','max key','field count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1aeafcc-3471-47b9-b59d-b14016329940",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Workload\n",
    "\n",
    "Choose the options in the UI and run the cell below it to start the workload (YCSB).  \n",
    "\n",
    "YCSB update (workload A) controls for Dense and Sparse table groups separated. Each group has a separate control.  However, all of the tables in the group use the same controls.  \n",
    "1. Each table's TPS (throughput per second)\n",
    "   1. 0=fast as possible\n",
    "   2. 1=1 TPS\n",
    "   3. 10=10 TPS\n",
    "2. Each table's threads (concurrency) used to achieve the desired TPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e009b417-01dc-4855-b557-3a1cd88f70c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3233053-e7a3-4c56-8544-6d75c718f1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b988810e854601950ee9c31cf19cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Ratios'), IntRangeSlider(value=(10, 100), description='Del Upd Ind:…"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show YCSB run controls\n",
    "VBox([HBox([Label('Ratios'), delupdins_proportion]),\n",
    "      HBox([Label('Sparse'), sparse_tps, sparse_threads, sparse_multiupdatesize, sparse_multiinsertsize, sparse_multideletesize]), \n",
    "      HBox([Label('Dense'),  dense_tps, dense_threads, dense_multiupdatesize, dense_multiinsertsize, dense_multideletesize])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1920d9d-bae0-49b0-adab-554352dcc7a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33058c18-c9e7-46f2-ab85-618136369430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'IntRangeSlider' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# start/restart YCSB run\u001b[39;00m\n\u001b[1;32m      2\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mexport SRCDB_ARC_USER=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_username\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    kill_ycsb;\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    list_table_counts;       \u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    y_del_proportion=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelupdins_proportion\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    y_upd_proportion=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelupdins_proportion\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;124m    y_ins_proportion=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m-\u001b[39m{\u001b[43mdelupdins_proportion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m}\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    y_target_sparse=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_tps\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    y_target_dense=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_tps\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    y_threads_sparse=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_threads\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    y_threads_dense=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_threads\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    y_multiinsertsize_dense=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_multiinsertsize\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m    y_multiupdatesize_dense=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_multiupdatesize\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    y_multideletesize_dense=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_multideletesize\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    y_multiinsertsize_sparse=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_multiinsertsize\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m    y_multiupdatesize_sparse=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_multiupdatesize\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m    y_multideletesize_sparse=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_multideletesize\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m    y_fieldlength_sparse=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_fieldlength\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m    y_fieldlength_dense=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_fieldlength\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    start_ycsb;\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,executable\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/usr/bin/bash\u001b[39m\u001b[38;5;124m\"\u001b[39m,cwd\u001b[38;5;241m=\u001b[39mnotebookpath)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'IntRangeSlider' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# start/restart YCSB run\n",
    "y_del_proportion=(delupdins_proportion.value[0]) / 100.0\n",
    "y_upd_proportion=(delupdins_proportion.value[1] - y_del_proportion) / 100.0\n",
    "y_ins_proportion=(100 - delupdins_proportion.value[1]) / 100.0\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value};\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    kill_ycsb;\n",
    "    list_table_counts;       \n",
    "    y_del_proportion={y_del_proportion}\n",
    "    y_upd_proportion={y_upd_proportion}\n",
    "    y_ins_proportion={y_ins_proportion}\n",
    "    y_target_sparse={sparse_tps.value} \n",
    "    y_target_dense={dense_tps.value} \n",
    "    y_threads_sparse={sparse_threads.value} \n",
    "    y_threads_dense={dense_threads.value} \n",
    "    y_multiinsertsize_dense={dense_multiinsertsize.value} \n",
    "    y_multiupdatesize_dense={dense_multiupdatesize.value} \n",
    "    y_multideletesize_dense={dense_multideletesize.value} \n",
    "    y_multiinsertsize_sparse={sparse_multiinsertsize.value} \n",
    "    y_multiupdatesize_sparse={sparse_multiupdatesize.value} \n",
    "    y_multideletesize_sparse={sparse_multideletesize.value} \n",
    "    y_fieldlength_sparse={sparse_fieldlength.value} \n",
    "    y_fieldlength_dense={dense_fieldlength.value} \n",
    "    start_ycsb;\"\"\",\n",
    "    shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6734eb-5342-48b5-868b-750218370b54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Arcion\n",
    "\n",
    "Choose the options in the UI and run the cell below it to start the replication.  \n",
    "\n",
    "The following control are avail in the demo.  \n",
    "- Arcion - replication type and CDC methods  \n",
    "- Threads - control the parallelism.\n",
    "- Target - null, unity catalog or delta lake\n",
    "\n",
    "NOTE: Full mode does not work at this time.\n",
    "\n",
    "For SQL Server, change tracking, cdc are available for demo.  \n",
    "\n",
    "Performance is mainly controlled by the thread count by the extract and apply process.\n",
    "Additional controls are customizable via modifying the YAML files directly below.\n",
    "- [CDC YAML files](./demo/sqlserver/yaml/cdc/)\n",
    "- [Change Tracking YAML files](./demo/sqlserver/yaml/change/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb753ed-5c15-4f74-9ed6-8ddf59a2ce42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "Make changes below and click `Run All Below`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d603b1d4-bb2c-418b-acfe-894441efe01e",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb701dde1b8408e935b0bad4d56ecd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='RAM'), BoundedIntText(value=10, description='RAM %:', max=80, min=1…"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show Arcion and DBX controls\n",
    "VBox([\n",
    "      HBox([Label('RAM'), ram_percent]),\n",
    "      HBox([Label('Modes'), repl_mode, cdc_mode]),\n",
    "      HBox([Label('Target'), dbx_destinations, dbx_staging ]),\n",
    "      HBox([Label('Threads'), snapshot_threads, realtime_threads, delta_threads]),\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbea2ac-b6ea-4cf4-9be1-bf665a7e6ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a0ede1-4383-4f3e-b9c8-5e601c35bfcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change real-time\n",
      "/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/bin/replicant 24.01.25.7 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "prog_dir=/home/rslee/github/dbx/ingestion/demo/sqlserver arcion_bin=/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/bin/replicant\n",
      "enable change tracking on database arcsrc\n",
      "skip ALTER DATABASE arcsrc SET CHANGE_TRACKING = ON  (CHANGE_RETENTION = 2 DAYS, AUTO_CLEANUP = ON);\n",
      "skip ALTER TABLE replicate_io_audit_ddl ENABLE CHANGE_TRACKING;\n",
      "skip ALTER TABLE replicate_io_audit_tbl_cons ENABLE CHANGE_TRACKING;\n",
      "skip ALTER TABLE replicate_io_audit_tbl_schema ENABLE CHANGE_TRACKING;\n",
      "skip ALTER TABLE YCSBDENSE ENABLE CHANGE_TRACKING;\n",
      "skip ALTER TABLE YCSBSPARSE ENABLE CHANGE_TRACKING;\n",
      "arcion pid 1116373\n",
      "arcion console is at /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd/arcion.log\n",
      "arcion log is at /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd\n",
      "arcion can be killed with . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_arcion)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ cd /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd\n",
      "+ set +x\n",
      "+ JAVA_HOME=\n",
      "+ REPLICANT_MEMORY_PERCENTAGE=10.0\n",
      "+ JAVA_OPTS='\"-Djava.security.egd=file:/dev/urandom\" \"-Doracle.jdbc.javaNetNio=false\" \"-XX:-UseCompressedOops\"'\n",
      "+ /opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/bin/replicant real-time /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd/src.yaml /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd/dst.yaml --applier /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd/applier.yaml --general /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd/general.yaml --extractor /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd/extractor.yaml --filter /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd/filter.yaml --statistics /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd/statistics.yaml --metadata /var/tmp/arcsrc/sqlserver/logs/3fb4e2cdd/metadata.yaml --overwrite --id 3fb4e2cdd --append-existing\n"
     ]
    }
   ],
   "source": [
    "# start/restart Arcion\n",
    "if ( f\"{dbx_access_token.value}\" == \"\" ) and ( f\"{dbx_destinations.value}\" != \"null\" ):\n",
    "    print(\"personal access token not entered.\")\n",
    "else:\n",
    "    # start a new run\n",
    "    print (f\"\"\"{cdc_mode.value} {repl_mode.value}\"\"\")\n",
    "    arcion_run = subprocess.run(f\"\"\"export ARCION_DOWNLOAD_URL='{arcion_download_url.value}';\n",
    "    export SRCDB_ARC_USER={src_username.value};\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    kill_arcion;\n",
    "    echo prog_dir=$PROG_DIR arcion_bin=$ARCION_BIN;\n",
    "    cd $PROG_DIR;\n",
    "    a_repltype='{repl_mode.value}'\n",
    "    REPLICANT_MEMORY_PERCENTAGE='{ram_percent.value}.0'\n",
    "    SRCDB_SNAPSHOT_THREADS='{snapshot_threads.value}' \n",
    "    SRCDB_REALTIME_THREADS='{realtime_threads.value}' \n",
    "    SRCDB_DELTA='{delta_threads.value}'\n",
    "    DSTDB_TYPE='{dbx_destinations.value}'\n",
    "    DSTDB_STAGE='{dbx_staging.value}'\n",
    "    DBX_SPARK_URL='{dbx_spark_url.value}'\n",
    "    DBX_DATABRICKS_URL='{dbx_databricks_url.value}'\n",
    "    DBX_ACCESS_TOKEN='{dbx_access_token.value}'\n",
    "    DBX_HOSTNAME='{dbx_hostname.value}'\n",
    "    DBX_DBFS_ROOT='/{dbx_username.value}'\n",
    "    DBX_USERNAME='{dbx_username.value}'\n",
    "    start_{cdc_mode.value}_arcion;\"\"\",\n",
    "    shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cafb49c3-df1b-4458-805c-b233876acf90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# MLFLow\n",
    "\n",
    "Save the artifacts in MLFlow.\n",
    "\n",
    "Artifacts are collected for 5 min (600 sec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f193b18-1efa-46be-afb3-37510d683db0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb57b96-e39c-465e-a939-2ec7aa27392a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param changed. starting new mlflow\n",
      "previous MLFlow process terminated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'ycsbdense': datetime.datetime(2024, 3, 10, 16, 45, 53, 432000), 'ycsbsparse': datetime.datetime(2024, 3, 10, 16, 45, 53, 422000)}\n",
      "{'ycsbdense': datetime.datetime(2024, 3, 10, 16, 45, 53, 432000), 'ycsbsparse': datetime.datetime(2024, 3, 10, 16, 45, 53, 422000)}\n",
      "{'ycsbdense': datetime.datetime(2024, 3, 10, 16, 45, 53, 432000), 'ycsbsparse': datetime.datetime(2024, 3, 10, 16, 45, 53, 422000)}\n",
      "{'ycsbdense': datetime.datetime(2024, 3, 10, 16, 45, 53, 432000), 'ycsbsparse': datetime.datetime(2024, 3, 10, 16, 45, 53, 422000)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/10 16:50:55 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/03/10 16:50:55 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "# use process to run MLflow without blocking the notebook.  thread does not work with mlflow\n",
    "\n",
    "import mlflow\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from multiprocessing import Process\n",
    "\n",
    "def log_artifacts():\n",
    "    pass\n",
    "\n",
    "from file_read_backwards import FileReadBackwards\n",
    "import datetime\n",
    "\n",
    "# convert ycsb log mlflow metric\n",
    "# time                    elapsed  cumulative      time period                                   per operations metric\n",
    "#                         sec      operations      ops/sec\n",
    "# 2024-03-07 10:05:38:240 410 sec: 409 operations; 1 current ops/sec; est completion in 116 days [UPDATE: Count=10, Max=15383, Min=6792, Avg=9264.6, 90=15359, 99=15383, 99.9=15383, 99.99=15383]\n",
    "# ycsb_tablename_[update|update-failed]_count=x\n",
    "# ycsb_tablename_[update|update-failed]_avg_microsec=x \n",
    "\n",
    "ycsb_date_time_pattern = r\"^(?P<dt>[0-9\\-]+ [0-9\\:]+)\"  # at the beginning\n",
    "ycsb_op_val_pattern = r'\\[([^]]*)\\]'                    # [Update: ] [Insert: ] ...\n",
    "\n",
    "def parse_ycsb_log_to_metric(table_name=\"ycsbsparse\",\n",
    "                    file=\"/var/tmp/arcsrc/sqlserver/logs/ycsb/ycsb.ycsbsparse.log\",\n",
    "                    previous_log_time=None, \n",
    "                    metrics={},\n",
    "                    ycsb_logfile_positions={}):\n",
    "    with FileReadBackwards(file, encoding=\"utf-8\") as ycsb_log_file:\n",
    "        count=0\n",
    "        for line in ycsb_log_file:            \n",
    "            # parse date time and bail if already processed\n",
    "            m = re.search(ycsb_date_time_pattern, line)\n",
    "            if m is None:\n",
    "                continue\n",
    "\n",
    "            log_time=datetime.datetime.strptime(m.group('dt'), '%Y-%m-%d %H:%M:%S:%f')\n",
    "            try:\n",
    "                if log_time == ycsb_logfile_positions[table_name]:\n",
    "                    return\n",
    "            except:\n",
    "                pass    \n",
    "\n",
    "            # parse [update: ...]\n",
    "            m = re.findall(ycsb_op_val_pattern, line.lower())\n",
    "            if m is None:\n",
    "                continue\n",
    "            \n",
    "            ycsb_logfile_positions[table_name] = log_time\n",
    "            for ops in m:\n",
    "                op_vals=ops.split(\":\")                  # update: ....\n",
    "                vals_array=op_vals[1].split(\",\")        # count=?, max=?, ...\n",
    "                try:    \n",
    "                    op_count=float(vals_array[0].split(\"=\")[1])    # [0] count=?\n",
    "                except:\n",
    "                    op_count=0.0\n",
    "                try:\n",
    "                    op_avg=float(vals_array[3].split(\"=\")[1])      # [1] avg=? if count=0, then this will be not defined\n",
    "                except:\n",
    "                    op_avg=0.0\n",
    "                metrics[f\"ycsb_{op_vals[0]}_count_{table_name}\"]=op_count\n",
    "                metrics[f\"ycsb_{op_vals[0]}_avg_microsec_{table_name}\"]=op_avg\n",
    "            return\n",
    "\n",
    "def get_ycsb_metrics(metrics={}):\n",
    "    ycsb_current_metrics={}\n",
    "    print(ycsb_logfile_positions)\n",
    "    ycsb_tables = pd.read_csv (f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\",header=None, names= ['table name','min key','max key','field count'])\n",
    "    for table_name in ycsb_tables['table name']:\n",
    "        table_name = table_name.lower()\n",
    "        parse_ycsb_log_to_metric(\n",
    "            table_name=table_name, \n",
    "            file=f\"/var/tmp/{src_username.value}/sqlserver/logs/ycsb/ycsb.{table_name}.log\",\n",
    "            previous_log_time=previous_log_time,\n",
    "            metrics=ycsb_current_metrics,\n",
    "            ycsb_logfile_positions=ycsb_logfile_positions)\n",
    "    return(ycsb_current_metrics)\n",
    "\n",
    "def get_prom_metrics(prom_metric_url=None,metric_prefix=\"\",metric_step=None):\n",
    "    # there is a limit on the number of metrics that you can log in a single log_batch call. This limit is typically 1000. \n",
    "    # timestamp=If unspecified, the number of milliseconds since the Unix epoch is used.\n",
    "    # step=If unspecified, the default value of zero is used\n",
    "    contents = requests.get(prom_metric_url)\n",
    "    all_metrics = {}\n",
    "    metrics_count = 0\n",
    "    for line in contents.text.splitlines():\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        key_val=line.rsplit(' ',1)  # split from the end in case the key has spaces\n",
    "        all_metrics[re.sub('[\" {}=,]',\"_\",key_val[0])]=float(key_val[1])\n",
    "        metrics_count += 1\n",
    "    return(all_metrics)\n",
    "\n",
    "\n",
    "def start_mlflow(max_intervals=5,experiment_id=None, log_interval_sec=60, all_params={}):\n",
    "    # stop previous run\n",
    "    mlflow_run = mlflow.active_run()\n",
    "    if not(mlflow_run is None):\n",
    "        # upload final artifacts\n",
    "        log_artifacts()\n",
    "        print(f\"\"\"stopping previous MLflow {mlflow_run.info.run_id}\"\"\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "    # start a new run\n",
    "    if experiment_id == '':\n",
    "        experiment_id=None\n",
    "    mlflow.start_run(experiment_id=experiment_id, log_system_metrics=True)\n",
    "\n",
    "    # params\n",
    "    mlflow.log_params(params=all_params)\n",
    "\n",
    "    # schema\n",
    "    dataset_source=f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\"\n",
    "    mlflow.log_artifact(dataset_source)\n",
    "    \n",
    "    # data\n",
    "    dataset_shape = pd.read_csv(dataset_source, header=None, names= ['table name','min key','max key','field count'])\n",
    "    dataset = mlflow.data.from_pandas(dataset_shape, source=dataset_source)\n",
    "    mlflow.log_input(dataset, context=\"training\")    \n",
    "\n",
    "    # wait to end\n",
    "    # TODO: Make this smarter by checking whether the process is still running\n",
    "    wait_count=0\n",
    "    while wait_count < max_intervals:\n",
    "        mlflow.log_metrics(metrics=get_prom_metrics(prom_metric_url=\"http://localhost:9399/metrics\"))\n",
    "        mlflow.log_metrics(metrics=get_prom_metrics(prom_metric_url=\"http://localhost:9100/metrics\"))\n",
    "        mlflow.log_metrics(metrics=get_ycsb_metrics())\n",
    "        time.sleep(log_interval_sec)\n",
    "        wait_count += 1\n",
    "\n",
    "    # upload the rest of the artifacts generated /var/tmp/{src_username.value}/sqlserver/logs\n",
    "    log_artifacts()\n",
    "    # experiment done\n",
    "    mlflow.end_run()\n",
    "\n",
    "def register_mlflow(exp_params):\n",
    "    mlflow_proc = Process(target=start_mlflow, kwargs={\"experiment_id\":experiment_id, \"all_params\":current_exp_params})\n",
    "    mlflow_proc.start()   \n",
    "    try:\n",
    "        mlflow_proc_state['proc'].terminate()\n",
    "        print(\"previous MLFlow process terminated\")\n",
    "    except:\n",
    "        pass\n",
    "    mlflow_proc_state['proc']       = mlflow_proc\n",
    "    mlflow_proc_state['exp_params'] = exp_params\n",
    "\n",
    "\n",
    "current_exp_params=exp_params()\n",
    "if not ('exp_params' in mlflow_proc_state):\n",
    "    print(\"first run of mlflow\")\n",
    "    register_mlflow(current_exp_params)\n",
    "elif current_exp_params != mlflow_proc_state['exp_params']:\n",
    "    print(\"param changed. starting new mlflow\")\n",
    "    register_mlflow(current_exp_params)\n",
    "elif not(mlflow_proc_state['proc'].is_alive()):\n",
    "    print(\"mlflow stopped. starting new mlflow with new step\")\n",
    "    register_mlflow(current_exp_params)\n",
    "else:\n",
    "    print(\"no parameters changed. New MLFLow experiment not needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7184a016-b7ef-4302-bee5-38ec30ed02f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Manually Kill Processes\n",
    "Uncomment below to kill desired processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79738332-5dec-4a1f-a0af-b2d9fc442150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/bin/replicant 24.01.25.7 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "kill 1002419\n",
      "kill 1002535\n",
      "/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/bin/replicant 24.01.25.7 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "kill 1001903\n",
      "kill 1001976\n",
      "kill 1001912\n",
      "kill 1002000\n",
      "kill 1001926\n",
      "kill 1002020\n",
      "kill 1001945\n",
      "kill 1002053\n",
      "kill 1001974\n",
      "kill 1002074\n",
      "kill 1002001\n",
      "kill 1002118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='. ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_ycsb;', returncode=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/10 16:45:54 WARNING mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics because creating `GPUMonitor` failed with error: `pynvml` is not installed, to log GPU metrics please run `pip install pynvml` to install it..\n"
     ]
    }
   ],
   "source": [
    "subprocess.run(f\"\"\". ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_arcion;\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\". ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_ycsb;\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9561e8-d3f2-4196-8a4d-ec42a98b78fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "def parse_arcion_stats():\n",
    "    with FileReadBackwards(\"/var/tmp/arcsrc/sqlserver/logs/3fb22d2a4/3fb22d2a4/replication_statistics_history_2024-03-07_3.CSV\", encoding=\"utf-8\") as BigFile:\n",
    "        max_start_time = None\n",
    "        all_metrics = {}\n",
    "        unprocessed_tables = arcion_stats_csv_positions.copy()\n",
    "        for line in BigFile:\n",
    "            if line==arcion_stats_csv_header_lines:\n",
    "                break\n",
    "            tokens=line.split(\",\")\n",
    "            print (line)\n",
    "\n",
    "            key=tokens[0]+\"_\"+tokens[1]+\"_\"+tokens[2]\n",
    "\n",
    "            # max start_time\n",
    "            start_time_str=tokens[5]\n",
    "            start_time=datetime.strptime(start_time_str, '%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "\n",
    "            if max_start_time is None:\n",
    "                max_start_time = start_time\n",
    "            \n",
    "            if key in arcion_stats_csv_positions:\n",
    "                if arcion_stats_csv_positions[key] < start_time:\n",
    "                    # line has newer data\n",
    "                    arcion_stats_csv_positions[key]=start_time\n",
    "                else:\n",
    "                    print(\"skip\")\n",
    "            else:\n",
    "                arcion_stats_csv_positions[key]=start_time\n",
    "\n",
    "            # continue until all tables are processed or time out\n",
    "            try:\n",
    "                del unprocessed_tables[key]\n",
    "            except:\n",
    "                pass\n",
    "            if len(unprocessed_tables) == 0:\n",
    "                break\n",
    "            if (max_start_time - start_time).seconds > 10:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Arcion License",
      "width": 184
     }
    ]
   },
   "notebookName": "sqlserver-arcion-dbx",
   "widgets": {
    "Arcion License": {
     "currentValue": "ewogICJsaWNlbnNlIiA6IHsKICAgICJ1dWlkIiA6ICI2MWQ0N2Y4Yi0zYmI4LTRhOTktYTAwYS1mMjA2OGEzMWM0MmYiLAogICAgIm93bmVyIiA6ICJEQiBTdXBwb3J0IFRyYWluaW5nIiwKICAgICJjcmVhdGVkIiA6ICIyMDIzLTEyLTA2VDAwOjAwWiIsCiAgICAiZXhwaXJlcyIgOiAiMjAyNC0wNi0wNlQwMDowMFoiLAogICAgInR5cGUiIDogIk9GRkxJTkUiLAogICAgImVkaXRpb24iIDogIkVOVEVSUFJJU0UiLAogICAgInNyYyIgOiBbICJBTEwiIF0sCiAgICAiZHN0IiA6IFsgIkFMTCIgXQogIH0sCiAgImtleSIgOiAiZXlKMWRXbGtJam9pTmpGa05EZG1PR0l0TTJKaU9DMDBZVGs1TFdFd01HRXRaakl3TmpoaE16RmpOREptSWl3aWIzZHVaWElpT2lKRVFpQlRkWEJ3YjNKMElGUnlZV2x1YVc1bklpd2lZM0psWVhSbFpDSTZJakl3TWpNdE1USXRNRFpVTURBNk1EQmFJaXdpWlhod2FYSmxjeUk2SWpJd01qUXRNRFl0TURaVU1EQTZNREJhSWl3aWRIbHdaU0k2SWs5R1JreEpUa1VpTENKbFpHbDBhVzl1SWpvaVJVNVVSVkpRVWtsVFJTSXNJbk55WXlJNld5SkJURXdpWFN3aVpITjBJanBiSWtGTVRDSmRmUT09Lk1HWUNNUUQ1SHVybWJrRm1ucURpR3lmY0J2dzVWQ2t5amJsTjFOM1pMc2pqdmlrRVdjOVVPZzZ3OEhUdHNCdmFFOWVBWjV3Q01RQ1lSNEtNTzZfYXhjMlF6RGZyTS1lenotTWFpcngyRnV3eEhSWkxQdjJzUEdaOVJ5UUl5cnpobldYZVRnVE1PdFk9Igp9",
     "nuid": "28b8555f-62d1-4e3f-bd83-80316798ef69",
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Arcion License",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Experiment ID": {
     "currentValue": "",
     "nuid": "730f4957-14a3-4127-bdb7-916f53691f22",
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Experiment ID",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
