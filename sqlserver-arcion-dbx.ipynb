{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a3e62d-a594-4d8e-b979-55f5d7162955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "Welcome to a demo of snapshot and real time replication to Databricks.\n",
    "\n",
    "Use this notebook customized schema, data, workload, and **legacy** Arcion.\n",
    "\n",
    "**NOTE**: **Databricks Personal Access Token** and **Arcion License** are required. \n",
    "\n",
    "- Initial Setup\n",
    "  - Open `Table of Contents` (Outline)\n",
    "  - Enter `Arcion License`\n",
    "  - Enter `Databricks Personal Access Token`\n",
    "  - Click `Run All`\n",
    "  - Click `View` -> `Results Only`\n",
    "  - Click `View` -> `Web Terminal`, \n",
    "    - enter `tmux attach`.  \n",
    "      - If fails with `session not found`, then wait a bit retry.\n",
    "    - In the `tmux`'s console window, `htop` will be displayed during the setup.\n",
    "    - Once the setup is complete, Arcion snapshot summary will be displayed.\n",
    "    - Wait for the setup to finish and the snapshot to complete. \n",
    "    - Takes about 5 minutes in for the setup to finish.\n",
    "- Iterate with the following:\n",
    "  - Configure Schema and Data\n",
    "  - Configure Workload\n",
    "  - Configure Arcion\n",
    "\n",
    "## Where is Data in Databricks\n",
    "  - Spark (Delta Lake) uses **Hive Meta Store** catalog: \n",
    "    - Open new tab Catalog -> hive_metastore -> <your username>\n",
    "    - find ycsbdense and ycsbsparse tables \n",
    "  - Lakehouse uses **Unity Catalog** catalog: \n",
    "    - Open new tab Catalog -> <your username> \n",
    "    - find ycsbdense and ycsbsparse tables \n",
    "\n",
    "## Frequent Demo Configurations\n",
    "- Step 1\n",
    "  - Click Real-Time\n",
    "  - Run just Arcion\n",
    "  - Change YCSB Size\n",
    "  - Watch real-time performance\n",
    "- Step 2\n",
    "  - Click Unity Catalog target\n",
    "  - Select full replication mode\n",
    "  - Run just Arcion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8484e2-ba04-4128-aa71-02f4fb01c5f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Personal Compute Cluster\n",
    "\n",
    "Choose at least 16GB of RAM for a demo.\n",
    "\n",
    "Processes use RAM.  The following is the minimum RAM usage.  The server needs enough RAM to avoid swapping.\n",
    "- Databricks: 5GB \n",
    "- SQL Server: 2GB\n",
    "- Arcion: 10% of server RAM.\n",
    "\n",
    "Note:\n",
    "- `vmstat 5`.  any non zero metrics under the `si` and `so` columns (swap in and swap out) indicate RAM shortage. \n",
    "- DBR 13 does not print output of subprocess.run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2746520-15e7-48c2-9521-d397d1c993d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: file-read-backwards in /opt/homebrew/lib/python3.10/site-packages (3.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: deepdiff in /opt/homebrew/lib/python3.10/site-packages (6.7.1)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /opt/homebrew/lib/python3.10/site-packages (from deepdiff) (4.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install file-read-backwards\n",
    "%pip install deepdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cca325-b2f8-4be4-aaf6-862be36fe237",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prep python env\n",
    "import subprocess\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import requests\n",
    "import deepdiff\n",
    "from ipywidgets import HBox, VBox, Label\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "# all exp parameters \n",
    "def exp_params():\n",
    "    all_params={\n",
    "    # arcion\n",
    "    \"arcion_download_url\": arcion_download_url.value,\n",
    "    \"srcdb_arc_user\": src_username.value,\n",
    "    \"repl_type\": repl_mode.value,\n",
    "    \"replicant_memory_percentage\": ram_percent.value,\n",
    "    \"srcdb_snapshot_threads\": snapshot_threads.value,\n",
    "    \"srcdb_realtime_threads\": realtime_threads.value, \n",
    "    \"srcdb_delta\": delta_threads.value,\n",
    "    \"dstdb_type\": dbx_destinations.value,\n",
    "    \"dstdb_stage\": dbx_staging.value,\n",
    "    \"dbx_spark_url\": dbx_spark_url.value,\n",
    "    \"dbx_databricks_url\": dbx_databricks_url.value,\n",
    "    \"dbx_hostname\": dbx_hostname.value,\n",
    "    \"dbx_dbfs_root\": dbx_username.value,\n",
    "    \"dbx_username\": dbx_username.value,\n",
    "\n",
    "    # schema and data\n",
    "    \"sparse_cntstart\": sparse_cntstart.value,\n",
    "    \"sparse_cnt\": sparse_cnt.value , \n",
    "    \"sparse_fieldcount\": sparse_fieldcount.value, \n",
    "    \"sparse_fieldlength\": sparse_fieldlength.value, \n",
    "    \"sparse_recordcount\": sparse_recordcount.value, \n",
    "    \"sparse_fillpct_start\": sparse_fillpct.value[0],\n",
    "    \"sparse_fillpct_end\": sparse_fillpct.value[1],\n",
    "    \"dense_cntstart\": dense_cntstart.value, \n",
    "    \"dense_cnt\": dense_cnt.value, \n",
    "    \"dense_fieldcount\": dense_fieldcount.value, \n",
    "    \"dense_fieldlength\": dense_fieldlength.value, \n",
    "    \"dense_recordcount\": dense_recordcount.value, \n",
    "    \"dense_fillpct_start\": dense_fillpct.value[0],\n",
    "    \"dense_fillpct_end\": dense_fillpct.value[1],\n",
    "\n",
    "    # workload\n",
    "    \"sparse_tps\": sparse_tps.value,\n",
    "    \"dense_tps\": dense_tps.value,\n",
    "    \"sparse_threads\": sparse_threads.value,\n",
    "    \"dense_threads\": dense_threads.value,\n",
    "    \"sparse_multiUpdateSize\": sparse_multiupdatesize.value,\n",
    "    \"sparse_multiInsertSize\": sparse_multiinsertsize.value,\n",
    "    \"sparse_multiDeleteSize\": sparse_multideletesize.value,\n",
    "    \"dense_multiUpdateSize\": dense_multiupdatesize.value,\n",
    "    \"dense_multiInsertSize\": dense_multiinsertsize.value,\n",
    "    \"dense_multiDeleteSize\": dense_multideletesize.value,\n",
    "    }\n",
    "\n",
    "    # cluster\n",
    "    try:\n",
    "        all_params[\"spark.databricks.clusterUsageTags.clusterNodeType\"] = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\")\n",
    "        all_params[\"spark.databricks.clusterUsageTags.cloudProvider\"]  =  spark.conf.get(\"spark.databricks.clusterUsageTags.cloudProvider\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return(all_params)\n",
    "\n",
    "# used to start new MLFlow when parameters changes \n",
    "try:\n",
    "    mlflow_proc_state\n",
    "except:\n",
    "    mlflow_proc_state={}\n",
    "\n",
    "try:\n",
    "    previous_exp_params\n",
    "except:\n",
    "    previous_exp_params={}\n",
    "try:\n",
    "    current_exp_params\n",
    "except:\n",
    "    current_exp_params={}\n",
    "\n",
    "try:\n",
    "    ycsb_logfile_positions\n",
    "except:\n",
    "    ycsb_logfile_positions={}\n",
    "try:\n",
    "    ycsb_metrics\n",
    "except:\n",
    "    ycsb_metrics={}\n",
    "try:\n",
    "    previous_log_time\n",
    "except:\n",
    "    previous_log_time=None    \n",
    "\n",
    "# arcion statistics CSV\n",
    "arcion_stats_csv_header_lines=\"catalog_name,schema_name,table_name,snapshot_start_range,snapshot_end_range,start_time,end_time,insert_count,update_count,upsert_count,delete_count,elapsed_time_sec,replicant_lag,total_lag\"\n",
    "arcion_key_index={'insert_count':7,'update_count':8,'upsert_count':9,'delete_count':10,'elapsed_time_sec':11,'replicant_lag':12,'total_lag':13}\n",
    "\n",
    "try:\n",
    "    arcion_stats_csv_positions\n",
    "except:\n",
    "    arcion_stats_csv_positions={}\n",
    "\n",
    "# setup GUI elements\n",
    "\n",
    "repl_mode = widgets.Dropdown(options=['snapshot', 'real-time', 'full'],value='real-time',\n",
    "    description='Replication:',\n",
    ")\n",
    "cdc_mode = widgets.Dropdown(options=['change', 'cdc'],value='change',\n",
    "    description='CDC Method:',\n",
    ")\n",
    "ram_percent = widgets.BoundedIntText(value=10,min=10,max=80,\n",
    "    description='RAM %:',\n",
    ")\n",
    "\n",
    "snapshot_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Snapshot Threads:',\n",
    ")\n",
    "\n",
    "realtime_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Real Time Threads:',\n",
    ")    \n",
    "\n",
    "delta_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Delta Snapshot Threads:',\n",
    ")    \n",
    "\n",
    "dbx_destinations = widgets.Dropdown(options=['null', 'deltalake', 'unitycatalog'],value='null',\n",
    "    description='Destinations:',\n",
    ")\n",
    "dbx_staging = widgets.Dropdown(options=['dbfs'],value='dbfs',\n",
    "    description='Staging:',\n",
    ")\n",
    "\n",
    "sparse_cnt = widgets.BoundedIntText(value=4,min=1,max=1000,\n",
    "    description='Tbl End:',\n",
    ")\n",
    "sparse_cntstart = widgets.BoundedIntText(value=1,min=1,max=1000,\n",
    "    description='Tbl Start:',\n",
    ")\n",
    "\n",
    "sparse_fieldcount = widgets.BoundedIntText(value=50,min=0,max=9000,\n",
    "    description='# of Fields:',\n",
    ")\n",
    "sparse_fieldlength = widgets.BoundedIntText(value=10,min=1,max=1000,\n",
    "    description='Field Len:',\n",
    ")\n",
    "\n",
    "sparse_tps = widgets.BoundedIntText(value=2000,min=0,max=10000,\n",
    "    description='TPS:',\n",
    ")\n",
    "sparse_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Threads:',\n",
    ")\n",
    "sparse_recordcount = widgets.Text(value=\"1M\",\n",
    "    description='Rec Cnt:',\n",
    ")\n",
    "\n",
    "sparse_fillpct = widgets.IntRangeSlider(value=[0,0],min=0,max=100,step=1,\n",
    "    description='Fill Range:', orientation='horizontal', readout=False\n",
    ")\n",
    "\n",
    "dense_cnt = widgets.BoundedIntText(value=2,min=1,max=1000,\n",
    "    description='Tbl End:',\n",
    ")\n",
    "dense_cntstart = widgets.BoundedIntText(value=1,min=1,max=1000,\n",
    "    description='Tbl Start:',\n",
    ")\n",
    "\n",
    "dense_fieldcount = widgets.BoundedIntText(value=10,min=0,max=9000,\n",
    "    description='# of Fields:',\n",
    ")\n",
    "dense_fieldlength = widgets.BoundedIntText(value=100,min=1,max=1000,\n",
    "    description='Field Len:',\n",
    ")\n",
    "dense_recordcount = widgets.Text(value=\"100K\",\n",
    "    description='Rec Cnt:',\n",
    ")\n",
    "\n",
    "dense_tps = widgets.BoundedIntText(value=2000,min=0,max=10000,\n",
    "    description='TPS:',\n",
    ")\n",
    "dense_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Threads:',\n",
    ")\n",
    "\n",
    "delupdins_proportion = widgets.IntRangeSlider(value=[1,999],min=0,max=1000,step=1,\n",
    "    description='Del Upd Ind:', orientation='horizontal', readout=True\n",
    ")\n",
    "\n",
    "dense_multiupdatesize = widgets.BoundedIntText(value=1024,min=0,max=10240, description='Upd Size:')\n",
    "dense_multiinsertsize = widgets.BoundedIntText(value=0,min=0,max=10240, description='Ins Size:')\n",
    "dense_multideletesize = widgets.BoundedIntText(value=0,min=0,max=10240, description='Del Size:')\n",
    "\n",
    "sparse_multiupdatesize = widgets.BoundedIntText(value=1024,min=0,max=10240, description='Upd Size:')\n",
    "sparse_multiinsertsize = widgets.BoundedIntText(value=0,min=0,max=10240, description='Ins Size:')\n",
    "sparse_multideletesize = widgets.BoundedIntText(value=0,min=0,max=10240, description='Del Size:')\n",
    "\n",
    "dense_fillpct = widgets.IntRangeSlider(value=[1,99],min=0,max=100,step=1,\n",
    "    description='Fill Range:', orientation='horizontal', readout=False\n",
    ")\n",
    "\n",
    "dbx_spark_url = widgets.Textarea(value='',\n",
    "    description='Spark URL:',\n",
    ")\n",
    "\n",
    "dbx_databricks_url = widgets.Textarea(value='',\n",
    "    description='Databricks URL:',\n",
    ")\n",
    "\n",
    "dbx_hostname = widgets.Textarea(value='',\n",
    "    description='Hostname:',\n",
    ")\n",
    "\n",
    "src_username = widgets.Textarea(value='',\n",
    "    description='SRC User:',\n",
    ")\n",
    "\n",
    "dbx_username = widgets.Textarea(value='',\n",
    "    description='DST User:',\n",
    ")\n",
    "\n",
    "arcion_license = widgets.Textarea(value='',\n",
    "    description='Lic',\n",
    ")\n",
    "\n",
    "arcion_download_url = widgets.Textarea(value='https://arcion-releases.s3.us-west-1.amazonaws.com/general/replicant/replicant-cli-24.01.25.7.zip',\n",
    "    description='Download URL',\n",
    ")\n",
    "\n",
    "dbx_access_token = widgets.Password(value='',\n",
    "    description='Access Token',\n",
    ")\n",
    "\n",
    "dbx_default_catalog = widgets.Textarea(value='',\n",
    "    description='HMS Catalog',\n",
    ")\n",
    "\n",
    "\n",
    "# cluster where the notebook is running to auto populate the destinations\n",
    "spark_url=\"\"\n",
    "databricks_url=\"\"\n",
    "workspaceUrl=\"\"\n",
    "username=\"\"\n",
    "try:\n",
    "    cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "    workspace_id =spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n",
    "\n",
    "    # clusterName = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\")\n",
    "\n",
    "    workspaceUrl = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['tags']['browserHostName']\n",
    "\n",
    "    # below does not work on GCP\n",
    "    # sc.getConf().getAll() to see what is avail\n",
    "    # workspaceUrl = spark.conf.get(\"spark.databricks.workspaceUrl\") # host name\n",
    "\n",
    "    http_path = f\"sql/protocolv1/o/{workspace_id}/{cluster_id}\"\n",
    "\n",
    "    spark_url=f\"jdbc:spark://{workspaceUrl}:443/default;transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;\"\n",
    "    databricks_url=f\"jdbc:databricks://{workspaceUrl}:443/default;transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;\"\n",
    "\n",
    "except:\n",
    "    pass\n",
    "dbx_spark_url.value = spark_url\n",
    "dbx_databricks_url.value = databricks_url\n",
    "dbx_hostname.value = workspaceUrl\n",
    "\n",
    "try:\n",
    "    username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "    dbx_username.value = re.sub('[.@]','_',username)\n",
    "    src_username.value = re.sub('[.@]','_',username)\n",
    "except:\n",
    "    src_username.value='arcsrc'\n",
    "    dbx_username.value='arcdst'\n",
    "\n",
    "try:\n",
    "    dbx_default_catalog.value=spark.conf.get(\"spark.databricks.sql.initial.catalog.name\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check arcion license via os env\n",
    "try:\n",
    "    arclicenv=os.environ[\"ARCION_LICENSE\"]\n",
    "    if arclicenv != \"\": \n",
    "        arcion_license.value=arclicenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check arcion license via dbx widget\n",
    "try:\n",
    "    arclicwidget=dbutils.widgets.get(\"Arcion License\")\n",
    "    if arclicwidget != \"\": \n",
    "        arcion_license.value=arclicwidget\n",
    "        arcion_license.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check access token via dbx widget\n",
    "try:\n",
    "    acctokwidget=dbutils.widgets.get(\"Access Token\")\n",
    "    if acctokwidget != \"\": \n",
    "        dbx_access_token.value=acctokwidget\n",
    "        dbx_access_token.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check dpkg dir via dbx widget\n",
    "pkg_src_dir=widgets.Textarea(value='',\n",
    "    description='Pkg Src Dir:',\n",
    ")\n",
    "try:\n",
    "    pkgsrcdirwidget=dbutils.widgets.get(\"Package Source Dir\")\n",
    "    if pkgsrcdirwidget != \"\": \n",
    "        pkg_src_dir.value=pkgsrcdirwidget\n",
    "        pkg_src_dir.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check if os env has ARCION_LICENSE\n",
    "try:\n",
    "    arclicenv=os.getenv('ARCION_LICENSE')\n",
    "    if arclicenv != \"\": \n",
    "        arcion_license.value=arclicenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# gcp does not change cwd to notebook path\n",
    "pwd_result= subprocess.run(f\"\"\"pwd\"\"\",capture_output = True, text = True )\n",
    "if (pwd_result.stdout == \"/databricks/driver\\n\"):\n",
    "    notebookpath=\"/Workspace\" + str(pathlib.Path(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()).parent)\n",
    "else:\n",
    "    notebookpath = None\n",
    "\n",
    "# optional MLflow\n",
    "experiment_id=None\n",
    "try:\n",
    "    import mlflow\n",
    "    experiment_id=dbutils.widgets.get(\"Experiment ID\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# src_db\n",
    "src_db_type = widgets.Dropdown(value='SQL Server', options=['MySQL', 'Postgres', 'SQL Server'])\n",
    "src_db_host = widgets.Text(value='localhost', placeholder='hostname or IP')\n",
    "src_db_port = widgets.Text(value='', placeholder='port #')\n",
    "src_db_user = widgets.Text(value='', placeholder='username')\n",
    "src_db_pass = widgets.Text(value='', placeholder='user password')\n",
    "src_db_root_user = widgets.Text(value='', placeholder='root username')\n",
    "src_db_root_pass = widgets.Text(value='', placeholder='root password')\n",
    "\n",
    "# dst_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872e49ec-d202-4d22-8a13-7f4f6ce38b62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup\n",
    "  - Enter `Arcion License`\n",
    "  - Enter `Personal Access Token` (generate **One Day** and delete afterwards)\n",
    "  - Click **Menu Bar** ->  Run -> Run All Below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995664f9-a450-4da8-980d-e54a19214565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a746ba2-57a0-4a80-b8e0-4e8fdd0254ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09b7868029c42978d8c4e6d80a9c276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Arcion'), Textarea(value='ewogICJsaWNlbnNlIiA6IHsKICAgICJ1dWlkIiA6I…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enter license and DBX personal access token\n",
    "VBox([HBox([Label('Arcion'), arcion_license, arcion_download_url]),\n",
    "      HBox([Label('Local Stage'), pkg_src_dir]),\n",
    "      HBox([Label('DBX'), dbx_access_token, dbx_default_catalog]),\n",
    "      HBox([Label('Username'), src_username, dbx_username]),\n",
    "      HBox([Label('Workspace'), dbx_spark_url, dbx_databricks_url, dbx_hostname, ]),\n",
    "       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c848f2d-7c81-4bd2-a56e-ababeed2962c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a208371c-d54e-4158-81be-16483107efa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmux session ready. session arcdst already exists\n",
      "installing apt-utils\n",
      "installing mssql-server\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "open terminal failed: not a terminal\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "100   983  100   983    0     0   2611      0 --:--:-- --:--:-- --:--:--  2607\n",
      "curl: Failed writing body\n",
      "bin/install-sqlserver.sh: line 52: lsb_release: command not found\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "100   983  100   983    0     0   3958      0 --:--:-- --:--:-- --:--:--  3947\n",
      "curl: Failed writing body\n",
      "bin/install-sqlserver.sh: line 66: lsb_release: command not found\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "100    14  100    14    0     0     40      0 --:--:-- --:--:-- --:--:--    40\n",
      "curl: Failed writing body\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing mssql-tools18\n",
      "installing unixodbc-dev\n",
      "sqlserver start failed. 1\n",
      "deltalake /opt/stage/libs/SparkJDBC42.jar found\n",
      "lakehouse  /opt/stage/libs/DatabricksJDBC42.jar found\n",
      "postgres  /opt/stage/libs/postgresql-42.7.1.jar found\n",
      "mariadb  /opt/stage/libs/mariadb-java-client-3.3.2.jar found\n",
      "oracle /opt/stage/libs/ojdbc8.jar found\n",
      "log4j /opt/stage/libs/log4j-1.2.17.jar found\n",
      "sqlserver /opt/stage/libs/mssql-jdbc-12.6.1.jre8.jar found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: unknown user mssql\n",
      "sudo: error initializing audit plugin sudoers_audit\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  396M  100  396M    0     0  3137k      0  0:02:09  0:02:09 --:--:-- 3464k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcion   downloaded\n",
      "checking jar(s) in /opt/stage/arcion/replicant-cli/lib for updates\n",
      "checking jar(s) in /opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/lib for updates\n",
      "'/opt/stage/libs//SparkJDBC42.jar' -> '/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/lib/./SparkJDBC42.jar'\n",
      "'/opt/stage/libs//ojdbc8.jar' -> '/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/lib/./ojdbc8.jar'\n",
      "'/opt/stage/libs//log4j-1.2.17.jar' -> '/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/lib/./log4j-1.2.17.jar'\n",
      "'/opt/stage/libs//DatabricksJDBC42.jar' -> '/opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/lib/./DatabricksJDBC42.jar'\n",
      "setting /opt/stage/arcion/replicant.lic from $ARCION_LICENSE\n",
      "{\n",
      "  \"license\" : {\n",
      "    \"uuid\" : \"e87e0e0f-1b26-4117-aa86-e490a8f64f92\",\n",
      "    \"owner\" : \"Robert Lee\",\n",
      "    \"created\" : \"2023-02-22T00:00Z\",\n",
      "    \"expires\" : \"2024-02-22T00:00Z\",\n",
      "    \"type\" : \"OFFLINE\",\n",
      "    \"edition\" : \"ENTERPRISE\",\n",
      "    \"src\" : [ \"ALL\" ],\n",
      "    \"dst\" : [ \"ALL\" ]\n",
      "  },\n",
      "  \"key\" : \"eyJ1dWlkIjoiZTg3ZTBlMGYtMWIyNi00MTE3LWFhODYtZTQ5MGE4ZjY0ZjkyIiwib3duZXIiOiJSb2JlcnQgTGVlIiwiY3JlYXRlZCI6IjIwMjMtMDItMjJUMDA6MDBaIiwiZXhwaXJlcyI6IjIwMjQtMDItMjJUMDA6MDBaIiwidHlwZSI6Ik9GRkxJTkUiLCJlZGl0aW9uIjoiRU5URVJQUklTRSIsInNyYyI6WyJBTEwiXSwiZHN0IjpbIkFMTCJdfQ==.MGUCMFZdVk606QJzZhhgmNqqPiG9ZhSBT0psVGWVHesH_6GSMlX9O4qSiqkBJPYG_eo_0AIxALH0ICeebNWIdxdGRaGGFL5PVGs0_5ttTljBZJqccyrk8n-UlDJ6RVPuLPBmWT-nrw==\"\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 1: bin/install-ycsb.sh: Permission denied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/stage/arcion/replicant-cli/bin/replicant 23.09.29.11 23.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: SQL Server is not running.\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: waiting for db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: waiting for db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: waiting for db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: waiting for db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5: waiting for db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6: waiting for db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: waiting for db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8: waiting for db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: waiting for db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: waiting for db\n",
      "/opt/stage/arcion/replicant-cli/bin/replicant 23.09.29.11 23.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: SQL Server is not running.\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 42: /create_user.sql: Read-only file system\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n",
      "cat: /create_user.sql: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating user arcsrc\n",
      "/opt/stage/arcion/replicant-cli/bin/replicant 23.09.29.11 23.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: SQL Server is not running.\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 348: sqlcmd: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prometheus already downloaded\n",
      "prometheus node_exporter already downloaded\n",
      "prometheus sql_exporter being downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 24.3M  100 24.3M    0     0  4987k      0  0:00:04  0:00:04 --:--:-- 5833k\n",
      "x sql_exporter-0.14.0.darwin-arm64/\n",
      "x sql_exporter-0.14.0.darwin-arm64/README.md\n",
      "x sql_exporter-0.14.0.darwin-arm64/mssql_standard.collector.yml\n",
      "x sql_exporter-0.14.0.darwin-arm64/sql_exporter"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started /opt/stage/prom/sql_exporter-0.14.0.linux-amd64/sql_exporter.  log at /var/tmp/arcsrc/sqlserver/logs/sql_exporter.log\n",
      "started /opt/stage/prom/node_exporter-1.7.0.linux-amd64/node_exporter.  log at /var/tmp/arcsrc/sqlserver/logs/node_exporter.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "x sql_exporter-0.14.0.darwin-arm64/LICENSE\n",
      "x sql_exporter-0.14.0.darwin-arm64/sql_exporter.yml\n",
      "sed: /opt/stage/prom/sql_exporter-0.14.0.linux-amd64/sql_exporter.yml: No such file or directory\n",
      "bin/install-prometheus.sh: line 79: pushd: /opt/stage/prom/sql_exporter-0.14.0.linux-amd64: No such file or directory\n",
      "bin/install-prometheus.sh: line 83: popd: directory stack empty\n",
      "bin/install-prometheus.sh: line 81: /var/tmp/arcsrc/sqlserver/logs/sql_exporter.log: No such file or directory\n",
      "bin/install-prometheus.sh: line 89: /var/tmp/arcsrc/sqlserver/logs/node_exporter.log: No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='export SRCDB_ARC_USER=arcsrc; bin/install-prometheus.sh', returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup tmux, arcion, ycsb, sql server\n",
    "subprocess.run(f\"\"\". ./bin/setup-tmux.sh; setup_tmux '{dbx_username.value}'\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/download-jars.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"ARCION_LICENSE='{arcion_license.value}' ARCION_DOWNLOAD_URL='{arcion_download_url.value}' bin/install-arcion.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/install-ycsb.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "\n",
    "# mysql\n",
    "\n",
    "# pg\n",
    "\n",
    "\n",
    "# sqlserver\n",
    "subprocess.run(f\"\"\"SQL_SERVER_DPKG='{pkg_src_dir.value}'; bin/install-sqlserver.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; ping_sql_cli;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; create_user;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; set_sqlserver_ram '{dbx_username.value}';\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; bin/install-prometheus.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427dc8c9-96f3-4ac6-bf41-4efd371f3b04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Schema and Data\n",
    "\n",
    "Existing tables will be appended with additional rows if the `Fill Range` is the same.  \n",
    "Increase the `Table Count` to create additional tables.  \n",
    "\n",
    "The following options are available:\n",
    "- Table count (Table Cnt): The number of tables to create.  \n",
    "  - Table names are `ycsbdense`, `ycsbdense2`, `ycsbdense3`, ... and `ycssparse`, `ycsbdense2`, and `ycsbdense3` ...\n",
    "- Number of Fields (# of Fields): The number of fields per table.  \n",
    "  - The field names are `FIELD0`, `FIELD1`, `FIELD2`, ...\n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Field Length (Field Len): The length of random character data populated per field.  \n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Record Count (Rec Cnt): The number of records per table generated.\n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Fill Range: The relative start and end range of fields that are populated with data.  Be default: \n",
    "    - sparse tables are all NULLs by having the fill range be 0% to 0% ranges\n",
    "    - dense tables have all fields populated by having the fill range be 0% to 100% of ranges \n",
    "\n",
    "```sql\n",
    "[localhost][arcsrc] 1> \\describe ycsbsparse\n",
    "+-------------+-------------+-----------+-------------+----------------+-------------+\n",
    "| TABLE_SCHEM | COLUMN_NAME | TYPE_NAME | COLUMN_SIZE | DECIMAL_DIGITS | IS_NULLABLE |\n",
    "+-------------+-------------+-----------+-------------+----------------+-------------+\n",
    "| dbo         | YCSB_KEY    | int       |          10 |              0 | NO          |\n",
    "| dbo         | FIELD0      | text      |  2147483647 |         [NULL] | YES         |\n",
    "| dbo         | FIELD1      | text      |  2147483647 |         [NULL] | YES         |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd09cab-c63e-4195-9364-09011fd2b559",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "Make changes below and click `Run All Below`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43cd33ea4fb448e9b7d56bb032c1d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value=''), Label(value='Type'), Label(value='Host'), Label(value='Port'), …"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VBox([\n",
    "    HBox([Label(''),Label('Type'),Label('Host'),Label('Port'),Label('Username'),Label('User Password'),Label('Root User'),Label('Root Password')]),\n",
    "    HBox([Label('SRC'),src_db_type,src_db_host,src_db_port,src_db_user,src_db_pass,src_db_root_user,src_db_root_pass]),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c315fc-624c-442c-b712-518336f5bfd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f415a54dcc9a4065b437e315b6954a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Sparse'), BoundedIntText(value=1, description='Tbl Start:', max=100…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show YCSB Data Controls\n",
    "VBox([HBox([Label('Sparse'), sparse_cntstart,sparse_cnt, sparse_fieldcount, sparse_fieldlength, sparse_recordcount, sparse_fillpct]),\n",
    "    HBox([Label('Dense'),  dense_cntstart, dense_cnt, dense_fieldcount, dense_fieldlength, dense_recordcount, dense_fillpct])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370b278c-17fe-4f68-ac8f-aa0ee8c18568",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7f7c0f-9f1d-40e3-a4f8-a02d6a42bf0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/stage/arcion/replicant-cli/bin/replicant 23.09.29.11 23.09\n",
      "starting sparse load. /ycssparse.load.log"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: SQL Server is not running.\n",
      "touch: cannot touch '/ycsbsparse.load.log': Read-only file system\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 2 3 4\n",
      "starting dense load. /ycsbdense.load.log 1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 126: /ycsbsparse.load.log: Read-only file system\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 126: /ycsbsparse.load.log: Read-only file system\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 126: /ycsbsparse.load.log: Read-only file system\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 126: /ycsbsparse.load.log: Read-only file system\n",
      "touch: cannot touch '/ycsbdense.load.log': Read-only file system\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 106: /ycsbdense.load.log: Read-only file system\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 106: /ycsbdense.load.log: Read-only file system\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 540: /schema_dump.csv: Read-only file system\n",
      "schema dump at /schema_dump.csv\n",
      "./demo/sqlserver/run-ycsb-sqlserver-source.sh: line 331: sqlcmd: command not found\n",
      "touch: cannot touch '/list_table_counts.sql': Read-only file system\n",
      "table count at /list_table_counts.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/var/tmp/arcsrc/sqlserver/config/list_table_counts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m      2\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mexport SRCDB_ARC_USER=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_username\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    y_fieldcount=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_fieldcount\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m    list_table_counts\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,executable\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbash\u001b[39m\u001b[38;5;124m\"\u001b[39m,cwd\u001b[38;5;241m=\u001b[39mnotebookpath) \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# show tables\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/var/tmp/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msrc_username\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/sqlserver/config/list_table_counts.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfield count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/var/tmp/arcsrc/sqlserver/config/list_table_counts.csv'"
     ]
    }
   ],
   "source": [
    "# run load_sparse_data_cnt and load_dense_data_cnt \n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; \n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    y_fieldcount={sparse_fieldcount.value} \n",
    "    y_fieldlength={sparse_fieldlength.value}  \n",
    "    y_recordcount={sparse_recordcount.value} \n",
    "    y_fillstart={math.ceil((sparse_fillpct.value[0] * sparse_fieldcount.value) / 100)}      \n",
    "    y_fillend={int((sparse_fillpct.value[1] * sparse_fieldcount.value) / 100)}      \n",
    "    load_sparse_data_cnt {sparse_cnt.value} {sparse_cntstart.value};\n",
    "    y_fieldcount={dense_fieldcount.value} \n",
    "    y_fieldlength={dense_fieldlength.value} \n",
    "    y_recordcount={dense_recordcount.value} \n",
    "    y_fillstart={math.ceil((dense_fillpct.value[0] * dense_fieldcount.value) / 100)}      \n",
    "    y_fillend={int((dense_fillpct.value[1] * dense_fieldcount.value) / 100)}      \n",
    "    load_dense_data_cnt {dense_cnt.value} {dense_cntstart.value};\n",
    "    dump_schema;\n",
    "    list_table_counts\"\"\",\n",
    "    shell=True,executable=\"bash\",cwd=notebookpath) \n",
    "# show tables\n",
    "pd.read_csv (f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\",header=None, names= ['table name','min key','max key','field count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1aeafcc-3471-47b9-b59d-b14016329940",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Workload\n",
    "\n",
    "Choose the options in the UI and run the cell below it to start the workload (YCSB).  \n",
    "\n",
    "YCSB update (workload A) controls for Dense and Sparse table groups separated. Each group has a separate control.  However, all of the tables in the group use the same controls.  \n",
    "1. Each table's TPS (throughput per second)\n",
    "   1. 0=fast as possible\n",
    "   2. 1=1 TPS\n",
    "   3. 10=10 TPS\n",
    "2. Each table's threads (concurrency) used to achieve the desired TPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e009b417-01dc-4855-b557-3a1cd88f70c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3233053-e7a3-4c56-8544-6d75c718f1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# show YCSB run controls\n",
    "VBox([HBox([Label('DML Ratio'), delupdins_proportion]),\n",
    "      HBox([Label('Sparse'), sparse_tps, sparse_threads, sparse_multiupdatesize, sparse_multiinsertsize, sparse_multideletesize]), \n",
    "      HBox([Label('Dense'),  dense_tps, dense_threads, dense_multiupdatesize, dense_multiinsertsize, dense_multideletesize])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1920d9d-bae0-49b0-adab-554352dcc7a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33058c18-c9e7-46f2-ab85-618136369430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# start/restart YCSB run\n",
    "# 1 = 0.1% of total TPS = delete.  Total of 0.1% * 100 = 10% (up to this many records are can be deleted)\n",
    "if (delupdins_proportion.value[0] > 1):\n",
    "    delupdins_proportion.value = [1,delupdins_proportion.value[1]]\n",
    "y_del_proportion=(delupdins_proportion.value[0]) / 1000.0\n",
    "y_upd_proportion=(delupdins_proportion.value[1] - delupdins_proportion.value[0]) / 1000.0\n",
    "y_ins_proportion=(1000 - delupdins_proportion.value[1]) / 1000.0\n",
    "# set the min TPS to match multi row size.  Otherwise, txn will to to fill up the size\n",
    "min_tps=max(sparse_multiupdatesize.value, sparse_multideletesize.value, sparse_multiupdatesize.value)\n",
    "if (sparse_tps.value < min_tps):\n",
    "    sparse_tps.value = min_tps\n",
    "min_tps=max(dense_multiupdatesize.value, dense_multideletesize.value, dense_multiupdatesize.value)    \n",
    "if (dense_tps.value < min_tps):\n",
    "    dense_tps.value = min_tps\n",
    "# start the actual run\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value};\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    kill_ycsb;\n",
    "    list_table_counts;       \n",
    "    y_del_proportion={y_del_proportion}\n",
    "    y_upd_proportion={y_upd_proportion}\n",
    "    y_ins_proportion={y_ins_proportion}\n",
    "    y_target_sparse={sparse_tps.value} \n",
    "    y_target_dense={dense_tps.value} \n",
    "    y_threads_sparse={sparse_threads.value} \n",
    "    y_threads_dense={dense_threads.value} \n",
    "    y_multiinsertsize_dense={dense_multiinsertsize.value} \n",
    "    y_multiupdatesize_dense={dense_multiupdatesize.value} \n",
    "    y_multideletesize_dense={dense_multideletesize.value} \n",
    "    y_multiinsertsize_sparse={sparse_multiinsertsize.value} \n",
    "    y_multiupdatesize_sparse={sparse_multiupdatesize.value} \n",
    "    y_multideletesize_sparse={sparse_multideletesize.value} \n",
    "    y_fieldlength_sparse={sparse_fieldlength.value} \n",
    "    y_fieldlength_dense={dense_fieldlength.value} \n",
    "    start_ycsb;\"\"\",\n",
    "    shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6734eb-5342-48b5-868b-750218370b54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Arcion\n",
    "\n",
    "Choose the options in the UI and run the cell below it to start the replication.  \n",
    "\n",
    "The following control are avail in the demo.  \n",
    "- Arcion - replication type and CDC methods  \n",
    "- Threads - control the parallelism.\n",
    "- Target - null, unity catalog or delta lake\n",
    "\n",
    "NOTE: Full mode does not work at this time.\n",
    "\n",
    "For SQL Server, change tracking, cdc are available for demo.  \n",
    "\n",
    "Performance is mainly controlled by the thread count by the extract and apply process.\n",
    "Additional controls are customizable via modifying the YAML files directly below.\n",
    "- [CDC YAML files](./demo/sqlserver/yaml/cdc/)\n",
    "- [Change Tracking YAML files](./demo/sqlserver/yaml/change/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb753ed-5c15-4f74-9ed6-8ddf59a2ce42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "Make changes below and click `Run All Below`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d603b1d4-bb2c-418b-acfe-894441efe01e",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show Arcion and DBX controls\n",
    "VBox([\n",
    "      HBox([Label('RAM'), ram_percent]),\n",
    "      HBox([Label('Modes'), repl_mode, cdc_mode]),\n",
    "      HBox([Label('Target'), dbx_destinations, dbx_staging ]),\n",
    "      HBox([Label('Threads'), snapshot_threads, realtime_threads, delta_threads]),\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbea2ac-b6ea-4cf4-9be1-bf665a7e6ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a0ede1-4383-4f3e-b9c8-5e601c35bfcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# start/restart Arcion\n",
    "if ( f\"{dbx_access_token.value}\" == \"\" ) and ( f\"{dbx_destinations.value}\" != \"null\" ):\n",
    "    print(\"personal access token not entered.\")\n",
    "else:\n",
    "    # start a new run\n",
    "    print (f\"\"\"{cdc_mode.value} {repl_mode.value}\"\"\")\n",
    "    arcion_run = subprocess.run(f\"\"\"export ARCION_DOWNLOAD_URL='{arcion_download_url.value}';\n",
    "    export SRCDB_ARC_USER={src_username.value};\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    kill_arcion;\n",
    "    disable_cdc;\n",
    "    disable_change_tracking;\n",
    "    echo prog_dir=$PROG_DIR arcion_bin=$ARCION_BIN;\n",
    "    cd $PROG_DIR;\n",
    "    a_repltype='{repl_mode.value}'\n",
    "    REPLICANT_MEMORY_PERCENTAGE='{ram_percent.value}.0'\n",
    "    SRCDB_SNAPSHOT_THREADS='{snapshot_threads.value}' \n",
    "    SRCDB_REALTIME_THREADS='{realtime_threads.value}' \n",
    "    SRCDB_DELTA='{delta_threads.value}'\n",
    "    DSTDB_TYPE='{dbx_destinations.value}'\n",
    "    DSTDB_STAGE='{dbx_staging.value}'\n",
    "    DBX_SPARK_URL='{dbx_spark_url.value}'\n",
    "    DBX_DATABRICKS_URL='{dbx_databricks_url.value}'\n",
    "    DBX_ACCESS_TOKEN='{dbx_access_token.value}'\n",
    "    DBX_HOSTNAME='{dbx_hostname.value}'\n",
    "    DBX_DBFS_ROOT='/{dbx_username.value}'\n",
    "    DBX_USERNAME='{dbx_username.value}'\n",
    "    start_{cdc_mode.value}_arcion;\"\"\",\n",
    "    shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cafb49c3-df1b-4458-805c-b233876acf90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# MLFLow\n",
    "\n",
    "Save the artifacts in MLFlow.\n",
    "\n",
    "Artifacts are collected for 5 min (600 sec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f193b18-1efa-46be-afb3-37510d683db0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb57b96-e39c-465e-a939-2ec7aa27392a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use process to run MLflow without blocking the notebook.  thread does not work with mlflow\n",
    "\n",
    "import mlflow\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from multiprocessing import Process\n",
    "\n",
    "def log_artifacts():\n",
    "    pass\n",
    "\n",
    "from file_read_backwards import FileReadBackwards\n",
    "import datetime\n",
    "\n",
    "# convert ycsb log mlflow metric\n",
    "# time                    elapsed  cumulative      time period                                   per operations metric\n",
    "#                         sec      operations      ops/sec\n",
    "# 2024-03-07 10:05:38:240 410 sec: 409 operations; 1 current ops/sec; est completion in 116 days [UPDATE: Count=10, Max=15383, Min=6792, Avg=9264.6, 90=15359, 99=15383, 99.9=15383, 99.99=15383]\n",
    "# ycsb_tablename_[update|update-failed]_count=x\n",
    "# ycsb_tablename_[update|update-failed]_avg_microsec=x \n",
    "\n",
    "ycsb_date_time_pattern = r\"^(?P<dt>[0-9\\-]+ [0-9\\:]+)\"  # at the beginning\n",
    "ycsb_op_val_pattern = r'\\[([^]]*)\\]'                    # [Update: ] [Insert: ] ...\n",
    "\n",
    "def parse_ycsb_log_to_metric(table_name=\"ycsbsparse\",\n",
    "                    file=\"/var/tmp/arcsrc/sqlserver/logs/ycsb/ycsb.ycsbsparse.log\",\n",
    "                    previous_log_time=None, \n",
    "                    metrics={},\n",
    "                    ycsb_logfile_positions={}):\n",
    "    with FileReadBackwards(file, encoding=\"utf-8\") as ycsb_log_file:\n",
    "        count=0\n",
    "        for line in ycsb_log_file:            \n",
    "            # parse date time and bail if already processed\n",
    "            m = re.search(ycsb_date_time_pattern, line)\n",
    "            if m is None:\n",
    "                continue\n",
    "\n",
    "            log_time=datetime.datetime.strptime(m.group('dt'), '%Y-%m-%d %H:%M:%S:%f')\n",
    "            try:\n",
    "                if log_time == ycsb_logfile_positions[table_name]:\n",
    "                    return\n",
    "            except:\n",
    "                pass    \n",
    "\n",
    "            # parse [update: ...]\n",
    "            m = re.findall(ycsb_op_val_pattern, line.lower())\n",
    "            if m is None:\n",
    "                continue\n",
    "            \n",
    "            ycsb_logfile_positions[table_name] = log_time\n",
    "            for ops in m:\n",
    "                op_vals=ops.split(\":\")                  # update: ....\n",
    "                vals_array=op_vals[1].split(\",\")        # count=?, max=?, ...\n",
    "                try:    \n",
    "                    op_count=float(vals_array[0].split(\"=\")[1])    # [0] count=?\n",
    "                except:\n",
    "                    op_count=0.0\n",
    "                try:\n",
    "                    op_avg=float(vals_array[3].split(\"=\")[1])      # [1] avg=? if count=0, then this will be not defined\n",
    "                except:\n",
    "                    op_avg=0.0\n",
    "                metrics[f\"ycsb_{op_vals[0]}_count_{table_name}\"]=op_count\n",
    "                metrics[f\"ycsb_{op_vals[0]}_avg_microsec_{table_name}\"]=op_avg\n",
    "            return\n",
    "\n",
    "def get_ycsb_metrics(metrics={}):\n",
    "    ycsb_current_metrics={}\n",
    "    print(ycsb_logfile_positions)\n",
    "    ycsb_tables = pd.read_csv (f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\",header=None, names= ['table name','min key','max key','field count'])\n",
    "    for table_name in ycsb_tables['table name']:\n",
    "        table_name = table_name.lower()\n",
    "        parse_ycsb_log_to_metric(\n",
    "            table_name=table_name, \n",
    "            file=f\"/var/tmp/{src_username.value}/sqlserver/logs/ycsb/ycsb.{table_name}.log\",\n",
    "            previous_log_time=previous_log_time,\n",
    "            metrics=ycsb_current_metrics,\n",
    "            ycsb_logfile_positions=ycsb_logfile_positions)\n",
    "    return(ycsb_current_metrics)\n",
    "\n",
    "def get_prom_metrics(prom_metric_url=None,metric_prefix=\"\",metric_step=None):\n",
    "    # there is a limit on the number of metrics that you can log in a single log_batch call. This limit is typically 1000. \n",
    "    # timestamp=If unspecified, the number of milliseconds since the Unix epoch is used.\n",
    "    # step=If unspecified, the default value of zero is used\n",
    "    contents = requests.get(prom_metric_url)\n",
    "    all_metrics = {}\n",
    "    metrics_count = 0\n",
    "    for line in contents.text.splitlines():\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        key_val=line.rsplit(' ',1)  # split from the end in case the key has spaces\n",
    "        all_metrics[re.sub('[\" {}=,]',\"_\",key_val[0])]=float(key_val[1])\n",
    "        metrics_count += 1\n",
    "    return(all_metrics)\n",
    "\n",
    "\n",
    "def start_mlflow(max_intervals=5,experiment_id=None, log_interval_sec=60, all_params={}):\n",
    "    # stop previous run\n",
    "    mlflow_run = mlflow.active_run()\n",
    "    if not(mlflow_run is None):\n",
    "        # upload final artifacts\n",
    "        log_artifacts()\n",
    "        print(f\"\"\"stopping previous MLflow {mlflow_run.info.run_id}\"\"\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "    # start a new run\n",
    "    if experiment_id == '':\n",
    "        experiment_id=None\n",
    "    mlflow.start_run(experiment_id=experiment_id, log_system_metrics=True)\n",
    "\n",
    "    # params\n",
    "    mlflow.log_params(params=all_params)\n",
    "\n",
    "    # schema\n",
    "    dataset_source=f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\"\n",
    "    mlflow.log_artifact(dataset_source)\n",
    "    \n",
    "    # data\n",
    "    dataset_shape = pd.read_csv(dataset_source, header=None, names= ['table name','min key','max key','field count'])\n",
    "    dataset = mlflow.data.from_pandas(dataset_shape, source=dataset_source)\n",
    "    mlflow.log_input(dataset, context=\"training\")    \n",
    "\n",
    "    # wait to end\n",
    "    # TODO: Make this smarter by checking whether the process is still running\n",
    "    wait_count=0\n",
    "    while wait_count < max_intervals:\n",
    "        mlflow.log_metrics(metrics=get_prom_metrics(prom_metric_url=\"http://localhost:9399/metrics\"))\n",
    "        mlflow.log_metrics(metrics=get_prom_metrics(prom_metric_url=\"http://localhost:9100/metrics\"))\n",
    "        mlflow.log_metrics(metrics=get_ycsb_metrics())\n",
    "        time.sleep(log_interval_sec)\n",
    "        wait_count += 1\n",
    "\n",
    "    # upload the rest of the artifacts generated /var/tmp/{src_username.value}/sqlserver/logs\n",
    "    log_artifacts()\n",
    "    # experiment done\n",
    "    mlflow.end_run()\n",
    "\n",
    "def register_mlflow(exp_params):\n",
    "    mlflow_proc = Process(target=start_mlflow, kwargs={\"experiment_id\":experiment_id, \"all_params\":current_exp_params})\n",
    "    mlflow_proc.start()   \n",
    "    try:\n",
    "        mlflow_proc_state['proc'].terminate()\n",
    "        print(\"previous MLFlow process terminated\")\n",
    "    except:\n",
    "        pass\n",
    "    mlflow_proc_state['proc']       = mlflow_proc\n",
    "    mlflow_proc_state['exp_params'] = exp_params\n",
    "\n",
    "\n",
    "current_exp_params=exp_params()\n",
    "if not ('exp_params' in mlflow_proc_state):\n",
    "    print(\"first run of mlflow\")\n",
    "    register_mlflow(current_exp_params)\n",
    "elif current_exp_params != mlflow_proc_state['exp_params']:\n",
    "    print(\"param changed. starting new mlflow\")\n",
    "    register_mlflow(current_exp_params)\n",
    "elif not(mlflow_proc_state['proc'].is_alive()):\n",
    "    print(\"mlflow stopped. starting new mlflow with new step\")\n",
    "    register_mlflow(current_exp_params)\n",
    "else:\n",
    "    print(\"no parameters changed. New MLFLow experiment not needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7184a016-b7ef-4302-bee5-38ec30ed02f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Manually Kill Processes\n",
    "Uncomment below to kill desired processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79738332-5dec-4a1f-a0af-b2d9fc442150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# subprocess.run(f\"\"\". ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_arcion;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "# subprocess.run(f\"\"\". ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_ycsb;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9561e8-d3f2-4196-8a4d-ec42a98b78fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "def parse_arcion_stats():\n",
    "    with FileReadBackwards(\"/var/tmp/arcsrc/sqlserver/logs/3fb22d2a4/3fb22d2a4/replication_statistics_history_2024-03-07_3.CSV\", encoding=\"utf-8\") as BigFile:\n",
    "        max_start_time = None\n",
    "        all_metrics = {}\n",
    "        unprocessed_tables = arcion_stats_csv_positions.copy()\n",
    "        for line in BigFile:\n",
    "            if line==arcion_stats_csv_header_lines:\n",
    "                break\n",
    "            tokens=line.split(\",\")\n",
    "            print (line)\n",
    "\n",
    "            key=tokens[0]+\"_\"+tokens[1]+\"_\"+tokens[2]\n",
    "\n",
    "            # max start_time\n",
    "            start_time_str=tokens[5]\n",
    "            start_time=datetime.strptime(start_time_str, '%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "\n",
    "            if max_start_time is None:\n",
    "                max_start_time = start_time\n",
    "            \n",
    "            if key in arcion_stats_csv_positions:\n",
    "                if arcion_stats_csv_positions[key] < start_time:\n",
    "                    # line has newer data\n",
    "                    arcion_stats_csv_positions[key]=start_time\n",
    "                else:\n",
    "                    print(\"skip\")\n",
    "            else:\n",
    "                arcion_stats_csv_positions[key]=start_time\n",
    "\n",
    "            # continue until all tables are processed or time out\n",
    "            try:\n",
    "                del unprocessed_tables[key]\n",
    "            except:\n",
    "                pass\n",
    "            if len(unprocessed_tables) == 0:\n",
    "                break\n",
    "            if (max_start_time - start_time).seconds > 10:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Arcion License",
      "width": 184
     }
    ]
   },
   "notebookName": "sqlserver-arcion-dbx",
   "widgets": {
    "Access Token": {
     "currentValue": "dapi29ffee1b82da9b7ba3e11193abd842bf",
     "nuid": "a3fdf5f2-8290-4a28-b8ac-89bea1252303",
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Access Token",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Arcion License": {
     "currentValue": "ewogICJsaWNlbnNlIiA6IHsKICAgICJ1dWlkIiA6ICI2MWQ0N2Y4Yi0zYmI4LTRhOTktYTAwYS1mMjA2OGEzMWM0MmYiLAogICAgIm93bmVyIiA6ICJEQiBTdXBwb3J0IFRyYWluaW5nIiwKICAgICJjcmVhdGVkIiA6ICIyMDIzLTEyLTA2VDAwOjAwWiIsCiAgICAiZXhwaXJlcyIgOiAiMjAyNC0wNi0wNlQwMDowMFoiLAogICAgInR5cGUiIDogIk9GRkxJTkUiLAogICAgImVkaXRpb24iIDogIkVOVEVSUFJJU0UiLAogICAgInNyYyIgOiBbICJBTEwiIF0sCiAgICAiZHN0IiA6IFsgIkFMTCIgXQogIH0sCiAgImtleSIgOiAiZXlKMWRXbGtJam9pTmpGa05EZG1PR0l0TTJKaU9DMDBZVGs1TFdFd01HRXRaakl3TmpoaE16RmpOREptSWl3aWIzZHVaWElpT2lKRVFpQlRkWEJ3YjNKMElGUnlZV2x1YVc1bklpd2lZM0psWVhSbFpDSTZJakl3TWpNdE1USXRNRFpVTURBNk1EQmFJaXdpWlhod2FYSmxjeUk2SWpJd01qUXRNRFl0TURaVU1EQTZNREJhSWl3aWRIbHdaU0k2SWs5R1JreEpUa1VpTENKbFpHbDBhVzl1SWpvaVJVNVVSVkpRVWtsVFJTSXNJbk55WXlJNld5SkJURXdpWFN3aVpITjBJanBiSWtGTVRDSmRmUT09Lk1HWUNNUUQ1SHVybWJrRm1ucURpR3lmY0J2dzVWQ2t5amJsTjFOM1pMc2pqdmlrRVdjOVVPZzZ3OEhUdHNCdmFFOWVBWjV3Q01RQ1lSNEtNTzZfYXhjMlF6RGZyTS1lenotTWFpcngyRnV3eEhSWkxQdjJzUEdaOVJ5UUl5cnpobldYZVRnVE1PdFk9Igp9",
     "nuid": "28b8555f-62d1-4e3f-bd83-80316798ef69",
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Arcion License",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Experiment ID": {
     "currentValue": "",
     "nuid": "730f4957-14a3-4127-bdb7-916f53691f22",
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Experiment ID",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Package Source Dir": {
     "currentValue": "",
     "nuid": "f5e384ea-c669-45a4-9240-046fec8728e9",
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Package Source Dir",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
