{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a3e62d-a594-4d8e-b979-55f5d7162955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "Welcome to a demo of snapshot and real time replication to Databricks.\n",
    "\n",
    "Use this notebook customized schema, data, workload, and **legacy** Arcion.\n",
    "\n",
    "**NOTE**: **Databricks Personal Access Token** and **Arcion License** are required. \n",
    "\n",
    "- Initial Setup\n",
    "  - Open `Table of Contents` (Outline)\n",
    "  - Enter `Arcion License`\n",
    "  - Enter `Databricks Personal Access Token`\n",
    "  - Click `Run All`\n",
    "  - Click `View` -> `Results Only`\n",
    "  - Click `View` -> `Web Terminal`, \n",
    "    - enter `tmux attach`.  \n",
    "      - If fails with `session not found`, then wait a bit retry.\n",
    "    - In the `tmux`'s console window, `htop` will be displayed during the setup.\n",
    "    - Once the setup is complete, Arcion snapshot summary will be displayed.\n",
    "    - Wait for the setup to finish and the snapshot to complete. \n",
    "    - Takes about 5 minutes in for the setup to finish.\n",
    "- Iterate with the following:\n",
    "  - Configure Schema and Data\n",
    "  - Configure Workload\n",
    "  - Configure Arcion\n",
    "\n",
    "## Where is Data in Databricks\n",
    "  - Spark (Delta Lake) uses **Hive Meta Store** catalog: \n",
    "    - Open new tab Catalog -> hive_metastore -> <your username>\n",
    "    - find ycsbdense and ycsbsparse tables \n",
    "  - Lakehouse uses **Unity Catalog** catalog: \n",
    "    - Open new tab Catalog -> <your username> \n",
    "    - find ycsbdense and ycsbsparse tables \n",
    "\n",
    "## Frequent Demo Configurations\n",
    "- Step 1\n",
    "  - Click Real-Time\n",
    "  - Run just Arcion\n",
    "  - Change YCSB Size\n",
    "  - Watch real-time performance\n",
    "- Step 2\n",
    "  - Click Unity Catalog target\n",
    "  - Select full replication mode\n",
    "  - Run just Arcion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8484e2-ba04-4128-aa71-02f4fb01c5f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Personal Compute Cluster\n",
    "\n",
    "Choose at least 16GB of RAM for a demo.\n",
    "\n",
    "Processes use RAM.  The following is the minimum RAM usage.  The server needs enough RAM to avoid swapping.\n",
    "- Databricks: 5GB \n",
    "- SQL Server: 2GB\n",
    "- Arcion: 10% of server RAM.\n",
    "\n",
    "Note:\n",
    "- `vmstat 5`.  any non zero metrics under the `si` and `so` columns (swap in and swap out) indicate RAM shortage. \n",
    "- DBR 13 does not print output of subprocess.run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE=\"./env/perf1-dbo.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2746520-15e7-48c2-9521-d397d1c993d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: file-read-backwards in /home/rslee/.local/lib/python3.10/site-packages (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deepdiff in /home/rslee/.local/lib/python3.10/site-packages (6.7.1)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /home/rslee/.local/lib/python3.10/site-packages (from deepdiff) (4.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bpytop in /home/rslee/.local/lib/python3.10/site-packages (1.0.68)\n",
      "Requirement already satisfied: psutil<6.0.0,>=5.7.0 in /usr/lib/python3/dist-packages (from bpytop) (5.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mlflow in /home/rslee/.local/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: Flask<4 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (3.0.2)\n",
      "Requirement already satisfied: pandas<3 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (2.2.0)\n",
      "Requirement already satisfied: cloudpickle<4 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (3.0.0)\n",
      "Requirement already satisfied: matplotlib<4 in /usr/lib/python3/dist-packages (from mlflow) (3.5.1)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (15.0.0)\n",
      "Requirement already satisfied: querystring-parser<2 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (0.4.4)\n",
      "Requirement already satisfied: pytz<2025 in /usr/lib/python3/dist-packages (from mlflow) (2022.1)\n",
      "Requirement already satisfied: numpy<2 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (1.4.1.post1)\n",
      "Requirement already satisfied: entrypoints<1 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (3.1.3)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (4.25.2)\n",
      "Requirement already satisfied: graphene<4 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (3.3)\n",
      "Requirement already satisfied: gunicorn<22 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (21.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (2.31.0)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /usr/lib/python3/dist-packages (from mlflow) (5.0.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/lib/python3/dist-packages (from mlflow) (4.6.4)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (2.0.28)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (3.5.2)\n",
      "Requirement already satisfied: scipy<2 in /usr/lib/python3/dist-packages (from mlflow) (1.8.0)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/lib/python3/dist-packages (from mlflow) (5.4.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (8.1.7)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (3.1.42)\n",
      "Requirement already satisfied: packaging<24 in /usr/lib/python3/dist-packages (from mlflow) (21.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /home/rslee/.local/lib/python3.10/site-packages (from mlflow) (1.13.1)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/rslee/.local/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/rslee/.local/lib/python3.10/site-packages (from Flask<4->mlflow) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/rslee/.local/lib/python3.10/site-packages (from Flask<4->mlflow) (1.7.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /home/rslee/.local/lib/python3.10/site-packages (from Flask<4->mlflow) (3.0.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/rslee/.local/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in /home/rslee/.local/lib/python3.10/site-packages (from graphene<4->mlflow) (9.0.1)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /home/rslee/.local/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /home/rslee/.local/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/rslee/.local/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rslee/.local/lib/python3.10/site-packages (from pandas<3->mlflow) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/rslee/.local/lib/python3.10/site-packages (from pandas<3->mlflow) (2023.4)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.17.3->mlflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rslee/.local/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.17.3->mlflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.17.3->mlflow) (1.26.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/rslee/.local/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/rslee/.local/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/rslee/.local/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/rslee/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install file-read-backwards \n",
    "%pip install deepdiff\n",
    "%pip install bpytop\n",
    "%pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cca325-b2f8-4be4-aaf6-862be36fe237",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prep python env\n",
    "import subprocess\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import requests\n",
    "import deepdiff\n",
    "import time\n",
    "from pathlib import Path\n",
    "from ipywidgets import HBox, VBox, Label\n",
    "from file_read_backwards import FileReadBackwards\n",
    "from pylib.mlflowutils import *\n",
    "\n",
    "def nine_char_id():\n",
    "    # %s   seconds since 1970-01-01 00:00:00 UTC\n",
    "    # %N   nanoseconds (000000000..999999999)\n",
    "    return(hex(int(time.time_ns() / 100000000 ))[2:])\n",
    "\n",
    "# all exp parameters \n",
    "def exp_params():\n",
    "    all_params={\n",
    "    # run\n",
    "    \"arcion_run_id\": arcion_run_id,\n",
    "    # arcion\n",
    "    \"arcion_download_url\": arcion_download_url.value,\n",
    "    \"srcdb_arc_user\": src_username.value,\n",
    "    \"repl_type\": repl_mode.value,\n",
    "    \"extraction_method\": extraction_method.value,\n",
    "    \"replicant_memory_percentage\": ram_percent.value,\n",
    "    \"srcdb_snapshot_threads\": snapshot_threads.value,\n",
    "    \"srcdb_realtime_threads\": realtime_threads.value, \n",
    "    \"srcdb_delta\": delta_threads.value,\n",
    "    \"dstdb_type\": dbx_destinations.value,\n",
    "    \"dstdb_stage\": dbx_staging.value,\n",
    "    \"dbx_spark_url\": dbx_spark_url.value,\n",
    "    \"dbx_databricks_url\": dbx_databricks_url.value,\n",
    "    \"dbx_hostname\": dbx_hostname.value,\n",
    "    \"dbx_dbfs_root\": dbx_username.value,\n",
    "    \"dbx_username\": dbx_username.value,\n",
    "\n",
    "    # schema and data\n",
    "    \"sparse_cntstart\": sparse_cntstart.value,\n",
    "    \"sparse_cnt\": sparse_cnt.value , \n",
    "    \"sparse_fieldcount\": sparse_fieldcount.value, \n",
    "    \"sparse_fieldlength\": sparse_fieldlength.value, \n",
    "    \"sparse_recordcount\": sparse_recordcount.value, \n",
    "    \"sparse_fillpct_start\": sparse_fillpct.value[0],\n",
    "    \"sparse_fillpct_end\": sparse_fillpct.value[1],\n",
    "    \"dense_cntstart\": dense_cntstart.value, \n",
    "    \"dense_cnt\": dense_cnt.value, \n",
    "    \"dense_fieldcount\": dense_fieldcount.value, \n",
    "    \"dense_fieldlength\": dense_fieldlength.value, \n",
    "    \"dense_recordcount\": dense_recordcount.value, \n",
    "    \"dense_fillpct_start\": dense_fillpct.value[0],\n",
    "    \"dense_fillpct_end\": dense_fillpct.value[1],\n",
    "\n",
    "    # workload\n",
    "    \"sparse_tps\": sparse_tps.value,\n",
    "    \"dense_tps\": dense_tps.value,\n",
    "    \"sparse_threads\": sparse_threads.value,\n",
    "    \"dense_threads\": dense_threads.value,\n",
    "    \"sparse_multiUpdateSize\": sparse_multiupdatesize.value,\n",
    "    \"sparse_multiInsertSize\": sparse_multiinsertsize.value,\n",
    "    \"sparse_multiDeleteSize\": sparse_multideletesize.value,\n",
    "    \"dense_multiUpdateSize\": dense_multiupdatesize.value,\n",
    "    \"dense_multiInsertSize\": dense_multiinsertsize.value,\n",
    "    \"dense_multiDeleteSize\": dense_multideletesize.value,\n",
    "    \"ram_percent_ycsb\": ram_percent_ycsb.value,\n",
    "\n",
    "    # database\n",
    "    \"ram_mb_sqlserver\": ram_mb_sqlserver.value,\n",
    "\n",
    "    }\n",
    "\n",
    "    # cluster\n",
    "    try:\n",
    "        all_params[\"spark.databricks.clusterUsageTags.clusterNodeType\"] = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\")\n",
    "        all_params[\"spark.databricks.clusterUsageTags.cloudProvider\"]  =  spark.conf.get(\"spark.databricks.clusterUsageTags.cloudProvider\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return(all_params)\n",
    "\n",
    "arcion_run_id=None\n",
    "\n",
    "# used to start new MLFlow when parameters changes \n",
    "try:\n",
    "    mlflow_proc_state\n",
    "except:\n",
    "    mlflow_proc_state={}\n",
    "\n",
    "try:\n",
    "    previous_exp_params\n",
    "except:\n",
    "    previous_exp_params={}\n",
    "try:\n",
    "    current_exp_params\n",
    "except:\n",
    "    current_exp_params={}\n",
    "\n",
    "try:\n",
    "    ycsb_logfile_positions\n",
    "except:\n",
    "    ycsb_logfile_positions={}\n",
    "try:\n",
    "    ycsb_metrics\n",
    "except:\n",
    "    ycsb_metrics={}\n",
    "try:\n",
    "    previous_log_time\n",
    "except:\n",
    "    previous_log_time=None    \n",
    "\n",
    "# arcion statistics CSV\n",
    "arcion_stats_csv_header_lines=\"catalog_name,schema_name,table_name,snapshot_start_range,snapshot_end_range,start_time,end_time,insert_count,update_count,upsert_count,delete_count,elapsed_time_sec,replicant_lag,total_lag\"\n",
    "arcion_key_index={'insert_count':7,'update_count':8,'upsert_count':9,'delete_count':10,'elapsed_time_sec':11,'replicant_lag':12,'total_lag':13}\n",
    "arc_stat_catalog_name_idx=0\n",
    "arc_stat_schema_name_idx=1\n",
    "arc_stat_table_name_idx=2\n",
    "arc_stat_start_time_idx=5\n",
    "arc_stat_end_time_idx=6\n",
    "arc_stat_insert_count_idx=7\n",
    "arc_stat_update_count_idx=8\n",
    "arc_stat_upsert_count_idx=9\n",
    "arc_stat_delete_count_idx=10\n",
    "arc_stat_replicant_lag_idx=12\n",
    "arc_stat_total_lag_idx=13\n",
    "arc_default_lag=9223372036854775807\n",
    "\n",
    "try:\n",
    "    arcion_stats_csv_positions\n",
    "except:\n",
    "    arcion_stats_csv_positions={}\n",
    "\n",
    "# setup GUI elements\n",
    "\n",
    "repl_mode = widgets.Dropdown(options=['snapshot', 'real-time', 'full'],value='real-time',\n",
    "    description='Replication:',\n",
    ")\n",
    "cdc_mode = widgets.Dropdown(options=['change', 'cdc'],value='change',\n",
    "    description='CDC Method:',\n",
    ")\n",
    "ram_percent = widgets.BoundedIntText(value=10,min=10,max=80,\n",
    "    description='RAM %:',\n",
    ")\n",
    "\n",
    "extraction_method = widgets.Dropdown(options=['BCP', 'QUERY'],value='QUERY',\n",
    "    description='Extraction Method:',\n",
    ")\n",
    "\n",
    "ram_percent_ycsb = widgets.BoundedIntText(value=1,min=1,max=80,\n",
    "    description='RAM %:',\n",
    ")\n",
    "\n",
    "ram_mb_sqlserver = widgets.BoundedIntText(value=1024,min=1,max=8096,\n",
    "    description='RAM MB:',\n",
    ")\n",
    "\n",
    "snapshot_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Snapshot Threads:',\n",
    ")\n",
    "\n",
    "realtime_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Real Time Threads:',\n",
    ")    \n",
    "\n",
    "delta_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Delta Snapshot Threads:',\n",
    ")    \n",
    "\n",
    "dbx_destinations = widgets.Dropdown(options=['null', 'deltalake', 'unitycatalog'],value='null',\n",
    "    description='Destinations:',\n",
    ")\n",
    "try:\n",
    "    if spark.conf.get(\"spark.databricks.unityCatalog.enabled\")=='false':\n",
    "        dbx_destinations = widgets.Dropdown(options=['null', 'deltalake'],value='null', description='Destinations:',)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dbx_staging = widgets.Dropdown(options=['dbfs'],value='dbfs',\n",
    "    description='Staging:',\n",
    ")\n",
    "\n",
    "sparse_cnt = widgets.BoundedIntText(value=4,min=1,max=1000,\n",
    "    description='Tbl End:',\n",
    ")\n",
    "sparse_cntstart = widgets.BoundedIntText(value=1,min=1,max=1000,\n",
    "    description='Tbl Start:',\n",
    ")\n",
    "\n",
    "sparse_fieldcount = widgets.BoundedIntText(value=50,min=0,max=9000,\n",
    "    description='# of Fields:',\n",
    ")\n",
    "sparse_fieldlength = widgets.BoundedIntText(value=10,min=1,max=1000,\n",
    "    description='Field Len:',\n",
    ")\n",
    "\n",
    "sparse_tps = widgets.BoundedIntText(value=1,min=0,max=10000,\n",
    "    description='TPS:',\n",
    ")\n",
    "sparse_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Threads:',\n",
    ")\n",
    "sparse_recordcount = widgets.Text(value=\"1M\",\n",
    "    description='Rec Cnt:',\n",
    ")\n",
    "\n",
    "sparse_fillpct = widgets.IntRangeSlider(value=[0,0],min=0,max=100,step=1,\n",
    "    description='Fill Range:', orientation='horizontal', readout=False\n",
    ")\n",
    "\n",
    "dense_cnt = widgets.BoundedIntText(value=2,min=1,max=1000,\n",
    "    description='Tbl End:',\n",
    ")\n",
    "dense_cntstart = widgets.BoundedIntText(value=1,min=1,max=1000,\n",
    "    description='Tbl Start:',\n",
    ")\n",
    "\n",
    "dense_fieldcount = widgets.BoundedIntText(value=10,min=0,max=9000,\n",
    "    description='# of Fields:',\n",
    ")\n",
    "dense_fieldlength = widgets.BoundedIntText(value=100,min=1,max=1000,\n",
    "    description='Field Len:',\n",
    ")\n",
    "dense_recordcount = widgets.Text(value=\"100K\",\n",
    "    description='Rec Cnt:',\n",
    ")\n",
    "\n",
    "dense_tps = widgets.BoundedIntText(value=1,min=0,max=10000,\n",
    "    description='TPS:',\n",
    ")\n",
    "dense_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Threads:',\n",
    ")\n",
    "\n",
    "delupdins_proportion = widgets.IntRangeSlider(value=[1,999],min=0,max=1000,step=1,\n",
    "    description='Del Upd Ind:', orientation='horizontal', readout=True\n",
    ")\n",
    "\n",
    "# sqlserver max is 2100 total perpared parameters\n",
    "dense_multiupdatesize = widgets.BoundedIntText(value=100,min=0,max=2000, description='Upd TPS:')\n",
    "dense_multiinsertsize = widgets.BoundedIntText(value=1,min=0,max=2000, description='Ins TPS:')\n",
    "dense_multideletesize = widgets.BoundedIntText(value=1,min=0,max=2000, description='Del TPS:')\n",
    "\n",
    "sparse_multiupdatesize = widgets.BoundedIntText(value=100,min=0,max=2000, description='Upd TPS:')\n",
    "sparse_multiinsertsize = widgets.BoundedIntText(value=1,min=0,max=2000, description='Ins TPS:')\n",
    "sparse_multideletesize = widgets.BoundedIntText(value=1,min=0,max=2000, description='Del TPS:')\n",
    "\n",
    "dense_fillpct = widgets.IntRangeSlider(value=[1,99],min=0,max=100,step=1,\n",
    "    description='Fill Range:', orientation='horizontal', readout=False\n",
    ")\n",
    "\n",
    "ycsb_data_gen = widgets.Dropdown(options=['special char', 'char and num'],value='char and num', description='Data Type',)\n",
    "\n",
    "dbx_spark_url = widgets.Textarea(value='',\n",
    "    description='Spark URL:',\n",
    ")\n",
    "\n",
    "dbx_databricks_url = widgets.Textarea(value='',\n",
    "    description='Databricks URL:',\n",
    ")\n",
    "\n",
    "dbx_hostname = widgets.Textarea(value='',\n",
    "    description='Hostname:',\n",
    ")\n",
    "\n",
    "src_username = widgets.Textarea(value='',\n",
    "    description='SRC User:',\n",
    ")\n",
    "\n",
    "dbx_username = widgets.Textarea(value='',\n",
    "    description='DST User:',\n",
    ")\n",
    "\n",
    "arcion_license = widgets.Textarea(value='',\n",
    "    description='Lic',\n",
    ")\n",
    "\n",
    "arcion_download_url = widgets.Textarea(value='https://arcion-releases.s3.us-west-1.amazonaws.com/general/replicant/replicant-cli-24.01.25.20.zip',\n",
    "    description='Download URL',\n",
    ")\n",
    "\n",
    "dbx_access_token = widgets.Password(value='',\n",
    "    description='Access Token',\n",
    ")\n",
    "\n",
    "dbx_default_catalog = widgets.Textarea(value='',\n",
    "    description='HMS Catalog',\n",
    ")\n",
    "\n",
    "\n",
    "# cluster where the notebook is running to auto populate the destinations\n",
    "spark_url=\"\"\n",
    "databricks_url=\"\"\n",
    "workspaceUrl=\"\"\n",
    "username=\"\"\n",
    "try:\n",
    "    cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "    workspace_id =spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n",
    "\n",
    "    # clusterName = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\")\n",
    "\n",
    "    workspaceUrl = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['tags']['browserHostName']\n",
    "\n",
    "    # below does not work on GCP\n",
    "    # sc.getConf().getAll() to see what is avail\n",
    "    # workspaceUrl = spark.conf.get(\"spark.databricks.workspaceUrl\") # host name\n",
    "\n",
    "    http_path = f\"sql/protocolv1/o/{workspace_id}/{cluster_id}\"\n",
    "\n",
    "    spark_url=f\"jdbc:spark://{workspaceUrl}:443/default;transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;\"\n",
    "    databricks_url=f\"jdbc:databricks://{workspaceUrl}:443/default;transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;\"\n",
    "\n",
    "except:\n",
    "    pass\n",
    "dbx_spark_url.value = spark_url\n",
    "dbx_databricks_url.value = databricks_url\n",
    "dbx_hostname.value = workspaceUrl\n",
    "\n",
    "try:\n",
    "    username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "    dbx_username.value = re.sub('[.@]','_',username)\n",
    "    src_username.value = re.sub('[.@]','_',username)\n",
    "except:\n",
    "    src_username.value='arcsrc'\n",
    "    dbx_username.value='arcdst'\n",
    "\n",
    "try:\n",
    "    dbx_default_catalog.value=spark.conf.get(\"spark.databricks.sql.initial.catalog.name\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check arcion license via os env\n",
    "try:\n",
    "    arclicenv=os.environ[\"ARCION_LICENSE\"]\n",
    "    if arclicenv != \"\": \n",
    "        arcion_license.value=arclicenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check arcion license via dbx widget\n",
    "try:\n",
    "    arclicwidget=dbutils.widgets.get(\"Arcion License\")\n",
    "    if arclicwidget != \"\": \n",
    "        arcion_license.value=arclicwidget\n",
    "        arcion_license.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check access token via dbx widget\n",
    "try:\n",
    "    acctokwidget=dbutils.widgets.get(\"Access Token\")\n",
    "    if acctokwidget != \"\": \n",
    "        dbx_access_token.value=acctokwidget\n",
    "        dbx_access_token.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    CONFIG_FILE=dbutils.widgets.get(\"Config\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check dpkg dir via dbx widget\n",
    "pkg_src_dir=widgets.Textarea(value='',\n",
    "    description='Pkg Src Dir:',\n",
    ")\n",
    "try:\n",
    "    pkgsrcdirwidget=dbutils.widgets.get(\"Package Source Dir\")\n",
    "    if pkgsrcdirwidget != \"\": \n",
    "        pkg_src_dir.value=pkgsrcdirwidget\n",
    "        pkg_src_dir.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check if os env has ARCION_LICENSE\n",
    "try:\n",
    "    arclicenv=os.getenv('ARCION_LICENSE')\n",
    "    if arclicenv != \"\": \n",
    "        arcion_license.value=arclicenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# gcp does not change cwd to notebook path\n",
    "pwd_result= subprocess.run(f\"\"\"pwd\"\"\",capture_output = True, text = True )\n",
    "cwd=pwd_result.stdout.splitlines()[-1]\n",
    "if (pwd_result.stdout == \"/databricks/driver\\n\"):\n",
    "    notebookpath=\"/Workspace\" + str(pathlib.Path(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()).parent)\n",
    "else:\n",
    "    notebookpath = None\n",
    "\n",
    "# optional MLflow\n",
    "experiment_id=None\n",
    "try:\n",
    "    import mlflow\n",
    "    experiment_id=dbutils.widgets.get(\"Experiment ID\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# src_db\n",
    "src_db_type = widgets.Dropdown(value='sqlserver', options=['mysql', 'postgresql', 'sqlserver'])\n",
    "src_db_host = widgets.Text(value='localhost', placeholder='hostname or IP')\n",
    "src_db_port = widgets.Text(value='', placeholder='port #')\n",
    "src_db_user = widgets.Text(value='', placeholder='username')\n",
    "src_db_pass = widgets.Text(value='', placeholder='user password')\n",
    "src_db_root_user = widgets.Text(value='', placeholder='root username')\n",
    "src_db_root_pass = widgets.Text(value='', placeholder='root password')\n",
    "\n",
    "# dst_db\n",
    "\n",
    "# change defaults based on the dropdown\n",
    "data = pd.read_csv('resources/map.csv',dtype=str) \n",
    "def update_db_defaults(args=None):\n",
    "    x=data.loc[(data['group']==src_db_type.value)]\n",
    "    src_db_port.value = x['port'].values[0] #\n",
    "    src_db_root_user.value = x['root_user'].values[0] #\n",
    "    src_db_root_pass.value = x['root_pw'].values[0] #\n",
    "src_db_type.observe(update_db_defaults, 'value')\n",
    "update_db_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872e49ec-d202-4d22-8a13-7f4f6ce38b62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup\n",
    "  - Enter `Arcion License`\n",
    "  - Enter `Personal Access Token` (generate **One Day** and delete afterwards)\n",
    "  - Click **Menu Bar** ->  Run -> Run All Below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995664f9-a450-4da8-980d-e54a19214565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (CONFIG_FILE is not None) and (CONFIG_FILE != \"\"):\n",
    "     CONFIG_FILE=Path(CONFIG_FILE).resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a746ba2-57a0-4a80-b8e0-4e8fdd0254ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d24d9f96fe45939248a46aa7230022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Arcion'), Textarea(value='', description='Lic'), Textarea(value='ht…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enter license and DBX personal access token\n",
    "\n",
    "VBox([HBox([Label('Arcion'), arcion_license, arcion_download_url,pkg_src_dir]),\n",
    "      HBox([Label('DBX'), dbx_access_token, dbx_default_catalog]),\n",
    "      HBox([Label('Username'), src_username, dbx_username]),\n",
    "      HBox([Label('Workspace'), dbx_spark_url, dbx_databricks_url, dbx_hostname, ]),\n",
    "      HBox([Label('DB RAM'), ram_mb_sqlserver, ]),\n",
    "       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c848f2d-7c81-4bd2-a56e-ababeed2962c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a208371c-d54e-4158-81be-16483107efa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmux session ready. session arcdst already exists\n",
      "deltalake /opt/stage/libs/SparkJDBC42.jar found\n",
      "lakehouse  /opt/stage/libs/DatabricksJDBC42.jar found\n",
      "postgres  /opt/stage/libs/postgresql-42.7.1.jar found\n",
      "mariadb  /opt/stage/libs/mariadb-java-client-3.3.2.jar found\n",
      "oracle /opt/stage/libs/ojdbc8.jar found\n",
      "log4j /opt/stage/libs/log4j-1.2.17.jar found\n",
      "sqlserver /opt/stage/libs/mssql-jdbc-12.6.1.jre8.jar found\n",
      "arcion  /opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/bin/replicant found\n",
      "checking jar(s) in /opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/lib for updates\n",
      "checking jar(s) in /opt/stage/arcion/replicant-cli-24.01.25.1/replicant-cli/lib for updates\n",
      "checking jar(s) in /opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/lib for updates\n",
      "Arcion license found\n",
      "YCSB  /opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT  found\n",
      "numfmt found\n",
      "bc found\n",
      "checking jar(s) in /opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT/lib for updates\n",
      "apt-utils already installed\n",
      "mssql-server already installed\n",
      "mssql-tools18 already installed\n",
      "unixodbc-dev alrady installed\n",
      "sqlserver already started\n",
      "source ./env/perf1-dbo.sh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "open terminal failed: not a terminal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/bin/replicant 24.01.25.20 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "source ./env/perf1-dbo.sh\n",
      "/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/bin/replicant 24.01.25.20 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "creating user cdcadmin\n",
      "Msg 15025, Level 16, State 1, Server arcion-perf-uswest2, Line 1\n",
      "The server principal 'cdcadmin' already exists.\n",
      "Msg 1801, Level 16, State 1, Server arcion-perf-uswest2, Line 1\n",
      "Database 'perf1' already exists. Choose a different database name.\n",
      "Msg 40508, Level 16, State 1, Server arcion-perf-uswest2, Line 1\n",
      "USE statement is not supported to switch between databases. Use a new connection to connect to a different database.\n",
      "Msg 15023, Level 16, State 5, Server arcion-perf-uswest2, Line 1\n",
      "User, group, or role 'cdcadmin' already exists in the current database.\n",
      "Msg 15151, Level 16, State 1, Server arcion-perf-uswest2, Line 1\n",
      "Cannot alter the role 'db_owner', because it does not exist or you do not have permission.\n",
      "Msg 15151, Level 16, State 1, Server arcion-perf-uswest2, Line 1\n",
      "Cannot alter the role 'db_ddladmin', because it does not exist or you do not have permission.\n",
      "Msg 15151, Level 16, State 1, Server arcion-perf-uswest2, Line 1\n",
      "Cannot alter the user 'cdcadmin', because it does not exist or you do not have permission.\n",
      "Msg 40517, Level 16, State 1, Server arcion-perf-uswest2, Line 1\n",
      "Keyword or statement option 'default_database' is not supported in this version of SQL Server.\n",
      "source ./env/perf1-dbo.sh\n",
      "/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/bin/replicant 24.01.25.20 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "Msg 2812, Level 16, State 62, Server arcion-perf-uswest2, Line 1\n",
      "Could not find stored procedure 'sp_configure'.\n",
      "Msg 40510, Level 16, State 1, Server arcion-perf-uswest2, Line 1\n",
      "Statement 'CONFIG' is not supported in this version of SQL Server.\n",
      "Msg 2812, Level 16, State 62, Server arcion-perf-uswest2, Line 1\n",
      "Could not find stored procedure 'sp_configure'.\n",
      "Msg 40510, Level 16, State 1, Server arcion-perf-uswest2, Line 1\n",
      "Statement 'CONFIG' is not supported in this version of SQL Server.\n",
      "prometheus already downloaded\n",
      "prometheus node_exporter already downloaded\n",
      "prometheus sql_exporter already downloaded\n",
      "started /opt/stage/prom/sql_exporter-0.14.0.linux-amd64/sql_exporter.  log at /var/tmp/arcsrc/sqlserver/logs/sql_exporter.log\n",
      "started /opt/stage/prom/node_exporter-1.7.0.linux-amd64/node_exporter.  log at /var/tmp/arcsrc/sqlserver/logs/node_exporter.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='export SRCDB_ARC_USER=arcsrc; export CONFIG_FILE=\"./env/perf1-dbo.sh\"; bin/install-prometheus.sh', returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup tmux, arcion, ycsb\n",
    "subprocess.run(f\"\"\". ./bin/setup-tmux.sh; setup_tmux '{dbx_username.value}'\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/download-jars.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"ARCION_LICENSE='{arcion_license.value}' ARCION_DOWNLOAD_URL='{arcion_download_url.value}' bin/install-arcion.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/install-ycsb.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "\n",
    "# mysql\n",
    "\n",
    "# pg\n",
    "\n",
    "\n",
    "# sqlserver\n",
    "subprocess.run(f\"\"\"export CONFIG_FILE=\"{CONFIG_FILE}\"; SQL_SERVER_DPKG='{pkg_src_dir.value}'; bin/install-sqlserver.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; export CONFIG_FILE=\"{CONFIG_FILE}\"; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; ping_sql_cli;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; export CONFIG_FILE=\"{CONFIG_FILE}\"; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; create_user;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; export CONFIG_FILE=\"{CONFIG_FILE}\"; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; MAX_SQLSERVER_RAM={ram_mb_sqlserver.value} set_sqlserver_ram '{dbx_username.value}';\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; export CONFIG_FILE=\"{CONFIG_FILE}\"; bin/install-prometheus.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean up the hack need for mlflow ycsb log parsing\n",
    "x=subprocess.run(f\"\"\"export CONFIG_FILE=\"{CONFIG_FILE}\"; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; echo $SRCDB_SCHEMA\"\"\",\n",
    "                 capture_output=True, shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "srcdb_schema=x.stdout.decode(\"utf-8\").splitlines()[-1]\n",
    "x=subprocess.run(f\"\"\"export CONFIG_FILE=\"{CONFIG_FILE}\"; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; echo $SRCDB_ARC_USER\"\"\",\n",
    "                 capture_output=True, shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "src_username.value=x.stdout.decode(\"utf-8\").splitlines()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427dc8c9-96f3-4ac6-bf41-4efd371f3b04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Schema and Data\n",
    "\n",
    "Existing tables will be appended with additional rows if the `Fill Range` is the same.  \n",
    "Increase the `Table Count` to create additional tables.  \n",
    "\n",
    "The following options are available:\n",
    "- Table count (Table Cnt): The number of tables to create.  \n",
    "  - Table names are `ycsbdense`, `ycsbdense2`, `ycsbdense3`, ... and `ycssparse`, `ycsbdense2`, and `ycsbdense3` ...\n",
    "- Number of Fields (# of Fields): The number of fields per table.  \n",
    "  - The field names are `FIELD0`, `FIELD1`, `FIELD2`, ...\n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Field Length (Field Len): The length of random character data populated per field.  \n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Record Count (Rec Cnt): The number of records per table generated.\n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Fill Range: The relative start and end range of fields that are populated with data.  Be default: \n",
    "    - sparse tables are all NULLs by having the fill range be 0% to 0% ranges\n",
    "    - dense tables have all fields populated by having the fill range be 0% to 100% of ranges \n",
    "\n",
    "```sql\n",
    "[localhost][arcsrc] 1> \\describe ycsbsparse\n",
    "+-------------+-------------+-----------+-------------+----------------+-------------+\n",
    "| TABLE_SCHEM | COLUMN_NAME | TYPE_NAME | COLUMN_SIZE | DECIMAL_DIGITS | IS_NULLABLE |\n",
    "+-------------+-------------+-----------+-------------+----------------+-------------+\n",
    "| dbo         | YCSB_KEY    | int       |          10 |              0 | NO          |\n",
    "| dbo         | FIELD0      | text      |  2147483647 |         [NULL] | YES         |\n",
    "| dbo         | FIELD1      | text      |  2147483647 |         [NULL] | YES         |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd09cab-c63e-4195-9364-09011fd2b559",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "Make changes below and click `Run All Below`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c315fc-624c-442c-b712-518336f5bfd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3582749a07984e388b752e94b503183b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Sparse'), BoundedIntText(value=1, description='Tbl Start:', max=100…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show YCSB Data Controls\n",
    "VBox([HBox([Label('Sparse'), sparse_cntstart,sparse_cnt, sparse_fieldcount, sparse_fieldlength, sparse_recordcount, sparse_fillpct]),\n",
    "    HBox([Label('Dense'),  dense_cntstart, dense_cnt, dense_fieldcount, dense_fieldlength, dense_recordcount, dense_fillpct])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370b278c-17fe-4f68-ac8f-aa0ee8c18568",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7f7c0f-9f1d-40e3-a4f8-a02d6a42bf0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source /home/rslee/github/dbx/ingestion/env/perf1-dbo.sh\n",
      "/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/bin/replicant 24.01.25.20 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "table count at /var/tmp/cdcadmin/sqlserver/config/list_table_counts.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sparse load. /var/tmp/cdcadmin/sqlserver/logs/ycsb/ycssparse.load.log 1 2 3 4\n",
      "starting dense load. /var/tmp/cdcadmin/sqlserver/logs/ycsb/ycsbdense.load.log 1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "schema dump at /var/tmp/cdcadmin/sqlserver/config/schema_dump.csv\n",
      "table count at /var/tmp/cdcadmin/sqlserver/config/list_table_counts.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table name</th>\n",
       "      <th>min key</th>\n",
       "      <th>max key</th>\n",
       "      <th>field count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [table name, min key, max key, field count]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run load_sparse_data_cnt and load_dense_data_cnt \n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; \n",
    "    export CONFIG_FILE=\"{CONFIG_FILE}\";\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    list_table_counts;\n",
    "    y_fieldcount={sparse_fieldcount.value} \n",
    "    y_fieldlength={sparse_fieldlength.value}  \n",
    "    y_recordcount={sparse_recordcount.value} \n",
    "    y_fillstart={math.ceil((sparse_fillpct.value[0] * sparse_fieldcount.value) / 100)}      \n",
    "    y_fillend={int((sparse_fillpct.value[1] * sparse_fieldcount.value) / 100)}      \n",
    "    load_sparse_data_cnt {sparse_cnt.value} {sparse_cntstart.value};\n",
    "    y_fieldcount={dense_fieldcount.value} \n",
    "    y_fieldlength={dense_fieldlength.value} \n",
    "    y_recordcount={dense_recordcount.value} \n",
    "    y_fillstart={math.ceil((dense_fillpct.value[0] * dense_fieldcount.value) / 100)}      \n",
    "    y_fillend={int((dense_fillpct.value[1] * dense_fieldcount.value) / 100)}      \n",
    "    load_dense_data_cnt {dense_cnt.value} {dense_cntstart.value};\n",
    "    dump_schema;\n",
    "    list_table_counts\"\"\",\n",
    "    shell=True,executable=\"bash\",cwd=notebookpath) \n",
    "# show tables\n",
    "pd.read_csv (f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\",header=None, names= ['table name','min key','max key','field count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1aeafcc-3471-47b9-b59d-b14016329940",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Workload\n",
    "\n",
    "Choose the options in the UI and run the cell below it to start the workload (YCSB).  \n",
    "\n",
    "YCSB update (workload A) controls for Dense and Sparse table groups separated. Each group has a separate control.  However, all of the tables in the group use the same controls.  \n",
    "1. Each table's TPS (throughput per second)\n",
    "   1. 0=fast as possible\n",
    "   2. 1=1 TPS\n",
    "   3. 10=10 TPS\n",
    "2. Each table's threads (concurrency) used to achieve the desired TPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e009b417-01dc-4855-b557-3a1cd88f70c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3233053-e7a3-4c56-8544-6d75c718f1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c340b90c4d34a65bc921a7db0fc3662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Sparse'), BoundedIntText(value=1, description='Threads:', max=8, mi…"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show YCSB run controls\n",
    "VBox([\n",
    "      HBox([Label('Sparse'), sparse_threads,  sparse_multideletesize, sparse_multiupdatesize, sparse_multiinsertsize, ]), \n",
    "      HBox([Label('Dense'),  dense_threads, dense_multideletesize, dense_multiupdatesize, dense_multiinsertsize, ]),\n",
    "      HBox([Label('YCSB'), ram_percent_ycsb, ycsb_data_gen]),\n",
    "      ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1920d9d-bae0-49b0-adab-554352dcc7a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33058c18-c9e7-46f2-ab85-618136369430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source ./env/perf1-dbo.sh\n",
      "/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/bin/replicant 24.01.25.20 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "running ycsb on /var/tmp/cdcadmin/sqlserver/config/list_table_counts.csv\n",
      "ycsb can be killed with . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_ycsb)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "table count at /var/tmp/cdcadmin/sqlserver/config/list_table_counts.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='export SRCDB_ARC_USER=arcsrc;\\n    export CONFIG_FILE=\"./env/perf1-dbo.sh\";\\n    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \\n    kill_ycsb;\\n    list_table_counts;       \\n    y_data_type=\"char and num\"\\n    y_threads_dense=1 \\n    y_threads_sparse=1               \\n    y_multiinsertsize_dense=1 \\n    y_multiupdatesize_dense=100 \\n    y_multideletesize_dense=1 \\n    y_multiinsertsize_sparse=1 \\n    y_multiupdatesize_sparse=100 \\n    y_multideletesize_sparse=1 \\n    y_fieldlength_sparse=10 \\n    y_fieldlength_dense=100 \\n    y_MinRAMPercentage=1.0\\n    y_MaxRAMPercentage=1.0\\n    start_ycsb;', returncode=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start/restart YCSB run\n",
    "# start the actual run\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value};\n",
    "    export CONFIG_FILE=\"{CONFIG_FILE}\";\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    kill_ycsb;\n",
    "    list_table_counts;       \n",
    "    y_data_type=\"{ycsb_data_gen.value}\"\n",
    "    y_threads_dense={dense_threads.value} \n",
    "    y_threads_sparse={sparse_threads.value}               \n",
    "    y_multiinsertsize_dense={dense_multiinsertsize.value} \n",
    "    y_multiupdatesize_dense={dense_multiupdatesize.value} \n",
    "    y_multideletesize_dense={dense_multideletesize.value} \n",
    "    y_multiinsertsize_sparse={sparse_multiinsertsize.value} \n",
    "    y_multiupdatesize_sparse={sparse_multiupdatesize.value} \n",
    "    y_multideletesize_sparse={sparse_multideletesize.value} \n",
    "    y_fieldlength_sparse={sparse_fieldlength.value} \n",
    "    y_fieldlength_dense={dense_fieldlength.value} \n",
    "    y_MinRAMPercentage={ram_percent_ycsb.value}.0\n",
    "    y_MaxRAMPercentage={ram_percent_ycsb.value}.0\n",
    "    start_ycsb;\"\"\",\n",
    "    shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6734eb-5342-48b5-868b-750218370b54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Arcion\n",
    "\n",
    "Choose the options in the UI and run the cell below it to start the replication.  \n",
    "\n",
    "The following control are avail in the demo.  \n",
    "- Arcion - replication type and CDC methods  \n",
    "- Threads - control the parallelism.\n",
    "- Target - null, unity catalog or delta lake\n",
    "\n",
    "NOTE: Full mode does not work at this time.\n",
    "\n",
    "For SQL Server, change tracking, cdc are available for demo.  \n",
    "\n",
    "Performance is mainly controlled by the thread count by the extract and apply process.\n",
    "Additional controls are customizable via modifying the YAML files directly below.\n",
    "- [CDC YAML files](./demo/sqlserver/yaml/cdc/)\n",
    "- [Change Tracking YAML files](./demo/sqlserver/yaml/change/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb753ed-5c15-4f74-9ed6-8ddf59a2ce42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "Make changes below and click `Run All Below`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d603b1d4-bb2c-418b-acfe-894441efe01e",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3511409fea2545a1b28d4804b86b9e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='RAM'), BoundedIntText(value=10, description='RAM %:', max=80, min=1…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show Arcion and DBX controls\n",
    "VBox([\n",
    "      HBox([Label('RAM'), ram_percent]),\n",
    "      HBox([Label('Modes'), repl_mode, cdc_mode, extraction_method]),\n",
    "      HBox([Label('Target'), dbx_destinations, dbx_staging ]),\n",
    "      HBox([Label('Threads'), snapshot_threads, realtime_threads, delta_threads]),\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbea2ac-b6ea-4cf4-9be1-bf665a7e6ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a0ede1-4383-4f3e-b9c8-5e601c35bfcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change real-time\n",
      "source ./env/perf1-dbo.sh\n",
      "/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/bin/replicant 24.01.25.20 24.01\n",
      "PATH=/opt/stage/bin/jsqsh-dist-3.0-SNAPSHOT/bin added\n",
      "Msg 40508, Level 16, State 1, Server arcion-perf-uswest2, Line 4\n",
      "USE statement is not supported to switch between databases. Use a new connection to connect to a different database.\n",
      "disable/drop trigger replicate_io_audit_ddl_trigger\n",
      "DISABLE TRIGGER \"replicate_io_audit_ddl_trigger\" ON DATABASE\n",
      "drop trigger \"replicate_io_audit_ddl_trigger\" on DATABASE\n",
      "following tables still have change trakcing enabled and change tracking can't be disabled\n",
      "ALTER TABLE cdc_benchmark1.perf_table_1 DISABLE CHANGE_TRACKING;\n",
      "ALTER TABLE cdc_benchmark3.perf_table_1 DISABLE CHANGE_TRACKING;\n",
      "change tracking on database already disabled\n",
      "prog_dir=/home/rslee/github/dbx/ingestion/demo/sqlserver arcion_bin=/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/bin/replicant\n",
      "enable change tracking on database perf1\n",
      "ALTER DATABASE perf1 SET CHANGE_TRACKING = ON  (CHANGE_RETENTION = 2 DAYS, AUTO_CLEANUP = ON);\n",
      "Msg 42008, Level 16, State 5, Server arcion-perf-uswest2, Line 5\n",
      "ODBC error: State: 42000: Error: 5088 Message:'[Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Change tracking is already enabled for database 'perf1'.'.\n",
      "Msg 5069, Level 16, State 3, Server arcion-perf-uswest2, Line 5\n",
      "ALTER DATABASE statement failed.\n",
      "arcion pid 95859\n",
      "arcion console is at /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788/arcion.log\n",
      "arcion log is at /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788\n",
      "arcion can be killed with . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_arcion)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ cd /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788\n",
      "+ set +x\n",
      "+ JAVA_HOME=\n",
      "+ REPLICANT_MEMORY_PERCENTAGE=10.0\n",
      "+ JAVA_OPTS='\"-Djava.security.egd=file:/dev/urandom\" \"-Doracle.jdbc.javaNetNio=false\" \"-XX:-UseCompressedOops\"'\n",
      "+ /opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/bin/replicant real-time /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788/src.yaml /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788/dst.yaml --applier /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788/applier.yaml --general /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788/general.yaml --extractor /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788/extractor.yaml --filter /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788/filter.yaml --statistics /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788/statistics.yaml --metadata /var/tmp/cdcadmin/sqlserver/logs/3fdf8d788/metadata.yaml --overwrite --id 3fdf8d788 --append-existing\n"
     ]
    }
   ],
   "source": [
    "# start/restart Arcion\n",
    "\n",
    "if ( f\"{dbx_access_token.value}\" == \"\" ) and ( f\"{dbx_destinations.value}\" != \"null\" ):\n",
    "    print(\"personal access token not entered.\")\n",
    "else:\n",
    "    arcion_run_id=nine_char_id()\n",
    "    # start a new run\n",
    "    print (f\"\"\"{cdc_mode.value} {repl_mode.value}\"\"\")\n",
    "    arcion_run = subprocess.run(f\"\"\"export CONFIG_FILE=\"{CONFIG_FILE}\"; \n",
    "    export ARCION_DOWNLOAD_URL='{arcion_download_url.value}';        \n",
    "    export SRCDB_ARC_USER={src_username.value};\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    kill_arcion;\n",
    "    disable_cdc;\n",
    "    disable_change_tracking;\n",
    "    echo prog_dir=$PROG_DIR arcion_bin=$ARCION_BIN;\n",
    "    cd $PROG_DIR;\n",
    "    NINE_CHAR_ID='{arcion_run_id}'\n",
    "    a_repltype='{repl_mode.value}'\n",
    "    EXTRACTION_METHOD='{extraction_method.value}'\n",
    "    REPLICANT_MEMORY_PERCENTAGE='{ram_percent.value}.0'\n",
    "    SRCDB_SNAPSHOT_THREADS='{snapshot_threads.value}' \n",
    "    SRCDB_REALTIME_THREADS='{realtime_threads.value}' \n",
    "    SRCDB_DELTA='{delta_threads.value}'\n",
    "    DSTDB_TYPE='{dbx_destinations.value}'\n",
    "    DSTDB_STAGE='{dbx_staging.value}'\n",
    "    DBX_SPARK_URL='{dbx_spark_url.value}'\n",
    "    DBX_DATABRICKS_URL='{dbx_databricks_url.value}'\n",
    "    DBX_ACCESS_TOKEN='{dbx_access_token.value}'\n",
    "    DBX_HOSTNAME='{dbx_hostname.value}'\n",
    "    DBX_DBFS_ROOT='/{dbx_username.value}'\n",
    "    DBX_USERNAME='{dbx_username.value}'\n",
    "    start_{cdc_mode.value}_arcion;\"\"\",\n",
    "    shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cafb49c3-df1b-4458-805c-b233876acf90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# MLFLow\n",
    "\n",
    "Save the artifacts in MLFlow.\n",
    "\n",
    "Artifacts are collected for 5 min (600 sec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f193b18-1efa-46be-afb3-37510d683db0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb57b96-e39c-465e-a939-2ec7aa27392a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first run of mlflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time from ycsbdense:2024-05-01 13:03:55.047000\n",
      "start time from ycsbdense:2024-05-01 13:03:55.047000\n",
      "start time from ycsbdense:2024-05-01 13:03:45.047000\n",
      "start time from ycsbdense:2024-05-01 13:03:35.047000\n",
      "start time from ycsbdense:2024-05-01 13:03:25.047000\n",
      "start time from ycsbdense:2024-05-01 13:03:15.047000\n",
      "start time from ycsbdense:2024-05-01 13:03:05.047000\n",
      "start time from ycsbdense:2024-05-01 13:02:55.047000\n",
      "start time from ycsbdense:2024-05-01 13:02:45.047000\n",
      "start time from ycsbdense:2024-05-01 13:02:35.047000\n",
      "start time from ycsbdense:2024-05-01 13:02:25.047000\n",
      "start time from ycsbdense:2024-05-01 13:02:15.047000\n",
      "start time from ycsbdense:2024-05-01 13:02:05.047000\n",
      "start time from ycsbdense:2024-05-01 13:01:55.047000\n",
      "start time from ycsbdense:2024-05-01 13:01:45.047000\n",
      "start time from ycsbdense:2024-05-01 13:01:35.047000\n",
      "start time from ycsbdense:2024-05-01 13:01:25.047000\n",
      "start time from ycsbdense:2024-05-01 13:01:15.047000\n",
      "start time from ycsbdense:2024-05-01 13:01:05.047000\n",
      "start time from ycsbdense:2024-05-01 13:00:55.047000\n",
      "start time from ycsbdense:2024-05-01 13:00:45.047000\n",
      "start time from ycsbdense:2024-05-01 13:00:35.047000\n",
      "start time from ycsbdense:2024-05-01 13:00:25.047000\n",
      "start time from ycsbdense:2024-05-01 13:00:15.047000\n",
      "start time from ycsbdense:2024-05-01 13:00:05.047000\n",
      "start time from ycsbdense:2024-05-01 12:59:55.047000\n",
      "start time from ycsbdense:2024-05-01 12:59:45.047000\n",
      "start time from ycsbdense:2024-05-01 12:59:35.047000\n",
      "start time from ycsbdense:2024-05-01 12:59:25.047000\n",
      "start time from ycsbdense:2024-05-01 12:59:15.047000\n",
      "start time from ycsbdense:2024-05-01 12:59:05.047000\n",
      "start time from ycsbdense:2024-05-01 12:58:55.047000\n",
      "start time from ycsbdense:2024-05-01 12:58:45.047000\n",
      "start time from ycsbdense:2024-05-01 12:58:35.047000\n",
      "start time from ycsbdense:2024-05-01 12:58:25.047000\n",
      "start time from ycsbdense:2024-05-01 12:58:15.047000\n",
      "start time from ycsbdense:2024-05-01 12:58:05.047000\n",
      "start time from ycsbdense:2024-05-01 12:57:55.047000\n",
      "start time from ycsbdense:2024-05-01 12:57:45.047000\n",
      "start time from ycsbdense:2024-05-01 12:57:35.047000\n",
      "start time from ycsbdense:2024-05-01 12:57:25.047000\n",
      "start time from ycsbdense:2024-05-01 12:57:15.047000\n",
      "start time from ycsbdense:2024-05-01 12:57:05.047000\n",
      "start time from ycsbdense:2024-05-01 12:56:55.047000\n",
      "start time from ycsbdense:2024-05-01 12:56:45.047000\n",
      "start time from ycsbdense:2024-05-01 12:56:35.047000\n",
      "start time from ycsbdense:2024-05-01 12:56:25.047000\n",
      "start time from ycsbdense:2024-05-01 12:56:15.047000\n",
      "start time from ycsbdense:2024-05-01 12:56:05.047000\n",
      "start time from ycsbdense:2024-05-01 12:55:55.047000\n",
      "start time from ycsbdense:2024-05-01 12:55:45.047000\n",
      "start time from ycsbdense:2024-05-01 12:55:35.047000\n",
      "start time from ycsbdense:2024-05-01 12:55:25.047000\n",
      "start time from ycsbdense:2024-05-01 12:55:15.047000\n",
      "start time from ycsbdense:2024-05-01 12:55:05.047000\n",
      "start time from ycsbdense:2024-05-01 12:54:55.047000\n",
      "start time from ycsbdense:2024-05-01 12:54:45.121000\n",
      "start time from ycsbdense2:2024-05-01 13:03:55.087000\n",
      "start time from ycsbdense2:2024-05-01 13:03:45.087000\n",
      "start time from ycsbdense2:2024-05-01 13:03:35.087000\n",
      "start time from ycsbdense2:2024-05-01 13:03:25.087000\n",
      "start time from ycsbdense2:2024-05-01 13:03:15.087000\n",
      "start time from ycsbdense2:2024-05-01 13:03:05.087000\n",
      "start time from ycsbdense2:2024-05-01 13:02:55.087000\n",
      "start time from ycsbdense2:2024-05-01 13:02:45.087000\n",
      "start time from ycsbdense2:2024-05-01 13:02:35.087000\n",
      "start time from ycsbdense2:2024-05-01 13:02:25.087000\n",
      "start time from ycsbdense2:2024-05-01 13:02:15.087000\n",
      "start time from ycsbdense2:2024-05-01 13:02:05.087000\n",
      "start time from ycsbdense2:2024-05-01 13:01:55.087000\n",
      "start time from ycsbdense2:2024-05-01 13:01:45.087000\n",
      "start time from ycsbdense2:2024-05-01 13:01:35.087000\n",
      "start time from ycsbdense2:2024-05-01 13:01:25.087000\n",
      "start time from ycsbdense2:2024-05-01 13:01:15.087000\n",
      "start time from ycsbdense2:2024-05-01 13:01:05.087000\n",
      "start time from ycsbdense2:2024-05-01 13:00:55.087000\n",
      "start time from ycsbdense2:2024-05-01 13:00:45.087000\n",
      "start time from ycsbdense2:2024-05-01 13:00:35.087000\n",
      "start time from ycsbdense2:2024-05-01 13:00:25.087000\n",
      "start time from ycsbdense2:2024-05-01 13:00:15.087000\n",
      "start time from ycsbdense2:2024-05-01 13:00:05.087000\n",
      "start time from ycsbdense2:2024-05-01 12:59:55.087000\n",
      "start time from ycsbdense2:2024-05-01 12:59:45.087000\n",
      "start time from ycsbdense2:2024-05-01 12:59:35.087000\n",
      "start time from ycsbdense2:2024-05-01 12:59:25.087000\n",
      "start time from ycsbdense2:2024-05-01 12:59:15.087000\n",
      "start time from ycsbdense2:2024-05-01 12:59:05.087000\n",
      "start time from ycsbdense2:2024-05-01 12:58:55.087000\n",
      "start time from ycsbdense2:2024-05-01 12:58:45.087000\n",
      "start time from ycsbdense2:2024-05-01 12:58:35.087000\n",
      "start time from ycsbdense2:2024-05-01 12:58:25.087000\n",
      "start time from ycsbdense2:2024-05-01 12:58:15.087000\n",
      "start time from ycsbdense2:2024-05-01 12:58:05.087000\n",
      "start time from ycsbdense2:2024-05-01 12:57:55.087000\n",
      "start time from ycsbdense2:2024-05-01 12:57:45.087000\n",
      "start time from ycsbdense2:2024-05-01 12:57:35.087000\n",
      "start time from ycsbdense2:2024-05-01 12:57:25.087000\n",
      "start time from ycsbdense2:2024-05-01 12:57:15.087000\n",
      "start time from ycsbdense2:2024-05-01 12:57:05.087000\n",
      "start time from ycsbdense2:2024-05-01 12:56:55.087000\n",
      "start time from ycsbdense2:2024-05-01 12:56:45.087000\n",
      "start time from ycsbdense2:2024-05-01 12:56:35.087000\n",
      "start time from ycsbdense2:2024-05-01 12:56:25.087000\n",
      "start time from ycsbdense2:2024-05-01 12:56:15.087000\n",
      "start time from ycsbdense2:2024-05-01 12:56:05.087000\n",
      "start time from ycsbdense2:2024-05-01 12:55:55.087000\n",
      "start time from ycsbdense2:2024-05-01 12:55:45.087000\n",
      "start time from ycsbdense2:2024-05-01 12:55:35.087000\n",
      "start time from ycsbdense2:2024-05-01 12:55:25.087000\n",
      "start time from ycsbdense2:2024-05-01 12:55:15.087000\n",
      "start time from ycsbdense2:2024-05-01 12:55:05.087000\n",
      "start time from ycsbdense2:2024-05-01 12:54:55.087000\n",
      "start time from ycsbdense2:2024-05-01 12:54:45.141000\n",
      "start time from ycsbsparse:2024-05-01 13:03:55.170000\n",
      "start time from ycsbsparse:2024-05-01 13:03:45.170000\n",
      "start time from ycsbsparse:2024-05-01 13:03:35.170000\n",
      "start time from ycsbsparse:2024-05-01 13:03:25.170000\n",
      "start time from ycsbsparse:2024-05-01 13:03:15.170000\n",
      "start time from ycsbsparse:2024-05-01 13:03:05.170000\n",
      "start time from ycsbsparse:2024-05-01 13:02:55.170000\n",
      "start time from ycsbsparse:2024-05-01 13:02:45.170000\n",
      "start time from ycsbsparse:2024-05-01 13:02:35.170000\n",
      "start time from ycsbsparse:2024-05-01 13:02:25.170000\n",
      "start time from ycsbsparse:2024-05-01 13:02:15.170000\n",
      "start time from ycsbsparse:2024-05-01 13:02:05.170000\n",
      "start time from ycsbsparse:2024-05-01 13:01:55.170000\n",
      "start time from ycsbsparse:2024-05-01 13:01:45.170000\n",
      "start time from ycsbsparse:2024-05-01 13:01:35.170000\n",
      "start time from ycsbsparse:2024-05-01 13:01:25.170000\n",
      "start time from ycsbsparse:2024-05-01 13:01:15.170000\n",
      "start time from ycsbsparse:2024-05-01 13:01:05.170000\n",
      "start time from ycsbsparse:2024-05-01 13:00:55.170000\n",
      "start time from ycsbsparse:2024-05-01 13:00:45.170000\n",
      "start time from ycsbsparse:2024-05-01 13:00:35.170000\n",
      "start time from ycsbsparse:2024-05-01 13:00:25.171000\n",
      "start time from ycsbsparse:2024-05-01 13:00:15.170000\n",
      "start time from ycsbsparse:2024-05-01 13:00:05.170000\n",
      "start time from ycsbsparse:2024-05-01 12:59:55.170000\n",
      "start time from ycsbsparse:2024-05-01 12:59:45.170000\n",
      "start time from ycsbsparse:2024-05-01 12:59:35.170000\n",
      "start time from ycsbsparse:2024-05-01 12:59:25.170000\n",
      "start time from ycsbsparse:2024-05-01 12:59:15.170000\n",
      "start time from ycsbsparse:2024-05-01 12:59:05.171000\n",
      "start time from ycsbsparse:2024-05-01 12:58:55.170000\n",
      "start time from ycsbsparse:2024-05-01 12:58:45.171000\n",
      "start time from ycsbsparse:2024-05-01 12:58:35.171000\n",
      "start time from ycsbsparse:2024-05-01 12:58:25.170000\n",
      "start time from ycsbsparse:2024-05-01 12:58:15.171000\n",
      "start time from ycsbsparse:2024-05-01 12:58:05.170000\n",
      "start time from ycsbsparse:2024-05-01 12:57:55.170000\n",
      "start time from ycsbsparse:2024-05-01 12:57:45.171000\n",
      "start time from ycsbsparse:2024-05-01 12:57:35.170000\n",
      "start time from ycsbsparse:2024-05-01 12:57:25.170000\n",
      "start time from ycsbsparse:2024-05-01 12:57:15.170000\n",
      "start time from ycsbsparse:2024-05-01 12:57:05.171000\n",
      "start time from ycsbsparse:2024-05-01 12:56:55.170000\n",
      "start time from ycsbsparse:2024-05-01 12:56:45.171000\n",
      "start time from ycsbsparse:2024-05-01 12:56:35.171000\n",
      "start time from ycsbsparse:2024-05-01 12:56:25.171000\n",
      "start time from ycsbsparse:2024-05-01 12:56:15.170000\n",
      "start time from ycsbsparse:2024-05-01 12:56:05.170000\n",
      "start time from ycsbsparse:2024-05-01 12:55:55.170000\n",
      "start time from ycsbsparse:2024-05-01 12:55:45.171000\n",
      "start time from ycsbsparse:2024-05-01 12:55:35.170000\n",
      "start time from ycsbsparse:2024-05-01 12:55:25.170000\n",
      "start time from ycsbsparse:2024-05-01 12:55:15.171000\n",
      "start time from ycsbsparse:2024-05-01 12:55:05.170000\n",
      "start time from ycsbsparse:2024-05-01 12:54:55.171000\n",
      "start time from ycsbsparse:2024-05-01 12:54:45.249000\n",
      "start time from ycsbsparse2:2024-05-01 13:03:55.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:03:45.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:03:35.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:03:25.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:03:15.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:03:05.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:02:55.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:02:45.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:02:35.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:02:25.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:02:15.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:02:05.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:01:55.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:01:45.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:01:35.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:01:25.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:01:15.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:01:05.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:00:55.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:00:45.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:00:35.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:00:25.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:00:15.139000\n",
      "start time from ycsbsparse2:2024-05-01 13:00:05.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:59:55.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:59:45.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:59:35.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:59:25.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:59:15.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:59:05.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:58:55.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:58:45.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:58:35.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:58:25.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:58:15.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:58:05.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:57:55.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:57:45.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:57:35.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:57:25.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:57:15.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:57:05.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:56:55.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:56:45.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:56:35.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:56:25.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:56:15.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:56:05.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:55:55.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:55:45.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:55:35.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:55:25.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:55:15.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:55:05.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:54:55.139000\n",
      "start time from ycsbsparse2:2024-05-01 12:54:45.185000\n",
      "start time from ycsbsparse3:2024-05-01 13:03:55.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:03:45.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:03:35.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:03:25.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:03:15.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:03:05.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:02:55.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:02:45.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:02:35.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:02:25.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:02:15.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:02:05.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:01:55.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:01:45.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:01:35.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:01:25.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:01:15.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:01:05.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:00:55.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:00:45.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:00:35.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:00:25.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:00:15.254000\n",
      "start time from ycsbsparse3:2024-05-01 13:00:05.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:59:55.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:59:45.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:59:35.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:59:25.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:59:15.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:59:05.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:58:55.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:58:45.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:58:35.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:58:25.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:58:15.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:58:05.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:57:55.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:57:45.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:57:35.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:57:25.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:57:15.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:57:05.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:56:55.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:56:45.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:56:35.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:56:25.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:56:15.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:56:05.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:55:55.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:55:45.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:55:35.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:55:25.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:55:15.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:55:05.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:54:55.254000\n",
      "start time from ycsbsparse3:2024-05-01 12:54:45.300000\n",
      "start time from ycsbsparse4:2024-05-01 13:03:55.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:03:45.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:03:35.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:03:25.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:03:15.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:03:05.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:02:55.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:02:45.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:02:35.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:02:25.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:02:15.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:02:05.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:01:55.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:01:45.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:01:35.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:01:25.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:01:15.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:01:05.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:00:55.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:00:45.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:00:35.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:00:25.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:00:15.408000\n",
      "start time from ycsbsparse4:2024-05-01 13:00:05.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:59:55.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:59:45.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:59:35.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:59:25.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:59:15.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:59:05.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:58:55.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:58:45.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:58:35.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:58:25.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:58:15.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:58:05.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:57:55.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:57:45.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:57:35.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:57:25.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:57:15.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:57:05.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:56:55.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:56:45.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:56:35.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:56:25.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:56:15.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:56:05.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:55:55.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:55:45.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:55:35.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:55:25.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:55:15.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:55:05.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:54:55.408000\n",
      "start time from ycsbsparse4:2024-05-01 12:54:45.521000\n",
      "2024-05-01 12:54:45.521000\n",
      "2024-05-01 13:03:55.047000\n",
      "549.526\n",
      "dict_keys(['ycsb/update-failed_ycsbdense', 'ycsb/update-failed', 'ycsb/update-failed_max_microsec_ycsbdense', 'ycsb/update-failed_max_microsec', 'ycsb/delete_ycsbdense', 'ycsb/delete', 'ycsb/delete_max_microsec_ycsbdense', 'ycsb/delete_max_microsec', 'ycsb/update_ycsbdense', 'ycsb/update', 'ycsb/update_max_microsec_ycsbdense', 'ycsb/update_max_microsec', 'ycsb/insert_ycsbdense', 'ycsb/insert', 'ycsb/insert_max_microsec_ycsbdense', 'ycsb/insert_max_microsec', 'ycsb/update-failed_ycsbdense2', 'ycsb/update-failed_max_microsec_ycsbdense2', 'ycsb/delete_ycsbdense2', 'ycsb/delete_max_microsec_ycsbdense2', 'ycsb/update_ycsbdense2', 'ycsb/update_max_microsec_ycsbdense2', 'ycsb/insert_ycsbdense2', 'ycsb/insert_max_microsec_ycsbdense2', 'ycsb/update-failed_ycsbsparse', 'ycsb/update-failed_max_microsec_ycsbsparse', 'ycsb/delete_ycsbsparse', 'ycsb/delete_max_microsec_ycsbsparse', 'ycsb/update_ycsbsparse', 'ycsb/update_max_microsec_ycsbsparse', 'ycsb/insert_ycsbsparse', 'ycsb/insert_max_microsec_ycsbsparse', 'ycsb/update-failed_ycsbsparse2', 'ycsb/update-failed_max_microsec_ycsbsparse2', 'ycsb/delete_ycsbsparse2', 'ycsb/delete_max_microsec_ycsbsparse2', 'ycsb/insert_ycsbsparse2', 'ycsb/insert_max_microsec_ycsbsparse2', 'ycsb/update_ycsbsparse2', 'ycsb/update_max_microsec_ycsbsparse2', 'ycsb/update-failed_ycsbsparse3', 'ycsb/update-failed_max_microsec_ycsbsparse3', 'ycsb/delete_ycsbsparse3', 'ycsb/delete_max_microsec_ycsbsparse3', 'ycsb/update_ycsbsparse3', 'ycsb/update_max_microsec_ycsbsparse3', 'ycsb/insert_ycsbsparse3', 'ycsb/insert_max_microsec_ycsbsparse3', 'ycsb/update-failed_ycsbsparse4', 'ycsb/update-failed_max_microsec_ycsbsparse4', 'ycsb/delete_ycsbsparse4', 'ycsb/delete_max_microsec_ycsbsparse4', 'ycsb/update_ycsbsparse4', 'ycsb/update_max_microsec_ycsbsparse4', 'ycsb/insert_ycsbsparse4', 'ycsb/insert_max_microsec_ycsbsparse4'])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n",
      "None\n",
      "None\n",
      "0\n",
      "dict_keys([])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_94483/863413452.py\", line 378, in start_mlflow\n",
      "    mlflow.log_metrics(metrics=get_ycsb_metrics(),step=step)\n",
      "  File \"/tmp/ipykernel_94483/863413452.py\", line 319, in get_ycsb_metrics\n",
      "    start_time, end_time, _ = parse_ycsb_log_to_metric(ycsb_logfile_positions,start_time, end_time,\n",
      "  File \"/tmp/ipykernel_94483/863413452.py\", line 252, in parse_ycsb_log_to_metric\n",
      "    with FileReadBackwards(file, encoding=\"utf-8\") as ycsb_log_file:\n",
      "  File \"/home/rslee/.local/lib/python3.10/site-packages/file_read_backwards/file_read_backwards.py\", line 41, in __init__\n",
      "    self.iterator = FileReadBackwardsIterator(io.open(self.path, mode=\"rb\"), self.encoding, self.chunk_size)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/var/tmp/arcsrc/sqlserver/logs/ycsb/ycsb.dbo.ycsb.log'\n"
     ]
    }
   ],
   "source": [
    "# use process to run MLflow without blocking the notebook.  thread does not work with mlflow\n",
    "\n",
    "import mlflow\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from multiprocessing import Process\n",
    "\n",
    "def log_artifacts():\n",
    "    pass\n",
    "\n",
    "from file_read_backwards import FileReadBackwards\n",
    "import datetime\n",
    "\n",
    "# convert ycsb log mlflow metric\n",
    "# time                    elapsed  cumulative      time period                                   per operations metric\n",
    "#                         sec      operations      ops/sec\n",
    "# 2024-03-07 10:05:38:240 410 sec: 409 operations; 1 current ops/sec; est completion in 116 days [UPDATE: Count=10, Max=15383, Min=6792, Avg=9264.6, 90=15359, 99=15383, 99.9=15383, 99.99=15383]\n",
    "# ycsb_tablename_[update|update-failed]_count=x\n",
    "# ycsb_tablename_[update|update-failed]_avg_microsec=x \n",
    "\n",
    "ycsb_date_time_pattern = r\"^(?P<dt>[0-9\\-]+ [0-9\\:]+)\"  # at the beginning\n",
    "ycsb_op_val_pattern = r'\\[([^]]*)\\]'                    # [Update: ] [Insert: ] ...\n",
    "\n",
    "from file_read_backwards import FileReadBackwards\n",
    "import glob\n",
    "\n",
    "def tablulate_arc_stat_line(count, stat_type, cat_sch_tbl, arc_stats, replicant_lag, total_lag, replicant_lag_weights, total_lag_weights):\n",
    "    if count > 0:\n",
    "        #  per table DML stat\n",
    "        # catalog_schema_tablename\n",
    "        try:\n",
    "            arc_stats[f\"arcion/{stat_type}_{cat_sch_tbl}\"] += count\n",
    "        except:\n",
    "            arc_stats[f\"arcion/{stat_type}_{cat_sch_tbl}\"] = count\n",
    "        # overall DML \n",
    "        try:\n",
    "            arc_stats[f\"arcion/{stat_type}\"] += count\n",
    "        except:\n",
    "            arc_stats[f\"arcion/{stat_type}\"] = count\n",
    "\n",
    "        # skip defaut_lag value of 9223372036854775807   \n",
    "        if replicant_lag != arc_default_lag:\n",
    "            try:\n",
    "                replicant_lag_weights[f\"arcion/{stat_type}_{cat_sch_tbl}\"] += count * replicant_lag\n",
    "            except:\n",
    "                replicant_lag_weights[f\"arcion/{stat_type}_{cat_sch_tbl}\"] = count * replicant_lag\n",
    "            # overall DML \n",
    "            try:\n",
    "                replicant_lag_weights[f\"arcion/{stat_type}\"] += count * replicant_lag\n",
    "            except:\n",
    "                replicant_lag_weights[f\"arcion/{stat_type}\"] = count * replicant_lag\n",
    "\n",
    "        # skip defaut_lag value of 9223372036854775807   \n",
    "        if total_lag != arc_default_lag:\n",
    "            try:\n",
    "                total_lag_weights[f\"arcion/{stat_type}_{cat_sch_tbl}\"] += count * total_lag\n",
    "            except:\n",
    "                total_lag_weights[f\"arcion/{stat_type}_{cat_sch_tbl}\"] = count * total_lag\n",
    "            # overall DML \n",
    "            try:\n",
    "                total_lag_weights[f\"arcion/{stat_type}\"] += count * total_lag\n",
    "            except:\n",
    "                total_lag_weights[f\"arcion/{stat_type}\"] = count * total_lag\n",
    "\n",
    "        # per table weighted avg = weight / count\n",
    "        # could be case where replicant_lag_weight is not known 9223372036854775807\n",
    "        if f\"arcion/{stat_type}_{cat_sch_tbl}\" in replicant_lag_weights:\n",
    "            arc_stats[f\"arcion/{stat_type}_lag_replicant_{cat_sch_tbl}\"] = \\\n",
    "                replicant_lag_weights[f\"arcion/{stat_type}_{cat_sch_tbl}\"] / arc_stats[f\"arcion/{stat_type}_{cat_sch_tbl}\"]\n",
    "            arc_stats[f\"arcion/{stat_type}_lag_total_{cat_sch_tbl}\"] = \\\n",
    "                total_lag_weights[f\"arcion/{stat_type}_{cat_sch_tbl}\"] / arc_stats[f\"arcion/{stat_type}_{cat_sch_tbl}\"]\n",
    "\n",
    "        # overall DML \n",
    "        # could be case where replicant_lag_weight is not known 9223372036854775807            \n",
    "        if f\"arcion/{stat_type}\" in total_lag_weights:\n",
    "            arc_stats[f\"arcion/{stat_type}_lag_replicant\"] = \\\n",
    "                replicant_lag_weights[f\"arcion/{stat_type}\"] / arc_stats[f\"arcion/{stat_type}\"]\n",
    "            arc_stats[f\"arcion/{stat_type}_lag_total\"] = \\\n",
    "                total_lag_weights[f\"arcion/{stat_type}\"] / arc_stats[f\"arcion/{stat_type}\"]\n",
    "\n",
    "def set_previous_log(log_stat:dict, table_name:str=\"\"):\n",
    "    marker_key=f\"marker_{table_name}\"\n",
    "    first_key=f\"first_{table_name}\"\n",
    "    try:\n",
    "        log_stat[marker_key] = log_stat[first_key]\n",
    "    except:\n",
    "        # marker could not not defined if the file was empty\n",
    "        log_stat[marker_key] = None\n",
    "    # clear the first line read\n",
    "    log_stat[first_key] = None\n",
    "    \n",
    "def reached_previous_log(log_stat:dict, line:str, table_name:str=\"\", header_line=None):\n",
    "    marker_key=f\"marker_{table_name}\"\n",
    "    first_key=f\"first_{table_name}\"    \n",
    "\n",
    "    # reached header\n",
    "    if header_line is None:\n",
    "        pass\n",
    "    elif line==header_line:\n",
    "        return(True)\n",
    "\n",
    "    # will be the highwater maker for the next run\n",
    "    try:\n",
    "        if log_stat[first_key] is None:\n",
    "            log_stat[first_key] = line\n",
    "    except:\n",
    "        log_stat[first_key] = line\n",
    "        \n",
    "    # done when reached previous processed line\n",
    "    try:\n",
    "        if line==log_stat[marker_key]:\n",
    "            return(True)\n",
    "    except:\n",
    "        # not previous marker\n",
    "        pass\n",
    "\n",
    "    return(False)\n",
    "\n",
    "def parse_arcion_stats(run_id, user_id, db_type,arcion_stats_csv_positions):\n",
    "\n",
    "    file_list = glob.glob(f\"/var/tmp/{user_id}/{db_type}/logs/{run_id}/stats/{run_id}/{run_id}/replication_statistics_history_*.CSV\")\n",
    "    \n",
    "    # file is not ready yet\n",
    "    if len(file_list) == 0:\n",
    "        return({})\n",
    "    \n",
    "    firstlineread = None\n",
    "    arc_stats = {}\n",
    "    # temp dict used for weighted average\n",
    "    replicant_lag_weights = {}\n",
    "    total_lag_weights = {}\n",
    "    start_time=None\n",
    "    end_time=None\n",
    "\n",
    "    with FileReadBackwards(file_list[0], encoding=\"utf-8\") as BigFile:\n",
    "        for line in BigFile:\n",
    "            if reached_previous_log(log_stat=arcion_stats_csv_positions, line=line, header_line=arcion_stats_csv_header_lines):\n",
    "                break\n",
    "\n",
    "            csvline=line.split(\",\")\n",
    "            if (len(csvline)) < 13:\n",
    "                continue\n",
    "\n",
    "            if end_time is None:\n",
    "                end_time = csvline[arc_stat_end_time_idx]\n",
    "            start_time = csvline[arc_stat_start_time_idx]\n",
    "\n",
    "            # arcion_key_index={'insert_count':7,'update_count':8,'upsert_count':9,'delete_count':10,'elapsed_time_sec':11,'replicant_lag':12,'total_lag':13}\n",
    "\n",
    "            cat_sch_tbl=f\"{csvline[arc_stat_catalog_name_idx]}_{csvline[arc_stat_schema_name_idx]}_{csvline[arc_stat_table_name_idx]}\"\n",
    "            try:\n",
    "                insert_count=int(csvline[arc_stat_insert_count_idx])\n",
    "            except:\n",
    "                # could be header or some unknown format\n",
    "                continue\n",
    "\n",
    "            update_count=int(csvline[arc_stat_update_count_idx])\n",
    "            upsert_count=int(csvline[arc_stat_upsert_count_idx])\n",
    "            delete_count=int(csvline[arc_stat_delete_count_idx])\n",
    "            replicant_lag=int(csvline[arc_stat_replicant_lag_idx])\n",
    "            total_lag=int(csvline[arc_stat_total_lag_idx])\n",
    "\n",
    "            tablulate_arc_stat_line(count=insert_count, stat_type=\"insert\", cat_sch_tbl=cat_sch_tbl, arc_stats=arc_stats, replicant_lag=replicant_lag, total_lag=total_lag, replicant_lag_weights=replicant_lag_weights, total_lag_weights=total_lag_weights)\n",
    "            tablulate_arc_stat_line(count=update_count, stat_type=\"update\", cat_sch_tbl=cat_sch_tbl, arc_stats=arc_stats, replicant_lag=replicant_lag, total_lag=total_lag, replicant_lag_weights=replicant_lag_weights, total_lag_weights=total_lag_weights)\n",
    "            tablulate_arc_stat_line(count=upsert_count, stat_type=\"upsert\", cat_sch_tbl=cat_sch_tbl, arc_stats=arc_stats, replicant_lag=replicant_lag, total_lag=total_lag, replicant_lag_weights=replicant_lag_weights, total_lag_weights=total_lag_weights)\n",
    "            tablulate_arc_stat_line(count=delete_count, stat_type=\"delete\", cat_sch_tbl=cat_sch_tbl, arc_stats=arc_stats, replicant_lag=replicant_lag, total_lag=total_lag, replicant_lag_weights=replicant_lag_weights, total_lag_weights=total_lag_weights)\n",
    "\n",
    "\n",
    "    # set the end marker\n",
    "    set_previous_log(log_stat=arcion_stats_csv_positions)\n",
    "\n",
    "    # calculate count / s metric \n",
    "    try:\n",
    "        time_diff = (datetime.datetime.strptime(end_time, '%Y-%m-%dT%H:%M:%S.%fZ') -\n",
    "            datetime.datetime.strptime(start_time, '%Y-%m-%dT%H:%M:%S.%fZ')).total_seconds()\n",
    "    except:\n",
    "        time_diff = 0\n",
    "\n",
    "    for key in [\"insert\",\"update\",\"upsert\",\"delete\"]:\n",
    "        try:\n",
    "            if time_diff > 1:\n",
    "                arc_stats[f\"arcion/{key}_s\"] = arc_stats[f\"arcion/{key}\"] / time_diff\n",
    "            else:\n",
    "                arc_stats[f\"arcion/{key}_s\"] = arc_stats[f\"arcion/{key}\"]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # return the stat\n",
    "    return(arc_stats)\n",
    "\n",
    "\n",
    "def tablulate_ycsb_stat_line(line,metrics,table_name):\n",
    "    # parse [update: ...]\n",
    "    m = re.findall(ycsb_op_val_pattern, line.lower())\n",
    "    if m is None:\n",
    "        return\n",
    "    \n",
    "    # [UPDATE: Count=891, Max=63423, Min=4, Avg=194.94, 90=210, 99=350, 99.9=715, 99.99=63423]\n",
    "    for ops in m:\n",
    "        op_vals=ops.split(\":\")                  # update: ....\n",
    "        if len(op_vals) != 2:\n",
    "            break\n",
    "\n",
    "        vals_array=op_vals[1].split(\",\")        # count=?, max=?, ...\n",
    "        if len(vals_array) != 8:\n",
    "            break\n",
    "        \n",
    "        # count\n",
    "        try:    \n",
    "            op_count=float(vals_array[0].split(\"=\")[1])    # [0] count=?\n",
    "        except:\n",
    "            op_count=0.0\n",
    "        # per operation\n",
    "        try:\n",
    "            metrics[f\"ycsb/{op_vals[0]}_{table_name}\"] += op_count\n",
    "        except:\n",
    "            metrics[f\"ycsb/{op_vals[0]}_{table_name}\"] = op_count\n",
    "        # overall\n",
    "        try:\n",
    "            metrics[f\"ycsb/{op_vals[0]}\"] += op_count\n",
    "        except:\n",
    "            metrics[f\"ycsb/{op_vals[0]}\"] = op_count\n",
    "\n",
    "        # max\n",
    "        try:\n",
    "            op_max=float(vals_array[1].split(\"=\")[1])      # [1] max=? if count=0, then this will be not defined\n",
    "        except:\n",
    "            op_max=0.0\n",
    "        #pertable\n",
    "        try:\n",
    "            if metrics[f\"ycsb/{op_vals[0]}_max_microsec_{table_name}\"] < op_max:\n",
    "                metrics[f\"ycsb/{op_vals[0]}_max_microsec_{table_name}\"] = op_max\n",
    "        except:\n",
    "            metrics[f\"ycsb/{op_vals[0]}_max_microsec_{table_name}\"] = op_max\n",
    "        #overall\n",
    "        try:\n",
    "            if metrics[f\"ycsb/{op_vals[0]}_max_microsec\"] < op_max:\n",
    "                metrics[f\"ycsb/{op_vals[0]}_max_microsec\"] = op_max\n",
    "        except:\n",
    "            metrics[f\"ycsb/{op_vals[0]}_max_microsec\"] = op_max\n",
    "\n",
    "\n",
    "def parse_ycsb_log_to_metric(ycsb_logfile_positions,\n",
    "                    start_time,\n",
    "                    end_time,         \n",
    "                    table_name=\"ycsbsparse\",\n",
    "                    file=\"/var/tmp/arcsrc/sqlserver/logs/ycsb/ycsb.ycsbsparse.log\",\n",
    "                    metrics={},\n",
    "                    ):\n",
    "    \n",
    "    with FileReadBackwards(file, encoding=\"utf-8\") as ycsb_log_file:\n",
    "        count=0\n",
    "        for line in ycsb_log_file:      \n",
    "            if reached_previous_log(log_stat=ycsb_logfile_positions, line=line, table_name=table_name):\n",
    "                break\n",
    "            # endtime time\n",
    "            if end_time is None:\n",
    "                try:\n",
    "                    # 2024-03-27 14:18:57:038\n",
    "                    end_time = datetime.datetime.strptime(line[0:23], '%Y-%m-%d %H:%M:%S:%f')\n",
    "                    print(f\"end time from {table_name}:{end_time}\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # start time\n",
    "            try:\n",
    "                start_time = datetime.datetime.strptime(line[0:23], '%Y-%m-%d %H:%M:%S:%f')\n",
    "                print(f\"start time from {table_name}:{start_time}\")\n",
    "            except:\n",
    "                pass\n",
    "            tablulate_ycsb_stat_line(line=line,metrics=metrics,table_name=table_name)\n",
    "\n",
    "    # set the end marker\n",
    "    set_previous_log(log_stat=ycsb_logfile_positions, table_name=table_name)  \n",
    "    return(start_time, end_time, metrics)            \n",
    "\n",
    "def calc_count_s_ycsb(metrics, start_time, end_time):\n",
    "    # calculate count / s metric       \n",
    "    try:\n",
    "        time_diff = (end_time - start_time).total_seconds()\n",
    "    except:\n",
    "        time_diff = 0\n",
    "\n",
    "    print(start_time)\n",
    "    print(end_time)\n",
    "    print(time_diff)\n",
    "    print(metrics.keys())\n",
    "    for key in [\"insert\",\"update\",\"delete\"]:\n",
    "        try:\n",
    "            if time_diff > 1:\n",
    "                metrics[f\"ycsb/{key}_s\"] = metrics[f\"ycsb/{key}\"] / time_diff\n",
    "            else:\n",
    "                metrics[f\"ycsb/{key}_s\"] = metrics[f\"ycsb/{key}\"]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def get_arcion_metrics():\n",
    "    arc_stats=parse_arcion_stats(\n",
    "        run_id=arcion_run_id,\n",
    "        user_id=src_username.value,\n",
    "        db_type=src_db_type.value,\n",
    "        arcion_stats_csv_positions=arcion_stats_csv_positions)\n",
    "    return(arc_stats)\n",
    "\n",
    "def get_ycsb_metrics(metrics={}):\n",
    "    ycsb_current_metrics={}\n",
    "    start_time=None\n",
    "    end_time=None\n",
    "    ycsb_tables = pd.read_csv (f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\",header=None, names= ['table name','min key','max key','field count'])\n",
    "    for table_name in ycsb_tables['table name']:\n",
    "        table_name = table_name.lower()\n",
    "\n",
    "        if (srcdb_schema is not None) and (srcdb_schema == \"\"):\n",
    "            log_file_name=f\"/var/tmp/{src_username.value}/sqlserver/logs/ycsb/ycsb.{table_name}.log\"\n",
    "        else:\n",
    "            log_file_name=f\"/var/tmp/{src_username.value}/sqlserver/logs/ycsb/ycsb.{srcdb_schema}.{table_name}.log\"\n",
    "\n",
    "        start_time, end_time, _ = parse_ycsb_log_to_metric(ycsb_logfile_positions,start_time, end_time,\n",
    "            table_name=table_name, \n",
    "            file=log_file_name,\n",
    "            metrics=ycsb_current_metrics,\n",
    "            )\n",
    "    calc_count_s_ycsb(ycsb_current_metrics, start_time, end_time)\n",
    "    return(ycsb_current_metrics)\n",
    "\n",
    "def get_prom_metrics(prom_metric_url=None,metric_prefix=\"\",metric_step=None):\n",
    "    # there is a limit on the number of metrics that you can log in a single log_batch call. This limit is typically 1000. \n",
    "    # timestamp=If unspecified, the number of milliseconds since the Unix epoch is used.\n",
    "    # step=If unspecified, the default value of zero is used\n",
    "    contents = requests.get(prom_metric_url)\n",
    "    all_metrics = {}\n",
    "    metrics_count = 0\n",
    "    for line in contents.text.splitlines():\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        key,val=line.rsplit(' ', 1)       # split from the end in case the key has spaces\n",
    "        key=re.sub('[\" {}=,]', \"_\", key)  # change space,{},=,and comma into _\n",
    "        key=key.replace(\"_\", \"/\", 1)      # change the first _ to / to group based on the name space\n",
    "        all_metrics[key]=float(val)\n",
    "        metrics_count += 1\n",
    "    return(all_metrics)\n",
    "\n",
    "\n",
    "def start_mlflow(max_intervals=0,experiment_id=None, log_interval_sec=10, all_params={}, step=0):\n",
    "    # stop previous run\n",
    "    # max_intervals=0 makes the mlflow run forever\n",
    "    mlflow_run = mlflow.active_run()\n",
    "    if not(mlflow_run is None):\n",
    "        # upload final artifacts\n",
    "        log_artifacts()\n",
    "        print(f\"\"\"stopping previous MLflow {mlflow_run.info.run_id}\"\"\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "    # start a new run\n",
    "    if experiment_id == '':\n",
    "        experiment_id=None\n",
    "    mlflow.start_run(experiment_id=experiment_id, log_system_metrics=True)\n",
    "\n",
    "    # params\n",
    "    mlflow.log_params(params=all_params)\n",
    "\n",
    "    # schema\n",
    "    dataset_source=f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\"\n",
    "    mlflow.log_artifact(dataset_source)\n",
    "    \n",
    "    # data\n",
    "    dataset_shape = pd.read_csv(dataset_source, header=None, names= ['table name','min key','max key','field count'])\n",
    "    dataset = mlflow.data.from_pandas(dataset_shape, source=dataset_source)\n",
    "    mlflow.log_input(dataset, context=\"training\")    \n",
    "\n",
    "    # wait to end\n",
    "    # TODO: Make this smarter by checking whether the process is still running\n",
    "    wait_count=0\n",
    "    while (max_intervals == 0) or (wait_count < max_intervals):\n",
    "        mlflow.log_metrics(metrics=get_prom_metrics(prom_metric_url=\"http://localhost:9399/metrics\"),step=step)\n",
    "        mlflow.log_metrics(metrics=get_prom_metrics(prom_metric_url=\"http://localhost:9100/metrics\"),step=step)\n",
    "        mlflow.log_metrics(metrics=get_ycsb_metrics(),step=step)\n",
    "        mlflow.log_metrics(metrics=get_arcion_metrics(),step=step)\n",
    "        time.sleep(log_interval_sec)\n",
    "        wait_count += 1\n",
    "        step += 1\n",
    "\n",
    "    # upload the rest of the artifacts generated /var/tmp/{src_username.value}/sqlserver/logs\n",
    "    log_artifacts()\n",
    "    # experiment done\n",
    "    mlflow.end_run()\n",
    "\n",
    "def register_mlflow(exp_params):\n",
    "    mlflow_proc = Process(target=start_mlflow, kwargs={\"experiment_id\":experiment_id, \"all_params\":current_exp_params})\n",
    "    mlflow_proc.start()   \n",
    "    try:\n",
    "        mlflow_proc_state['proc'].terminate()\n",
    "        print(\"previous MLFlow process terminated\")\n",
    "    except:\n",
    "        pass\n",
    "    mlflow_proc_state['proc']       = mlflow_proc\n",
    "    mlflow_proc_state['exp_params'] = exp_params\n",
    "\n",
    "\n",
    "current_exp_params=exp_params()\n",
    "if not ('exp_params' in mlflow_proc_state):\n",
    "    print(\"first run of mlflow\")\n",
    "    register_mlflow(current_exp_params)\n",
    "elif current_exp_params != mlflow_proc_state['exp_params']:\n",
    "    print(\"param changed. starting new mlflow\")\n",
    "    register_mlflow(current_exp_params)\n",
    "elif not(mlflow_proc_state['proc'].is_alive()):\n",
    "    print(\"mlflow stopped. starting new mlflow with new step\")\n",
    "    register_mlflow(current_exp_params)\n",
    "else:\n",
    "    print(\"no parameters changed. New MLFLow experiment not needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7184a016-b7ef-4302-bee5-38ec30ed02f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Manually Kill Processes\n",
    "Uncomment below to kill desired processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79738332-5dec-4a1f-a0af-b2d9fc442150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/01 13:29:36 WARNING mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics because creating `GPUMonitor` failed with error: `pynvml` is not installed, to log GPU metrics please run `pip install pynvml` to install it..\n",
      "/home/rslee/.local/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:150: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n",
      "/home/rslee/.local/lib/python3.10/site-packages/mlflow/data/digest_utils.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  string_columns = trimmed_df.columns[(df.applymap(type) == str).all(0)]\n",
      "/home/rslee/.local/lib/python3.10/site-packages/mlflow/types/utils.py:393: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# subprocess.run(f\"\"\". ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_arcion;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "# subprocess.run(f\"\"\". ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_ycsb;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "# subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; drop_all_ycsb_tables;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Arcion License",
      "width": 184
     }
    ]
   },
   "notebookName": "sqlserver-arcion-dbx",
   "widgets": {
    "Access Token": {
     "currentValue": "dapi29ffee1b82da9b7ba3e11193abd842bf",
     "nuid": "a3fdf5f2-8290-4a28-b8ac-89bea1252303",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Access Token",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Arcion License": {
     "currentValue": "ewogICJsaWNlbnNlIiA6IHsKICAgICJ1dWlkIiA6ICI2MWQ0N2Y4Yi0zYmI4LTRhOTktYTAwYS1mMjA2OGEzMWM0MmYiLAogICAgIm93bmVyIiA6ICJEQiBTdXBwb3J0IFRyYWluaW5nIiwKICAgICJjcmVhdGVkIiA6ICIyMDIzLTEyLTA2VDAwOjAwWiIsCiAgICAiZXhwaXJlcyIgOiAiMjAyNC0wNi0wNlQwMDowMFoiLAogICAgInR5cGUiIDogIk9GRkxJTkUiLAogICAgImVkaXRpb24iIDogIkVOVEVSUFJJU0UiLAogICAgInNyYyIgOiBbICJBTEwiIF0sCiAgICAiZHN0IiA6IFsgIkFMTCIgXQogIH0sCiAgImtleSIgOiAiZXlKMWRXbGtJam9pTmpGa05EZG1PR0l0TTJKaU9DMDBZVGs1TFdFd01HRXRaakl3TmpoaE16RmpOREptSWl3aWIzZHVaWElpT2lKRVFpQlRkWEJ3YjNKMElGUnlZV2x1YVc1bklpd2lZM0psWVhSbFpDSTZJakl3TWpNdE1USXRNRFpVTURBNk1EQmFJaXdpWlhod2FYSmxjeUk2SWpJd01qUXRNRFl0TURaVU1EQTZNREJhSWl3aWRIbHdaU0k2SWs5R1JreEpUa1VpTENKbFpHbDBhVzl1SWpvaVJVNVVSVkpRVWtsVFJTSXNJbk55WXlJNld5SkJURXdpWFN3aVpITjBJanBiSWtGTVRDSmRmUT09Lk1HWUNNUUQ1SHVybWJrRm1ucURpR3lmY0J2dzVWQ2t5amJsTjFOM1pMc2pqdmlrRVdjOVVPZzZ3OEhUdHNCdmFFOWVBWjV3Q01RQ1lSNEtNTzZfYXhjMlF6RGZyTS1lenotTWFpcngyRnV3eEhSWkxQdjJzUEdaOVJ5UUl5cnpobldYZVRnVE1PdFk9Igp9",
     "nuid": "28b8555f-62d1-4e3f-bd83-80316798ef69",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Arcion License",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Config": {
     "currentValue": "",
     "nuid": "7d766068-6c1e-40d1-8af7-448fb3e04c1d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "Config",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Config",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Experiment ID": {
     "currentValue": "",
     "nuid": "730f4957-14a3-4127-bdb7-916f53691f22",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Experiment ID",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Package Source Dir": {
     "currentValue": "",
     "nuid": "f5e384ea-c669-45a4-9240-046fec8728e9",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Package Source Dir",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
