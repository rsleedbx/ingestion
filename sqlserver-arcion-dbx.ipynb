{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a3e62d-a594-4d8e-b979-55f5d7162955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "Welcome to a demo of snapshot and real time replication to Databricks.\n",
    "\n",
    "Use this notebook customized schema, data, workload, and **legacy** Arcion.\n",
    "\n",
    "**NOTE**: **Databricks Personal Access Token** and **Arcion License** are required. \n",
    "\n",
    "- Initial Setup\n",
    "  - Open `Table of Contents` (Outline)\n",
    "  - Enter `Arcion License`\n",
    "  - Enter `Databricks Personal Access Token`\n",
    "  - Click `Run All`\n",
    "  - Click `View` -> `Results Only`\n",
    "  - Click `View` -> `Web Terminal`, \n",
    "    - enter `tmux attach`.  \n",
    "      - If fails with `session not found`, then wait a bit retry.\n",
    "    - In the `tmux`'s console window, `htop` will be displayed during the setup.\n",
    "    - Once the setup is complete, Arcion snapshot summary will be displayed.\n",
    "    - Wait for the setup to finish and the snapshot to complete. \n",
    "    - Takes about 5 minutes in for the setup to finish.\n",
    "- Iterate with the following:\n",
    "  - Configure Schema and Data\n",
    "  - Configure Workload\n",
    "  - Configure Arcion\n",
    "\n",
    "## Where is Data in Databricks\n",
    "  - Spark (Delta Lake) uses **Hive Meta Store** catalog: \n",
    "    - Open new tab Catalog -> hive_metastore -> <your username>\n",
    "    - find ycsbdense and ycsbsparse tables \n",
    "  - Lakehouse uses **Unity Catalog** catalog: \n",
    "    - Open new tab Catalog -> <your username> \n",
    "    - find ycsbdense and ycsbsparse tables \n",
    "\n",
    "## Frequent Demo Configurations\n",
    "- Step 1\n",
    "  - Click Real-Time\n",
    "  - Run just Arcion\n",
    "  - Change YCSB Size\n",
    "  - Watch real-time performance\n",
    "- Step 2\n",
    "  - Click Unity Catalog target\n",
    "  - Select full replication mode\n",
    "  - Run just Arcion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8484e2-ba04-4128-aa71-02f4fb01c5f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Personal Compute Cluster\n",
    "\n",
    "Choose at least 16GB of RAM for a demo.\n",
    "\n",
    "Processes use RAM.  The following is the minimum RAM usage.  The server needs enough RAM to avoid swapping.\n",
    "- Databricks: 5GB \n",
    "- SQL Server: 2GB\n",
    "- Arcion: 10% of server RAM.\n",
    "\n",
    "Note:\n",
    "- `vmstat 5`.  any non zero metrics under the `si` and `so` columns (swap in and swap out) indicate RAM shortage. \n",
    "- DBR 13 does not print output of subprocess.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cca325-b2f8-4be4-aaf6-862be36fe237",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prep python env\n",
    "import subprocess\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import requests\n",
    "from ipywidgets import HBox, VBox, Label\n",
    "\n",
    "# setup GUI elements\n",
    "\n",
    "repl_mode = widgets.Dropdown(options=['snapshot', 'real-time', 'full'],value='snapshot',\n",
    "    description='Replication:',\n",
    ")\n",
    "cdc_mode = widgets.Dropdown(options=['change', 'cdc'],value='change',\n",
    "    description='CDC Method:',\n",
    ")\n",
    "ram_percent = widgets.BoundedIntText(value=10,min=10,max=80,\n",
    "    description='RAM %:',\n",
    ")\n",
    "\n",
    "snapshot_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Snapshot Threads:',\n",
    ")\n",
    "\n",
    "realtime_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Real Time Threads:',\n",
    ")    \n",
    "\n",
    "delta_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Delta Snapshot Threads:',\n",
    ")    \n",
    "\n",
    "dbx_destinations = widgets.Dropdown(options=['null', 'deltalake', 'unitycatalog'],value='null',\n",
    "    description='Destinations:',\n",
    ")\n",
    "dbx_staging = widgets.Dropdown(options=['dbfs'],value='dbfs',\n",
    "    description='Staging:',\n",
    ")\n",
    "\n",
    "sparse_cnt = widgets.BoundedIntText(value=4,min=1,max=1000,\n",
    "    description='Tbl End:',\n",
    ")\n",
    "sparse_cntstart = widgets.BoundedIntText(value=1,min=1,max=1000,\n",
    "    description='Tbl Start:',\n",
    ")\n",
    "\n",
    "sparse_fieldcount = widgets.BoundedIntText(value=50,min=0,max=9000,\n",
    "    description='# of Fields:',\n",
    ")\n",
    "sparse_fieldlength = widgets.BoundedIntText(value=10,min=1,max=1000,\n",
    "    description='Field Len:',\n",
    ")\n",
    "\n",
    "sparse_tps = widgets.BoundedIntText(value=1,min=0,max=1000,\n",
    "    description='TPS:',\n",
    ")\n",
    "sparse_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Threads:',\n",
    ")\n",
    "sparse_recordcount = widgets.Text(value=\"2K\",\n",
    "    description='Rec Cnt:',\n",
    ")\n",
    "\n",
    "sparse_fillpct = widgets.IntRangeSlider(value=[0,0],min=0,max=100,step=1,\n",
    "    description='Fill Range:', orientation='horizontal', readout=False\n",
    ")\n",
    "\n",
    "dense_cnt = widgets.BoundedIntText(value=2,min=1,max=1000,\n",
    "    description='Tbl End:',\n",
    ")\n",
    "dense_cntstart = widgets.BoundedIntText(value=1,min=1,max=1000,\n",
    "    description='Tbl Start:',\n",
    ")\n",
    "\n",
    "dense_fieldcount = widgets.BoundedIntText(value=10,min=0,max=9000,\n",
    "    description='# of Fields:',\n",
    ")\n",
    "dense_fieldlength = widgets.BoundedIntText(value=100,min=1,max=1000,\n",
    "    description='Field Len:',\n",
    ")\n",
    "dense_recordcount = widgets.Text(value=\"1K\",\n",
    "    description='Rec Cnt:',\n",
    ")\n",
    "\n",
    "dense_tps = widgets.BoundedIntText(value=1,min=0,max=1000,\n",
    "    description='TPS:',\n",
    ")\n",
    "dense_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Threads:',\n",
    ")\n",
    "\n",
    "dense_fillpct = widgets.IntRangeSlider(value=[0,100],min=0,max=100,step=1,\n",
    "    description='Fill Range:', orientation='horizontal', readout=False\n",
    ")\n",
    "\n",
    "dbx_spark_url = widgets.Textarea(value='',\n",
    "    description='Spark URL:',\n",
    ")\n",
    "\n",
    "dbx_databricks_url = widgets.Textarea(value='',\n",
    "    description='Databricks URL:',\n",
    ")\n",
    "\n",
    "dbx_hostname = widgets.Textarea(value='',\n",
    "    description='Hostname:',\n",
    ")\n",
    "\n",
    "src_username = widgets.Textarea(value='',\n",
    "    description='SRC User:',\n",
    ")\n",
    "\n",
    "dbx_username = widgets.Textarea(value='',\n",
    "    description='DST User:',\n",
    ")\n",
    "\n",
    "arcion_license = widgets.Textarea(value='',\n",
    "    description='Lic',\n",
    ")\n",
    "\n",
    "arcion_download_url = widgets.Textarea(value='https://arcion-releases.s3.us-west-1.amazonaws.com/general/replicant/replicant-cli-24.01.25.7.zip',\n",
    "    description='Download URL',\n",
    ")\n",
    "\n",
    "dbx_access_token = widgets.Password(value='',\n",
    "    description='Access Token',\n",
    ")\n",
    "\n",
    "dbx_default_catalog = widgets.Textarea(value='',\n",
    "    description='HMS Catalog',\n",
    ")\n",
    "\n",
    "\n",
    "# cluster where the notebook is running to auto populate the destinations\n",
    "spark_url=\"\"\n",
    "databricks_url=\"\"\n",
    "workspaceUrl=\"\"\n",
    "username=\"\"\n",
    "try:\n",
    "    cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "    workspace_id =spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n",
    "\n",
    "    # clusterName = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\")\n",
    "\n",
    "    workspaceUrl = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['tags']['browserHostName']\n",
    "\n",
    "    # below does not work on GCP\n",
    "    # sc.getConf().getAll() to see what is avail\n",
    "    # workspaceUrl = spark.conf.get(\"spark.databricks.workspaceUrl\") # host name\n",
    "\n",
    "    http_path = f\"sql/protocolv1/o/{workspace_id}/{cluster_id}\"\n",
    "\n",
    "    spark_url=f\"jdbc:spark://{workspaceUrl}:443/default;transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;\"\n",
    "    databricks_url=f\"jdbc:databricks://{workspaceUrl}:443/default;transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;\"\n",
    "\n",
    "except:\n",
    "    pass\n",
    "dbx_spark_url.value = spark_url\n",
    "dbx_databricks_url.value = databricks_url\n",
    "dbx_hostname.value = workspaceUrl\n",
    "\n",
    "try:\n",
    "    username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "    dbx_username.value = re.sub('[.@]','_',username)\n",
    "    src_username.value = re.sub('[.@]','_',username)\n",
    "except:\n",
    "    src_username.value='arcsrc'\n",
    "    dbx_username.value='arcdst'\n",
    "\n",
    "try:\n",
    "    dbx_default_catalog.value=spark.conf.get(\"spark.databricks.sql.initial.catalog.name\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check arcion license via os env\n",
    "try:\n",
    "    arclicenv=os.environ[\"ARCION_LICENSE\"]\n",
    "    if arclicenv != \"\": \n",
    "        arcion_license.value=arclicenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check arcion license via dbx widget\n",
    "try:\n",
    "    arclicwidget=dbutils.widgets.get(\"Arcion License\")\n",
    "    if arclicwidget != \"\": \n",
    "        arcion_license.value=arclicwidget\n",
    "        arcion_license.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check access token via dbx widget\n",
    "try:\n",
    "    acctokwidget=dbutils.widgets.get(\"Access Token\")\n",
    "    if acctokwidget != \"\": \n",
    "        dbx_access_token.value=acctokwidget\n",
    "        dbx_access_token.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check if os env has ARCION_LICENSE\n",
    "try:\n",
    "    arclicenv=os.getenv('ARCION_LICENSE')\n",
    "    if arclicenv != \"\": \n",
    "        arcion_license.value=arclicenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# gcp does not change cwd to notebook path\n",
    "pwd_result= subprocess.run(f\"\"\"pwd\"\"\",capture_output = True, text = True )\n",
    "if (pwd_result.stdout == \"/databricks/driver\\n\"):\n",
    "    notebookpath=\"/Workspace\" + str(pathlib.Path(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()).parent)\n",
    "else:\n",
    "    notebookpath = None\n",
    "\n",
    "# optional MLflow\n",
    "experiment_id=None\n",
    "try:\n",
    "    import mlflow\n",
    "    experiment_id=dbutils.widgets.get(\"Experiment ID\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872e49ec-d202-4d22-8a13-7f4f6ce38b62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup\n",
    "  - Enter `Arcion License`\n",
    "  - Enter `Personal Access Token` (generate **One Day** and delete afterwards)\n",
    "  - Click **Menu Bar** ->  Run -> Run All Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995664f9-a450-4da8-980d-e54a19214565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a746ba2-57a0-4a80-b8e0-4e8fdd0254ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# enter license and DBX personal access token\n",
    "VBox([HBox([Label('Arcion'), arcion_license, arcion_download_url]),\n",
    "      HBox([Label('DBX'), dbx_access_token, dbx_default_catalog]),\n",
    "      HBox([Label('Username'), src_username, dbx_username]),\n",
    "      HBox([Label('Workspace'), dbx_spark_url, dbx_databricks_url, dbx_hostname, ]),\n",
    "       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c848f2d-7c81-4bd2-a56e-ababeed2962c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a208371c-d54e-4158-81be-16483107efa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# setup tmux, arcion, ycsb, sql server\n",
    "subprocess.run(f\"\"\". ./bin/setup-tmux.sh; setup_tmux '{dbx_username.value}'\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/install-sqlserver.sh\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/download-jars.sh\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"ARCION_LICENSE='{arcion_license.value}' ARCION_DOWNLOAD_URL='{arcion_download_url.value}' bin/install-arcion.sh\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/install-ycsb.sh\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; ping_sql_cli;\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; create_user;\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; set_sqlserver_ram '{dbx_username.value}';\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; bin/install-prometheus.sh\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427dc8c9-96f3-4ac6-bf41-4efd371f3b04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Schema and Data\n",
    "\n",
    "Existing tables will be appended with additional rows if the `Fill Range` is the same.  \n",
    "Increase the `Table Count` to create additional tables.  \n",
    "\n",
    "The following options are available:\n",
    "- Table count (Table Cnt): The number of tables to create.  \n",
    "  - Table names are `ycsbdense`, `ycsbdense2`, `ycsbdense3`, ... and `ycssparse`, `ycsbdense2`, and `ycsbdense3` ...\n",
    "- Number of Fields (# of Fields): The number of fields per table.  \n",
    "  - The field names are `FIELD0`, `FIELD1`, `FIELD2`, ...\n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Field Length (Field Len): The length of random character data populated per field.  \n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Record Count (Rec Cnt): The number of records per table generated.\n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Fill Range: The relative start and end range of fields that are populated with data.  Be default: \n",
    "    - sparse tables are all NULLs by having the fill range be 0% to 0% ranges\n",
    "    - dense tables have all fields populated by having the fill range be 0% to 100% of ranges \n",
    "\n",
    "```sql\n",
    "[localhost][arcsrc] 1> \\describe ycsbsparse\n",
    "+-------------+-------------+-----------+-------------+----------------+-------------+\n",
    "| TABLE_SCHEM | COLUMN_NAME | TYPE_NAME | COLUMN_SIZE | DECIMAL_DIGITS | IS_NULLABLE |\n",
    "+-------------+-------------+-----------+-------------+----------------+-------------+\n",
    "| dbo         | YCSB_KEY    | int       |          10 |              0 | NO          |\n",
    "| dbo         | FIELD0      | text      |  2147483647 |         [NULL] | YES         |\n",
    "| dbo         | FIELD1      | text      |  2147483647 |         [NULL] | YES         |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd09cab-c63e-4195-9364-09011fd2b559",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "Make changes below and click `Run All Below`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c315fc-624c-442c-b712-518336f5bfd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# show YCSB Data Controls\n",
    "VBox([HBox([Label('Sparse'), sparse_cntstart,sparse_cnt, sparse_fieldcount, sparse_fieldlength, sparse_recordcount, sparse_fillpct]),\n",
    "    HBox([Label('Dense'),  dense_cntstart, dense_cnt, dense_fieldcount, dense_fieldlength, dense_recordcount, dense_fillpct])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370b278c-17fe-4f68-ac8f-aa0ee8c18568",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7f7c0f-9f1d-40e3-a4f8-a02d6a42bf0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run load_sparse_data_cnt and load_dense_data_cnt \n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; \n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    y_fieldcount={sparse_fieldcount.value} \n",
    "    y_fieldlength={sparse_fieldlength.value}  \n",
    "    y_recordcount={sparse_recordcount.value} \n",
    "    y_fillstart={math.ceil((sparse_fillpct.value[0] * sparse_fieldcount.value) / 100)}      \n",
    "    y_fillend={int((sparse_fillpct.value[1] * sparse_fieldcount.value) / 100)}      \n",
    "    load_sparse_data_cnt {sparse_cnt.value} {sparse_cntstart.value};\n",
    "    y_fieldcount={dense_fieldcount.value} \n",
    "    y_fieldlength={dense_fieldlength.value} \n",
    "    y_recordcount={dense_recordcount.value} \n",
    "    y_fillstart={math.ceil((dense_fillpct.value[0] * dense_fieldcount.value) / 100)}      \n",
    "    y_fillend={int((dense_fillpct.value[1] * dense_fieldcount.value) / 100)}      \n",
    "    load_dense_data_cnt {dense_cnt.value} {dense_cntstart.value};\n",
    "    dump_schema;\n",
    "    list_table_counts\"\"\",\n",
    "    shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath) \n",
    "# show tables\n",
    "pd.read_csv (f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\",header=None, names= ['table name','min key','max key','field count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1aeafcc-3471-47b9-b59d-b14016329940",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Workload\n",
    "\n",
    "Choose the options in the UI and run the cell below it to start the workload (YCSB).  \n",
    "\n",
    "YCSB update (workload A) controls for Dense and Sparse table groups separated. Each group has a separate control.  However, all of the tables in the group use the same controls.  \n",
    "1. Each table's TPS (throughput per second)\n",
    "   1. 0=fast as possible\n",
    "   2. 1=1 TPS\n",
    "   3. 10=10 TPS\n",
    "2. Each table's threads (concurrency) used to achieve the desired TPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e009b417-01dc-4855-b557-3a1cd88f70c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3233053-e7a3-4c56-8544-6d75c718f1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# show YCSB run controls\n",
    "VBox([HBox([Label('Sparse'), sparse_tps, sparse_threads]), HBox([Label('Dense'),  dense_tps, dense_threads])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1920d9d-bae0-49b0-adab-554352dcc7a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33058c18-c9e7-46f2-ab85-618136369430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# start/restart YCSB run\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value};\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    kill_ycsb;\n",
    "    y_target_sparse={sparse_tps.value} \n",
    "    y_target_dense={dense_tps.value} \n",
    "    y_threads_sparse={sparse_threads.value} \n",
    "    y_threads_dense={dense_threads.value} \n",
    "    y_fieldlength_sparse={sparse_fieldlength.value} \n",
    "    y_fieldlength_dense={dense_fieldlength.value} \n",
    "    start_ycsb;\"\"\",\n",
    "    shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6734eb-5342-48b5-868b-750218370b54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Arcion\n",
    "\n",
    "Choose the options in the UI and run the cell below it to start the replication.  \n",
    "\n",
    "The following control are avail in the demo.  \n",
    "- Arcion - replication type and CDC methods  \n",
    "- Threads - control the parallelism.\n",
    "- Target - null, unity catalog or delta lake\n",
    "\n",
    "NOTE: Full mode does not work at this time.\n",
    "\n",
    "For SQL Server, change tracking, cdc are available for demo.  \n",
    "\n",
    "Performance is mainly controlled by the thread count by the extract and apply process.\n",
    "Additional controls are customizable via modifying the YAML files directly below.\n",
    "- [CDC YAML files](./demo/sqlserver/yaml/cdc/)\n",
    "- [Change Tracking YAML files](./demo/sqlserver/yaml/change/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb753ed-5c15-4f74-9ed6-8ddf59a2ce42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "Make changes below and click `Run All Below`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d603b1d4-bb2c-418b-acfe-894441efe01e",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show Arcion and DBX controls\n",
    "VBox([\n",
    "      HBox([Label('RAM'), ram_percent]),\n",
    "      HBox([Label('Modes'), repl_mode, cdc_mode]),\n",
    "      HBox([Label('Target'), dbx_destinations, dbx_staging ]),\n",
    "      HBox([Label('Threads'), snapshot_threads, realtime_threads, delta_threads]),\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbea2ac-b6ea-4cf4-9be1-bf665a7e6ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a0ede1-4383-4f3e-b9c8-5e601c35bfcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# start/restart Arcion\n",
    "if ( f\"{dbx_access_token.value}\" == \"\" ) and ( f\"{dbx_destinations.value}\" != \"null\" ):\n",
    "    print(\"personal access token not entered.\")\n",
    "else:\n",
    "    # start a new run\n",
    "    print (f\"\"\"{cdc_mode.value} {repl_mode.value}\"\"\")\n",
    "    arcion_run = subprocess.run(f\"\"\"export ARCION_DOWNLOAD_URL='{arcion_download_url.value}';\n",
    "    export SRCDB_ARC_USER={src_username.value};\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    kill_arcion;\n",
    "    echo prog_dir=$PROG_DIR arcion_bin=$ARCION_BIN;\n",
    "    cd $PROG_DIR;\n",
    "    a_repltype='{repl_mode.value}'\n",
    "    REPLICANT_MEMORY_PERCENTAGE='{ram_percent.value}.0'\n",
    "    SRCDB_SNAPSHOT_THREADS='{snapshot_threads.value}' \n",
    "    SRCDB_REALTIME_THREADS='{realtime_threads.value}' \n",
    "    SRCDB_DELTA='{delta_threads.value}'\n",
    "    DSTDB_TYPE='{dbx_destinations.value}'\n",
    "    DSTDB_STAGE='{dbx_staging.value}'\n",
    "    DBX_SPARK_URL='{dbx_spark_url.value}'\n",
    "    DBX_DATABRICKS_URL='{dbx_databricks_url.value}'\n",
    "    DBX_ACCESS_TOKEN='{dbx_access_token.value}'\n",
    "    DBX_HOSTNAME='{dbx_hostname.value}'\n",
    "    DBX_DBFS_ROOT='/{dbx_username.value}'\n",
    "    DBX_USERNAME='{dbx_username.value}'\n",
    "    start_{cdc_mode.value}_arcion;\"\"\",\n",
    "    shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cafb49c3-df1b-4458-805c-b233876acf90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# MLFLow\n",
    "\n",
    "Save the artifacts in MLFlow.\n",
    "\n",
    "Artifacts are collected for 5 min (600 sec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f193b18-1efa-46be-afb3-37510d683db0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb57b96-e39c-465e-a939-2ec7aa27392a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from multiprocessing import Process\n",
    "\n",
    "def log_prom_metric(prom_metric_url=None,metric_prefix=\"\",metric_step=None):\n",
    "    # there is a limit on the number of metrics that you can log in a single log_batch call. This limit is typically 1000. \n",
    "    # timestamp=If unspecified, the number of milliseconds since the Unix epoch is used.\n",
    "    # step=If unspecified, the default value of zero is used\n",
    "    contents = requests.get(prom_metric_url)\n",
    "    all_metrics = {}\n",
    "    metrics_count = 0\n",
    "    for line in contents.text.splitlines():\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        key_val=line.rsplit(' ',1)  # split from the end in case the key has spaces\n",
    "        all_metrics[re.sub('[\" {}=,]',\"_\",key_val[0])]=float(key_val[1])\n",
    "        metrics_count += 1\n",
    "    return(all_metrics)\n",
    "\n",
    "\n",
    "def start_mlflow(max_intervals=5,experiment_id=None, log_interval_sec=60):\n",
    "    # stop previous run\n",
    "    mlflow_run = mlflow.active_run()\n",
    "    if not(mlflow_run is None):\n",
    "        print(f\"\"\"stopping previous MLflow {mlflow_run.info.run_id}\"\"\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "    # start a new run\n",
    "    if experiment_id == '':\n",
    "        experiment_id=None\n",
    "    mlflow.start_run(experiment_id=experiment_id, log_system_metrics=True)\n",
    "\n",
    "    # arcion\n",
    "    mlflow.log_param(\"arcion_download_url\", arcion_download_url.value)\n",
    "    mlflow.log_param(\"srcdb_arc_user\", src_username.value)\n",
    "    mlflow.log_param(\"repl_type\", repl_mode.value)\n",
    "    mlflow.log_param(\"replicant_memory_percentage\",ram_percent.value)\n",
    "    mlflow.log_param(\"srcdb_snapshot_threads\",snapshot_threads.value)\n",
    "    mlflow.log_param(\"srcdb_realtime_threads\",realtime_threads.value) \n",
    "    mlflow.log_param(\"srcdb_delta\",delta_threads.value)\n",
    "    mlflow.log_param(\"dstdb_type\",dbx_destinations.value)\n",
    "    mlflow.log_param(\"dstdb_stage\",dbx_staging.value)\n",
    "    mlflow.log_param(\"dbx_spark_url\",dbx_spark_url.value)\n",
    "    mlflow.log_param(\"dbx_databricks_url\",dbx_databricks_url.value)\n",
    "    mlflow.log_param(\"dbx_hostname\",dbx_hostname.value)\n",
    "    mlflow.log_param(\"dbx_dbfs_root\",dbx_username.value)\n",
    "    mlflow.log_param(\"dbx_username\",dbx_username.value)\n",
    "\n",
    "    # workload\n",
    "    mlflow.log_param(\"sparse_tps\",sparse_tps.value)\n",
    "    mlflow.log_param(\"dense_tps\",dense_tps.value)\n",
    "    mlflow.log_param(\"sparse_threads\",sparse_threads.value)\n",
    "    mlflow.log_param(\"dense_threads\",dense_threads.value)\n",
    "\n",
    "    # cluster\n",
    "    try:\n",
    "        mlflow.log_param(\"spark.databricks.clusterUsageTags.clusterNodeType\",spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\"))\n",
    "        mlflow.log_param(\"spark.databricks.clusterUsageTags.cloudProvider\",  spark.conf.get(\"spark.databricks.clusterUsageTags.cloudProvider\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # schema\n",
    "    dataset_source=f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\"\n",
    "    mlflow.log_artifact(dataset_source)\n",
    "    \n",
    "    # data\n",
    "    dataset_shape = pd.read_csv(dataset_source, header=None, names= ['table name','min key','max key','field count'])\n",
    "    dataset = mlflow.data.from_pandas(dataset_shape, source=dataset_source)\n",
    "    mlflow.log_input(dataset, context=\"training\")    \n",
    "\n",
    "    # wait to end\n",
    "    # TODO: Make this smarter by checking whether the process is still running\n",
    "    wait_count=0\n",
    "    while wait_count < max_intervals:\n",
    "        mlflow.log_metrics(metrics=log_prom_metric(prom_metric_url=\"http://localhost:9399/metrics\"))\n",
    "        mlflow.log_metrics(metrics=log_prom_metric(prom_metric_url=\"http://localhost:9100/metrics\"))\n",
    "        time.sleep(log_interval_sec)\n",
    "        wait_count += 1\n",
    "\n",
    "    # upload the rest of the artifacts generated /var/tmp/{src_username.value}/sqlserver/logs\n",
    "    # experiment done\n",
    "    mlflow.end_run()\n",
    "\n",
    "p = Process(target=start_mlflow, kwargs={\"experiment_id\":experiment_id})\n",
    "p.start()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Kill Processes\n",
    "Uncomment below to kill desired processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess.run(f\"\"\". ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_arcion';\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)\n",
    "# subprocess.run(f\"\"\". ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_ycsb';\"\"\",shell=True,executable=\"/usr/bin/bash\",cwd=notebookpath)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Arcion License",
      "width": 184
     }
    ]
   },
   "notebookName": "sqlserver-arcion-dbx",
   "widgets": {
    "Arcion License": {
     "currentValue": "ewogICJsaWNlbnNlIiA6IHsKICAgICJ1dWlkIiA6ICI2MWQ0N2Y4Yi0zYmI4LTRhOTktYTAwYS1mMjA2OGEzMWM0MmYiLAogICAgIm93bmVyIiA6ICJEQiBTdXBwb3J0IFRyYWluaW5nIiwKICAgICJjcmVhdGVkIiA6ICIyMDIzLTEyLTA2VDAwOjAwWiIsCiAgICAiZXhwaXJlcyIgOiAiMjAyNC0wNi0wNlQwMDowMFoiLAogICAgInR5cGUiIDogIk9GRkxJTkUiLAogICAgImVkaXRpb24iIDogIkVOVEVSUFJJU0UiLAogICAgInNyYyIgOiBbICJBTEwiIF0sCiAgICAiZHN0IiA6IFsgIkFMTCIgXQogIH0sCiAgImtleSIgOiAiZXlKMWRXbGtJam9pTmpGa05EZG1PR0l0TTJKaU9DMDBZVGs1TFdFd01HRXRaakl3TmpoaE16RmpOREptSWl3aWIzZHVaWElpT2lKRVFpQlRkWEJ3YjNKMElGUnlZV2x1YVc1bklpd2lZM0psWVhSbFpDSTZJakl3TWpNdE1USXRNRFpVTURBNk1EQmFJaXdpWlhod2FYSmxjeUk2SWpJd01qUXRNRFl0TURaVU1EQTZNREJhSWl3aWRIbHdaU0k2SWs5R1JreEpUa1VpTENKbFpHbDBhVzl1SWpvaVJVNVVSVkpRVWtsVFJTSXNJbk55WXlJNld5SkJURXdpWFN3aVpITjBJanBiSWtGTVRDSmRmUT09Lk1HWUNNUUQ1SHVybWJrRm1ucURpR3lmY0J2dzVWQ2t5amJsTjFOM1pMc2pqdmlrRVdjOVVPZzZ3OEhUdHNCdmFFOWVBWjV3Q01RQ1lSNEtNTzZfYXhjMlF6RGZyTS1lenotTWFpcngyRnV3eEhSWkxQdjJzUEdaOVJ5UUl5cnpobldYZVRnVE1PdFk9Igp9",
     "nuid": "28b8555f-62d1-4e3f-bd83-80316798ef69",
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Arcion License",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Experiment ID": {
     "currentValue": "",
     "nuid": "730f4957-14a3-4127-bdb7-916f53691f22",
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Experiment ID",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
