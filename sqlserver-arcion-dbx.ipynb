{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a3e62d-a594-4d8e-b979-55f5d7162955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "Welcome to a demo of snapshot and real time replication to Databricks.\n",
    "\n",
    "Use this notebook customized schema, data, workload, and **legacy** Arcion.\n",
    "\n",
    "**NOTE**: **Databricks Personal Access Token** and **Arcion License** are required. \n",
    "\n",
    "- Initial Setup\n",
    "  - Open `Table of Contents` (Outline)\n",
    "  - Enter `Arcion License`\n",
    "  - Enter `Databricks Personal Access Token`\n",
    "  - Click `Run All`\n",
    "  - Click `View` -> `Results Only`\n",
    "  - Click `View` -> `Web Terminal`, \n",
    "    - enter `tmux attach`.  \n",
    "      - If fails with `session not found`, then wait a bit retry.\n",
    "    - In the `tmux`'s console window, `htop` will be displayed during the setup.\n",
    "    - Once the setup is complete, Arcion snapshot summary will be displayed.\n",
    "    - Wait for the setup to finish and the snapshot to complete. \n",
    "    - Takes about 5 minutes in for the setup to finish.\n",
    "- Iterate with the following:\n",
    "  - Configure Schema and Data\n",
    "  - Configure Workload\n",
    "  - Configure Arcion\n",
    "\n",
    "## Where is Data in Databricks\n",
    "  - Spark (Delta Lake) uses **Hive Meta Store** catalog: \n",
    "    - Open new tab Catalog -> hive_metastore -> <your username>\n",
    "    - find ycsbdense and ycsbsparse tables \n",
    "  - Lakehouse uses **Unity Catalog** catalog: \n",
    "    - Open new tab Catalog -> <your username> \n",
    "    - find ycsbdense and ycsbsparse tables \n",
    "\n",
    "## Frequent Demo Configurations\n",
    "- Step 1\n",
    "  - Click Real-Time\n",
    "  - Run just Arcion\n",
    "  - Change YCSB Size\n",
    "  - Watch real-time performance\n",
    "- Step 2\n",
    "  - Click Unity Catalog target\n",
    "  - Select full replication mode\n",
    "  - Run just Arcion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8484e2-ba04-4128-aa71-02f4fb01c5f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Personal Compute Cluster\n",
    "\n",
    "Choose at least 16GB of RAM for a demo.\n",
    "\n",
    "Processes use RAM.  The following is the minimum RAM usage.  The server needs enough RAM to avoid swapping.\n",
    "- Databricks: 5GB \n",
    "- SQL Server: 2GB\n",
    "- Arcion: 10% of server RAM.\n",
    "\n",
    "Note:\n",
    "- `vmstat 5`.  any non zero metrics under the `si` and `so` columns (swap in and swap out) indicate RAM shortage. \n",
    "- DBR 13 does not print output of subprocess.run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2746520-15e7-48c2-9521-d397d1c993d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: file-read-backwards in /opt/homebrew/lib/python3.10/site-packages (3.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: deepdiff in /opt/homebrew/lib/python3.10/site-packages (6.7.1)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /opt/homebrew/lib/python3.10/site-packages (from deepdiff) (4.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install file-read-backwards \n",
    "%pip install deepdiff\n",
    "%pip install bpytop\n",
    "%pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76139ba2-74fc-4605-8343-6387b2f31dee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cca325-b2f8-4be4-aaf6-862be36fe237",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prep python env\n",
    "import subprocess\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import requests\n",
    "import deepdiff\n",
    "import time\n",
    "from ipywidgets import HBox, VBox, Label\n",
    "from file_read_backwards import FileReadBackwards\n",
    "from pylib.mlflowutils import *\n",
    "\n",
    "def nine_char_id():\n",
    "    # %s   seconds since 1970-01-01 00:00:00 UTC\n",
    "    # %N   nanoseconds (000000000..999999999)\n",
    "    return(hex(int(time.time_ns() / 100000000 ))[2:])\n",
    "\n",
    "# all exp parameters \n",
    "def exp_params():\n",
    "    all_params={\n",
    "    # arcion\n",
    "    \"arcion_download_url\": arcion_download_url.value,\n",
    "    \"srcdb_arc_user\": src_username.value,\n",
    "    \"repl_type\": repl_mode.value,\n",
    "    \"extraction_method\": extraction_method.value,\n",
    "    \"replicant_memory_percentage\": ram_percent.value,\n",
    "    \"srcdb_snapshot_threads\": snapshot_threads.value,\n",
    "    \"srcdb_realtime_threads\": realtime_threads.value, \n",
    "    \"srcdb_delta\": delta_threads.value,\n",
    "    \"dstdb_type\": dbx_destinations.value,\n",
    "    \"dstdb_stage\": dbx_staging.value,\n",
    "    \"dbx_spark_url\": dbx_spark_url.value,\n",
    "    \"dbx_databricks_url\": dbx_databricks_url.value,\n",
    "    \"dbx_hostname\": dbx_hostname.value,\n",
    "    \"dbx_dbfs_root\": dbx_username.value,\n",
    "    \"dbx_username\": dbx_username.value,\n",
    "\n",
    "    # schema and data\n",
    "    \"sparse_cntstart\": sparse_cntstart.value,\n",
    "    \"sparse_cnt\": sparse_cnt.value , \n",
    "    \"sparse_fieldcount\": sparse_fieldcount.value, \n",
    "    \"sparse_fieldlength\": sparse_fieldlength.value, \n",
    "    \"sparse_recordcount\": sparse_recordcount.value, \n",
    "    \"sparse_fillpct_start\": sparse_fillpct.value[0],\n",
    "    \"sparse_fillpct_end\": sparse_fillpct.value[1],\n",
    "    \"dense_cntstart\": dense_cntstart.value, \n",
    "    \"dense_cnt\": dense_cnt.value, \n",
    "    \"dense_fieldcount\": dense_fieldcount.value, \n",
    "    \"dense_fieldlength\": dense_fieldlength.value, \n",
    "    \"dense_recordcount\": dense_recordcount.value, \n",
    "    \"dense_fillpct_start\": dense_fillpct.value[0],\n",
    "    \"dense_fillpct_end\": dense_fillpct.value[1],\n",
    "\n",
    "    # workload\n",
    "    \"sparse_tps\": sparse_tps.value,\n",
    "    \"dense_tps\": dense_tps.value,\n",
    "    \"sparse_threads\": sparse_threads.value,\n",
    "    \"dense_threads\": dense_threads.value,\n",
    "    \"sparse_multiUpdateSize\": sparse_multiupdatesize.value,\n",
    "    \"sparse_multiInsertSize\": sparse_multiinsertsize.value,\n",
    "    \"sparse_multiDeleteSize\": sparse_multideletesize.value,\n",
    "    \"dense_multiUpdateSize\": dense_multiupdatesize.value,\n",
    "    \"dense_multiInsertSize\": dense_multiinsertsize.value,\n",
    "    \"dense_multiDeleteSize\": dense_multideletesize.value,\n",
    "    \"ram_percent_ycsb\": ram_percent_ycsb.value,\n",
    "\n",
    "    # database\n",
    "    \"ram_mb_sqlserver\": ram_mb_sqlserver.value,\n",
    "\n",
    "    }\n",
    "\n",
    "    # cluster\n",
    "    try:\n",
    "        all_params[\"spark.databricks.clusterUsageTags.clusterNodeType\"] = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\")\n",
    "        all_params[\"spark.databricks.clusterUsageTags.cloudProvider\"]  =  spark.conf.get(\"spark.databricks.clusterUsageTags.cloudProvider\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return(all_params)\n",
    "\n",
    "# used to start new MLFlow when parameters changes \n",
    "try:\n",
    "    mlflow_proc_state\n",
    "except:\n",
    "    mlflow_proc_state={}\n",
    "\n",
    "try:\n",
    "    previous_exp_params\n",
    "except:\n",
    "    previous_exp_params={}\n",
    "try:\n",
    "    current_exp_params\n",
    "except:\n",
    "    current_exp_params={}\n",
    "\n",
    "try:\n",
    "    ycsb_logfile_positions\n",
    "except:\n",
    "    ycsb_logfile_positions={}\n",
    "try:\n",
    "    ycsb_metrics\n",
    "except:\n",
    "    ycsb_metrics={}\n",
    "try:\n",
    "    previous_log_time\n",
    "except:\n",
    "    previous_log_time=None    \n",
    "\n",
    "# arcion statistics CSV\n",
    "arcion_stats_csv_header_lines=\"catalog_name,schema_name,table_name,snapshot_start_range,snapshot_end_range,start_time,end_time,insert_count,update_count,upsert_count,delete_count,elapsed_time_sec,replicant_lag,total_lag\"\n",
    "arcion_key_index={'insert_count':7,'update_count':8,'upsert_count':9,'delete_count':10,'elapsed_time_sec':11,'replicant_lag':12,'total_lag':13}\n",
    "arc_stat_catalog_name_idx=0\n",
    "arc_stat_schema_name_idx=1\n",
    "arc_stat_table_name_idx=2\n",
    "arc_stat_start_time_idx=5\n",
    "arc_stat_end_time_idx=6\n",
    "arc_stat_insert_count_idx=7\n",
    "arc_stat_update_count_idx=8\n",
    "arc_stat_upsert_count_idx=9\n",
    "arc_stat_delete_count_idx=10\n",
    "arc_stat_replicant_lag_idx=12\n",
    "arc_stat_total_lag_idx=13\n",
    "arc_default_lag=9223372036854775807\n",
    "\n",
    "try:\n",
    "    arcion_stats_csv_positions\n",
    "except:\n",
    "    arcion_stats_csv_positions={}\n",
    "\n",
    "# setup GUI elements\n",
    "\n",
    "repl_mode = widgets.Dropdown(options=['snapshot', 'real-time', 'full'],value='real-time',\n",
    "    description='Replication:',\n",
    ")\n",
    "cdc_mode = widgets.Dropdown(options=['change', 'cdc'],value='change',\n",
    "    description='CDC Method:',\n",
    ")\n",
    "ram_percent = widgets.BoundedIntText(value=10,min=10,max=80,\n",
    "    description='RAM %:',\n",
    ")\n",
    "\n",
    "extraction_method = widgets.Dropdown(options=['BCP', 'QUERY'],value='QUERY',\n",
    "    description='Extraction Method:',\n",
    ")\n",
    "\n",
    "ram_percent_ycsb = widgets.BoundedIntText(value=1,min=1,max=80,\n",
    "    description='RAM %:',\n",
    ")\n",
    "\n",
    "ram_mb_sqlserver = widgets.BoundedIntText(value=1024,min=1,max=8096,\n",
    "    description='RAM MB:',\n",
    ")\n",
    "\n",
    "snapshot_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Snapshot Threads:',\n",
    ")\n",
    "\n",
    "realtime_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Real Time Threads:',\n",
    ")    \n",
    "\n",
    "delta_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Delta Snapshot Threads:',\n",
    ")    \n",
    "\n",
    "dbx_destinations = widgets.Dropdown(options=['null', 'deltalake', 'unitycatalog'],value='null',\n",
    "    description='Destinations:',\n",
    ")\n",
    "try:\n",
    "    if spark.conf.get(\"spark.databricks.unityCatalog.enabled\")=='false':\n",
    "        dbx_destinations = widgets.Dropdown(options=['null', 'deltalake'],value='null', description='Destinations:',)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dbx_staging = widgets.Dropdown(options=['dbfs'],value='dbfs',\n",
    "    description='Staging:',\n",
    ")\n",
    "\n",
    "sparse_cnt = widgets.BoundedIntText(value=4,min=1,max=1000,\n",
    "    description='Tbl End:',\n",
    ")\n",
    "sparse_cntstart = widgets.BoundedIntText(value=1,min=1,max=1000,\n",
    "    description='Tbl Start:',\n",
    ")\n",
    "\n",
    "sparse_fieldcount = widgets.BoundedIntText(value=50,min=0,max=9000,\n",
    "    description='# of Fields:',\n",
    ")\n",
    "sparse_fieldlength = widgets.BoundedIntText(value=10,min=1,max=1000,\n",
    "    description='Field Len:',\n",
    ")\n",
    "\n",
    "sparse_tps = widgets.BoundedIntText(value=1,min=0,max=10000,\n",
    "    description='TPS:',\n",
    ")\n",
    "sparse_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Threads:',\n",
    ")\n",
    "sparse_recordcount = widgets.Text(value=\"1M\",\n",
    "    description='Rec Cnt:',\n",
    ")\n",
    "\n",
    "sparse_fillpct = widgets.IntRangeSlider(value=[0,0],min=0,max=100,step=1,\n",
    "    description='Fill Range:', orientation='horizontal', readout=False\n",
    ")\n",
    "\n",
    "dense_cnt = widgets.BoundedIntText(value=2,min=1,max=1000,\n",
    "    description='Tbl End:',\n",
    ")\n",
    "dense_cntstart = widgets.BoundedIntText(value=1,min=1,max=1000,\n",
    "    description='Tbl Start:',\n",
    ")\n",
    "\n",
    "dense_fieldcount = widgets.BoundedIntText(value=10,min=0,max=9000,\n",
    "    description='# of Fields:',\n",
    ")\n",
    "dense_fieldlength = widgets.BoundedIntText(value=100,min=1,max=1000,\n",
    "    description='Field Len:',\n",
    ")\n",
    "dense_recordcount = widgets.Text(value=\"100K\",\n",
    "    description='Rec Cnt:',\n",
    ")\n",
    "\n",
    "dense_tps = widgets.BoundedIntText(value=1,min=0,max=10000,\n",
    "    description='TPS:',\n",
    ")\n",
    "dense_threads = widgets.BoundedIntText(value=1,min=1,max=8,\n",
    "    description='Threads:',\n",
    ")\n",
    "\n",
    "delupdins_proportion = widgets.IntRangeSlider(value=[1,999],min=0,max=1000,step=1,\n",
    "    description='Del Upd Ind:', orientation='horizontal', readout=True\n",
    ")\n",
    "\n",
    "# sqlserver max is 2100 total perpared parameters\n",
    "dense_multiupdatesize = widgets.BoundedIntText(value=100,min=0,max=2000, description='Upd TPS:')\n",
    "dense_multiinsertsize = widgets.BoundedIntText(value=1,min=0,max=2000, description='Ins TPS:')\n",
    "dense_multideletesize = widgets.BoundedIntText(value=1,min=0,max=2000, description='Del TPS:')\n",
    "\n",
    "sparse_multiupdatesize = widgets.BoundedIntText(value=100,min=0,max=2000, description='Upd TPS:')\n",
    "sparse_multiinsertsize = widgets.BoundedIntText(value=1,min=0,max=2000, description='Ins TPS:')\n",
    "sparse_multideletesize = widgets.BoundedIntText(value=1,min=0,max=2000, description='Del TPS:')\n",
    "\n",
    "dense_fillpct = widgets.IntRangeSlider(value=[1,99],min=0,max=100,step=1,\n",
    "    description='Fill Range:', orientation='horizontal', readout=False\n",
    ")\n",
    "\n",
    "ycsb_data_gen = widgets.Dropdown(options=['special char', 'char and num'],value='char and num', description='Data Type',)\n",
    "\n",
    "dbx_spark_url = widgets.Textarea(value='',\n",
    "    description='Spark URL:',\n",
    ")\n",
    "\n",
    "dbx_databricks_url = widgets.Textarea(value='',\n",
    "    description='Databricks URL:',\n",
    ")\n",
    "\n",
    "dbx_hostname = widgets.Textarea(value='',\n",
    "    description='Hostname:',\n",
    ")\n",
    "\n",
    "src_username = widgets.Textarea(value='',\n",
    "    description='SRC User:',\n",
    ")\n",
    "\n",
    "dbx_username = widgets.Textarea(value='',\n",
    "    description='DST User:',\n",
    ")\n",
    "\n",
    "arcion_license = widgets.Textarea(value='',\n",
    "    description='Lic',\n",
    ")\n",
    "\n",
    "arcion_download_url = widgets.Textarea(value='https://arcion-releases.s3.us-west-1.amazonaws.com/general/replicant/replicant-cli-24.01.25.20.zip',\n",
    "    description='Download URL',\n",
    ")\n",
    "\n",
    "dbx_access_token = widgets.Password(value='',\n",
    "    description='Access Token',\n",
    ")\n",
    "\n",
    "dbx_default_catalog = widgets.Textarea(value='',\n",
    "    description='HMS Catalog',\n",
    ")\n",
    "\n",
    "\n",
    "# cluster where the notebook is running to auto populate the destinations\n",
    "spark_url=\"\"\n",
    "databricks_url=\"\"\n",
    "workspaceUrl=\"\"\n",
    "username=\"\"\n",
    "try:\n",
    "    cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "    workspace_id =spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n",
    "\n",
    "    # clusterName = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\")\n",
    "\n",
    "    workspaceUrl = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['tags']['browserHostName']\n",
    "\n",
    "    # below does not work on GCP\n",
    "    # sc.getConf().getAll() to see what is avail\n",
    "    # workspaceUrl = spark.conf.get(\"spark.databricks.workspaceUrl\") # host name\n",
    "\n",
    "    http_path = f\"sql/protocolv1/o/{workspace_id}/{cluster_id}\"\n",
    "\n",
    "    spark_url=f\"jdbc:spark://{workspaceUrl}:443/default;transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;\"\n",
    "    databricks_url=f\"jdbc:databricks://{workspaceUrl}:443/default;transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;\"\n",
    "\n",
    "except:\n",
    "    pass\n",
    "dbx_spark_url.value = spark_url\n",
    "dbx_databricks_url.value = databricks_url\n",
    "dbx_hostname.value = workspaceUrl\n",
    "\n",
    "try:\n",
    "    username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "    dbx_username.value = re.sub('[.@]','_',username)\n",
    "    src_username.value = re.sub('[.@]','_',username)\n",
    "except:\n",
    "    src_username.value='arcsrc'\n",
    "    dbx_username.value='arcdst'\n",
    "\n",
    "try:\n",
    "    dbx_default_catalog.value=spark.conf.get(\"spark.databricks.sql.initial.catalog.name\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check arcion license via os env\n",
    "try:\n",
    "    arclicenv=os.environ[\"ARCION_LICENSE\"]\n",
    "    if arclicenv != \"\": \n",
    "        arcion_license.value=arclicenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check arcion license via dbx widget\n",
    "try:\n",
    "    arclicwidget=dbutils.widgets.get(\"Arcion License\")\n",
    "    if arclicwidget != \"\": \n",
    "        arcion_license.value=arclicwidget\n",
    "        arcion_license.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check access token via dbx widget\n",
    "try:\n",
    "    acctokwidget=dbutils.widgets.get(\"Access Token\")\n",
    "    if acctokwidget != \"\": \n",
    "        dbx_access_token.value=acctokwidget\n",
    "        dbx_access_token.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "CONFIG_FILE=\"\"\n",
    "try:\n",
    "    CONFIG_FILE=dbutils.widgets.get(\"Config\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check dpkg dir via dbx widget\n",
    "pkg_src_dir=widgets.Textarea(value='',\n",
    "    description='Pkg Src Dir:',\n",
    ")\n",
    "try:\n",
    "    pkgsrcdirwidget=dbutils.widgets.get(\"Package Source Dir\")\n",
    "    if pkgsrcdirwidget != \"\": \n",
    "        pkg_src_dir.value=pkgsrcdirwidget\n",
    "        pkg_src_dir.disabled = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# check if os env has ARCION_LICENSE\n",
    "try:\n",
    "    arclicenv=os.getenv('ARCION_LICENSE')\n",
    "    if arclicenv != \"\": \n",
    "        arcion_license.value=arclicenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# gcp does not change cwd to notebook path\n",
    "pwd_result= subprocess.run(f\"\"\"pwd\"\"\",capture_output = True, text = True )\n",
    "if (pwd_result.stdout == \"/databricks/driver\\n\"):\n",
    "    notebookpath=\"/Workspace\" + str(pathlib.Path(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()).parent)\n",
    "else:\n",
    "    notebookpath = None\n",
    "\n",
    "# optional MLflow\n",
    "experiment_id=None\n",
    "try:\n",
    "    import mlflow\n",
    "    experiment_id=dbutils.widgets.get(\"Experiment ID\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# src_db\n",
    "src_db_type = widgets.Dropdown(value='sqlserver', options=['mysql', 'postgresql', 'sqlserver'])\n",
    "src_db_host = widgets.Text(value='localhost', placeholder='hostname or IP')\n",
    "src_db_port = widgets.Text(value='', placeholder='port #')\n",
    "src_db_user = widgets.Text(value='', placeholder='username')\n",
    "src_db_pass = widgets.Text(value='', placeholder='user password')\n",
    "src_db_root_user = widgets.Text(value='', placeholder='root username')\n",
    "src_db_root_pass = widgets.Text(value='', placeholder='root password')\n",
    "\n",
    "# dst_db\n",
    "\n",
    "# change defaults based on the dropdown\n",
    "data = pd.read_csv('resources/map.csv',dtype=str) \n",
    "def update_db_defaults(args=None):\n",
    "    x=data.loc[(data['group']==src_db_type.value)]\n",
    "    src_db_port.value = x['port'].values[0] #\n",
    "    src_db_root_user.value = x['root_user'].values[0] #\n",
    "    src_db_root_pass.value = x['root_pw'].values[0] #\n",
    "src_db_type.observe(update_db_defaults, 'value')\n",
    "update_db_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872e49ec-d202-4d22-8a13-7f4f6ce38b62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup\n",
    "  - Enter `Arcion License`\n",
    "  - Enter `Personal Access Token` (generate **One Day** and delete afterwards)\n",
    "  - Click **Menu Bar** ->  Run -> Run All Below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995664f9-a450-4da8-980d-e54a19214565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a746ba2-57a0-4a80-b8e0-4e8fdd0254ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d72f0aefeb4b7da8770cc2d3c89c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Arcion'), Textarea(value='ewogICJsaWNlbnNlIiA6IHsKICAgICJ1dWlkIiA6I…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enter license and DBX personal access token\n",
    "\n",
    "VBox([HBox([Label('Arcion'), arcion_license, arcion_download_url,pkg_src_dir]),\n",
    "      HBox([Label('DBX'), dbx_access_token, dbx_default_catalog]),\n",
    "      HBox([Label('Username'), src_username, dbx_username]),\n",
    "      HBox([Label('Workspace'), dbx_spark_url, dbx_databricks_url, dbx_hostname, ]),\n",
    "      HBox([Label('DB RAM'), ram_mb_sqlserver, ]),\n",
    "       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c848f2d-7c81-4bd2-a56e-ababeed2962c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a208371c-d54e-4158-81be-16483107efa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert.lee/.bashrc: line 2: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 3: compinit: command not found\n",
      "/Users/robert.lee/.bashrc: line 4: autoload: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmux session ready. new session arcdst created\n",
      "deltalake /opt/stage/libs/SparkJDBC42.jar found\n",
      "lakehouse  /opt/stage/libs/DatabricksJDBC42.jar found\n",
      "postgres  /opt/stage/libs/postgresql-42.7.1.jar found\n",
      "mariadb  /opt/stage/libs/mariadb-java-client-3.3.2.jar found\n",
      "oracle /opt/stage/libs/ojdbc8.jar found\n",
      "log4j /opt/stage/libs/log4j-1.2.17.jar found\n",
      "sqlserver /opt/stage/libs/mssql-jdbc-12.6.1.jre8.jar found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "open terminal failed: not a terminal\n",
      "/Users/robert.lee/.bashrc: line 2: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 3: compinit: command not found\n",
      "/Users/robert.lee/.bashrc: line 4: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 2: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 3: compinit: command not found\n",
      "/Users/robert.lee/.bashrc: line 4: autoload: command not found\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  396M  100  396M    0     0  4050k      0  0:01:40  0:01:40 --:--:-- 3590k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcion   downloaded\n",
      "checking jar(s) in /opt/stage/arcion/replicant-cli/lib for updates\n",
      "checking jar(s) in /opt/stage/arcion/replicant-cli-24.01.25.7/replicant-cli/lib for updates\n",
      "checking jar(s) in /opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/lib for updates\n",
      "'/opt/stage/libs//SparkJDBC42.jar' -> '/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/lib/./SparkJDBC42.jar'\n",
      "'/opt/stage/libs//ojdbc8.jar' -> '/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/lib/./ojdbc8.jar'\n",
      "'/opt/stage/libs//log4j-1.2.17.jar' -> '/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/lib/./log4j-1.2.17.jar'\n",
      "'/opt/stage/libs//DatabricksJDBC42.jar' -> '/opt/stage/arcion/replicant-cli-24.01.25.20/replicant-cli/lib/./DatabricksJDBC42.jar'\n",
      "Arcion license found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert.lee/.bashrc: line 2: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 3: compinit: command not found\n",
      "/Users/robert.lee/.bashrc: line 4: autoload: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YCSB  /opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT  found\n",
      "checking jar(s) in /opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT-old/lib for updates\n",
      "'/opt/stage/libs/log4j-1.2.17.jar' -> '/opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT-old/lib/./log4j-1.2.17.jar'\n",
      "'/opt/stage/libs/mssql-jdbc-12.6.1.jre8.jar' -> '/opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT-old/lib/./mssql-jdbc-12.6.1.jre8.jar'\n",
      "checking jar(s) in /opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT/lib for updates\n",
      "'/opt/stage/libs/log4j-1.2.17.jar' -> '/opt/stage/ycsb/ycsb-jdbc-binding-0.18.0-SNAPSHOT/lib/./log4j-1.2.17.jar'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "/Users/robert.lee/.bashrc: line 2: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 3: compinit: command not found\n",
      "/Users/robert.lee/.bashrc: line 4: autoload: command not found\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing apt-utils\n",
      "installing mssql-server\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "100   983  100   983    0     0   4275      0 --:--:-- --:--:-- --:--:--  4273\n",
      "curl: Failed writing body\n",
      "bin/install-sqlserver.sh: line 52: lsb_release: command not found\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "100   983  100   983    0     0   5555      0 --:--:-- --:--:-- --:--:--  5585\n",
      "curl: Failed writing body\n",
      "bin/install-sqlserver.sh: line 66: lsb_release: command not found\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "100    14  100    14    0     0     47      0 --:--:-- --:--:-- --:--:--    47\n",
      "curl: Failed writing body\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing mssql-tools18\n",
      "installing unixodbc-dev\n",
      "sqlserver start failed. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n",
      "sudo: unknown user mssql\n",
      "sudo: error initializing audit plugin sudoers_audit\n",
      "/Users/robert.lee/.bashrc: line 2: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 3: compinit: command not found\n",
      "/Users/robert.lee/.bashrc: line 4: autoload: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCION_BIN needs to point to replicant binary\n",
      "ARCION_BIN needs to point to replicant binary\n",
      "ARCION_BIN needs to point to replicant binary\n",
      "prometheus already downloaded\n",
      "prometheus node_exporter already downloaded\n",
      "prometheus sql_exporter being downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert.lee/.bashrc: line 2: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 3: compinit: command not found\n",
      "/Users/robert.lee/.bashrc: line 4: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 2: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 3: compinit: command not found\n",
      "/Users/robert.lee/.bashrc: line 4: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 2: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 3: compinit: command not found\n",
      "/Users/robert.lee/.bashrc: line 4: autoload: command not found\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 24.3M  100 24.3M    0     0  3612k      0  0:00:06  0:00:06 --:--:-- 4319k\n",
      "x sql_exporter-0.14.0.linux-amd64/\n",
      "x sql_exporter-0.14.0.linux-amd64/README.md\n",
      "x sql_exporter-0.14.0.linux-amd64/mssql_standard.collector.yml\n",
      "x sql_exporter-0.14.0.linux-amd64/sql_exporter\n",
      "x sql_exporter-0.14.0.linux-amd64/LICENSE\n",
      "x sql_exporter-0.14.0.linux-amd64/sql_exporter.yml\n",
      "bin/install-prometheus.sh: line 81: /var/tmp/arcsrc/sqlserver/logs/sql_exporter.log: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started /opt/stage/prom/sql_exporter-0.14.0.linux-amd64/sql_exporter.  log at /var/tmp/arcsrc/sqlserver/logs/sql_exporter.log\n",
      "started /opt/stage/prom/node_exporter-1.7.0.linux-amd64/node_exporter.  log at /var/tmp/arcsrc/sqlserver/logs/node_exporter.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bin/install-prometheus.sh: line 89: /var/tmp/arcsrc/sqlserver/logs/node_exporter.log: No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='export SRCDB_ARC_USER=arcsrc; bin/install-prometheus.sh', returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup tmux, arcion, ycsb\n",
    "subprocess.run(f\"\"\". ./bin/setup-tmux.sh; setup_tmux '{dbx_username.value}'\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/download-jars.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"ARCION_LICENSE='{arcion_license.value}' ARCION_DOWNLOAD_URL='{arcion_download_url.value}' bin/install-arcion.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"bin/install-ycsb.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "\n",
    "# mysql\n",
    "\n",
    "# pg\n",
    "\n",
    "\n",
    "# sqlserver\n",
    "subprocess.run(f\"\"\"export CONFIG_FILE=\"{CONFIG_FILE}\"; SQL_SERVER_DPKG='{pkg_src_dir.value}'; bin/install-sqlserver.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; export CONFIG_FILE=\"{CONFIG_FILE}\"; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; ping_sql_cli;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; export CONFIG_FILE=\"{CONFIG_FILE}\"; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; create_user;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; export CONFIG_FILE=\"{CONFIG_FILE}\"; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; MAX_SQLSERVER_RAM={ram_mb_sqlserver.value} set_sqlserver_ram '{dbx_username.value}';\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; export CONFIG_FILE=\"{CONFIG_FILE}\"; bin/install-prometheus.sh\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427dc8c9-96f3-4ac6-bf41-4efd371f3b04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Schema and Data\n",
    "\n",
    "Existing tables will be appended with additional rows if the `Fill Range` is the same.  \n",
    "Increase the `Table Count` to create additional tables.  \n",
    "\n",
    "The following options are available:\n",
    "- Table count (Table Cnt): The number of tables to create.  \n",
    "  - Table names are `ycsbdense`, `ycsbdense2`, `ycsbdense3`, ... and `ycssparse`, `ycsbdense2`, and `ycsbdense3` ...\n",
    "- Number of Fields (# of Fields): The number of fields per table.  \n",
    "  - The field names are `FIELD0`, `FIELD1`, `FIELD2`, ...\n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Field Length (Field Len): The length of random character data populated per field.  \n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Record Count (Rec Cnt): The number of records per table generated.\n",
    "  - Note the use of `K`,`M`,`B` ... suffix at the end.\n",
    "- Fill Range: The relative start and end range of fields that are populated with data.  Be default: \n",
    "    - sparse tables are all NULLs by having the fill range be 0% to 0% ranges\n",
    "    - dense tables have all fields populated by having the fill range be 0% to 100% of ranges \n",
    "\n",
    "```sql\n",
    "[localhost][arcsrc] 1> \\describe ycsbsparse\n",
    "+-------------+-------------+-----------+-------------+----------------+-------------+\n",
    "| TABLE_SCHEM | COLUMN_NAME | TYPE_NAME | COLUMN_SIZE | DECIMAL_DIGITS | IS_NULLABLE |\n",
    "+-------------+-------------+-----------+-------------+----------------+-------------+\n",
    "| dbo         | YCSB_KEY    | int       |          10 |              0 | NO          |\n",
    "| dbo         | FIELD0      | text      |  2147483647 |         [NULL] | YES         |\n",
    "| dbo         | FIELD1      | text      |  2147483647 |         [NULL] | YES         |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd09cab-c63e-4195-9364-09011fd2b559",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "Make changes below and click `Run All Below`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c315fc-624c-442c-b712-518336f5bfd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcb3df8f4f64d849169908ca48f59ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Sparse'), BoundedIntText(value=1, description='Tbl Start:', max=100…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show YCSB Data Controls\n",
    "VBox([HBox([Label('Sparse'), sparse_cntstart,sparse_cnt, sparse_fieldcount, sparse_fieldlength, sparse_recordcount, sparse_fillpct]),\n",
    "    HBox([Label('Dense'),  dense_cntstart, dense_cnt, dense_fieldcount, dense_fieldlength, dense_recordcount, dense_fillpct])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370b278c-17fe-4f68-ac8f-aa0ee8c18568",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7f7c0f-9f1d-40e3-a4f8-a02d6a42bf0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCION_BIN needs to point to replicant binary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert.lee/.bashrc: line 2: autoload: command not found\n",
      "/Users/robert.lee/.bashrc: line 3: compinit: command not found\n",
      "/Users/robert.lee/.bashrc: line 4: autoload: command not found\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/var/tmp/arcsrc/sqlserver/config/list_table_counts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m      2\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mexport SRCDB_ARC_USER=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_username\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    y_fieldcount=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_fieldcount\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m    list_table_counts\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,executable\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbash\u001b[39m\u001b[38;5;124m\"\u001b[39m,cwd\u001b[38;5;241m=\u001b[39mnotebookpath) \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# show tables\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/var/tmp/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msrc_username\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/sqlserver/config/list_table_counts.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfield count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/var/tmp/arcsrc/sqlserver/config/list_table_counts.csv'"
     ]
    }
   ],
   "source": [
    "# run load_sparse_data_cnt and load_dense_data_cnt \n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; \n",
    "    export CONFIG_FILE=\"{CONFIG_FILE}\";\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    y_fieldcount={sparse_fieldcount.value} \n",
    "    y_fieldlength={sparse_fieldlength.value}  \n",
    "    y_recordcount={sparse_recordcount.value} \n",
    "    y_fillstart={math.ceil((sparse_fillpct.value[0] * sparse_fieldcount.value) / 100)}      \n",
    "    y_fillend={int((sparse_fillpct.value[1] * sparse_fieldcount.value) / 100)}      \n",
    "    load_sparse_data_cnt {sparse_cnt.value} {sparse_cntstart.value};\n",
    "    y_fieldcount={dense_fieldcount.value} \n",
    "    y_fieldlength={dense_fieldlength.value} \n",
    "    y_recordcount={dense_recordcount.value} \n",
    "    y_fillstart={math.ceil((dense_fillpct.value[0] * dense_fieldcount.value) / 100)}      \n",
    "    y_fillend={int((dense_fillpct.value[1] * dense_fieldcount.value) / 100)}      \n",
    "    load_dense_data_cnt {dense_cnt.value} {dense_cntstart.value};\n",
    "    dump_schema;\n",
    "    list_table_counts\"\"\",\n",
    "    shell=True,executable=\"bash\",cwd=notebookpath) \n",
    "# show tables\n",
    "pd.read_csv (f\"/var/tmp/{src_username.value}/sqlserver/config/list_table_counts.csv\",header=None, names= ['table name','min key','max key','field count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1aeafcc-3471-47b9-b59d-b14016329940",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Workload\n",
    "\n",
    "Choose the options in the UI and run the cell below it to start the workload (YCSB).  \n",
    "\n",
    "YCSB update (workload A) controls for Dense and Sparse table groups separated. Each group has a separate control.  However, all of the tables in the group use the same controls.  \n",
    "1. Each table's TPS (throughput per second)\n",
    "   1. 0=fast as possible\n",
    "   2. 1=1 TPS\n",
    "   3. 10=10 TPS\n",
    "2. Each table's threads (concurrency) used to achieve the desired TPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e009b417-01dc-4855-b557-3a1cd88f70c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3233053-e7a3-4c56-8544-6d75c718f1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# show YCSB run controls\n",
    "VBox([\n",
    "      HBox([Label('Sparse'), sparse_threads,  sparse_multideletesize, sparse_multiupdatesize, sparse_multiinsertsize, ]), \n",
    "      HBox([Label('Dense'),  dense_threads, dense_multideletesize, dense_multiupdatesize, dense_multiinsertsize, ]),\n",
    "      HBox([Label('YCSB'), ram_percent_ycsb, ycsb_data_gen]),\n",
    "      ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1920d9d-bae0-49b0-adab-554352dcc7a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33058c18-c9e7-46f2-ab85-618136369430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# start/restart YCSB run\n",
    "# start the actual run\n",
    "subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value};\n",
    "    export CONFIG_FILE=\"{CONFIG_FILE}\";\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    kill_ycsb;\n",
    "    list_table_counts;       \n",
    "    y_data_type=\"{ycsb_data_gen.value}\"\n",
    "    y_threads_dense={dense_threads.value} \n",
    "    y_threads_sparse={sparse_threads.value}               \n",
    "    y_multiinsertsize_dense={dense_multiinsertsize.value} \n",
    "    y_multiupdatesize_dense={dense_multiupdatesize.value} \n",
    "    y_multideletesize_dense={dense_multideletesize.value} \n",
    "    y_multiinsertsize_sparse={sparse_multiinsertsize.value} \n",
    "    y_multiupdatesize_sparse={sparse_multiupdatesize.value} \n",
    "    y_multideletesize_sparse={sparse_multideletesize.value} \n",
    "    y_fieldlength_sparse={sparse_fieldlength.value} \n",
    "    y_fieldlength_dense={dense_fieldlength.value} \n",
    "    y_MinRAMPercentage={ram_percent_ycsb.value}.0\n",
    "    y_MaxRAMPercentage={ram_percent_ycsb.value}.0\n",
    "    start_ycsb;\"\"\",\n",
    "    shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6734eb-5342-48b5-868b-750218370b54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Arcion\n",
    "\n",
    "Choose the options in the UI and run the cell below it to start the replication.  \n",
    "\n",
    "The following control are avail in the demo.  \n",
    "- Arcion - replication type and CDC methods  \n",
    "- Threads - control the parallelism.\n",
    "- Target - null, unity catalog or delta lake\n",
    "\n",
    "NOTE: Full mode does not work at this time.\n",
    "\n",
    "For SQL Server, change tracking, cdc are available for demo.  \n",
    "\n",
    "Performance is mainly controlled by the thread count by the extract and apply process.\n",
    "Additional controls are customizable via modifying the YAML files directly below.\n",
    "- [CDC YAML files](./demo/sqlserver/yaml/cdc/)\n",
    "- [Change Tracking YAML files](./demo/sqlserver/yaml/change/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb753ed-5c15-4f74-9ed6-8ddf59a2ce42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "Make changes below and click `Run All Below`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d603b1d4-bb2c-418b-acfe-894441efe01e",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f71a480315465ca0dd955c60d1d4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='RAM'), BoundedIntText(value=10, description='RAM %:', max=80, min=1…"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show Arcion and DBX controls\n",
    "VBox([\n",
    "      HBox([Label('RAM'), ram_percent]),\n",
    "      HBox([Label('Modes'), repl_mode, cdc_mode, extraction_method]),\n",
    "      HBox([Label('Target'), dbx_destinations, dbx_staging ]),\n",
    "      HBox([Label('Threads'), snapshot_threads, realtime_threads, delta_threads]),\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbea2ac-b6ea-4cf4-9be1-bf665a7e6ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a0ede1-4383-4f3e-b9c8-5e601c35bfcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# start/restart Arcion\n",
    "\n",
    "if ( f\"{dbx_access_token.value}\" == \"\" ) and ( f\"{dbx_destinations.value}\" != \"null\" ):\n",
    "    print(\"personal access token not entered.\")\n",
    "else:\n",
    "    arcion_run_id=nine_char_id()\n",
    "    # start a new run\n",
    "    print (f\"\"\"{cdc_mode.value} {repl_mode.value}\"\"\")\n",
    "    arcion_run = subprocess.run(f\"\"\"export CONFIG_FILE=\"{CONFIG_FILE}\"; \n",
    "    export ARCION_DOWNLOAD_URL='{arcion_download_url.value}';        \n",
    "    export SRCDB_ARC_USER={src_username.value};\n",
    "    . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; \n",
    "    kill_arcion;\n",
    "    disable_cdc;\n",
    "    disable_change_tracking;\n",
    "    echo prog_dir=$PROG_DIR arcion_bin=$ARCION_BIN;\n",
    "    cd $PROG_DIR;\n",
    "    NINE_CHAR_ID='{arcion_run_id}'\n",
    "    a_repltype='{repl_mode.value}'\n",
    "    EXTRACTION_METHOD='{extraction_method.value}'\n",
    "    REPLICANT_MEMORY_PERCENTAGE='{ram_percent.value}.0'\n",
    "    SRCDB_SNAPSHOT_THREADS='{snapshot_threads.value}' \n",
    "    SRCDB_REALTIME_THREADS='{realtime_threads.value}' \n",
    "    SRCDB_DELTA='{delta_threads.value}'\n",
    "    DSTDB_TYPE='{dbx_destinations.value}'\n",
    "    DSTDB_STAGE='{dbx_staging.value}'\n",
    "    DBX_SPARK_URL='{dbx_spark_url.value}'\n",
    "    DBX_DATABRICKS_URL='{dbx_databricks_url.value}'\n",
    "    DBX_ACCESS_TOKEN='{dbx_access_token.value}'\n",
    "    DBX_HOSTNAME='{dbx_hostname.value}'\n",
    "    DBX_DBFS_ROOT='/{dbx_username.value}'\n",
    "    DBX_USERNAME='{dbx_username.value}'\n",
    "    start_{cdc_mode.value}_arcion;\"\"\",\n",
    "    shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cafb49c3-df1b-4458-805c-b233876acf90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# MLFLow\n",
    "\n",
    "Save the artifacts in MLFlow.\n",
    "\n",
    "Artifacts are collected for 5 min (600 sec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f193b18-1efa-46be-afb3-37510d683db0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb57b96-e39c-465e-a939-2ec7aa27392a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_exp_params=exp_params()\n",
    "if not ('exp_params' in mlflow_proc_state):\n",
    "    print(\"first run of mlflow\")\n",
    "    register_mlflow(current_exp_params)\n",
    "elif current_exp_params != mlflow_proc_state['exp_params']:\n",
    "    print(\"param changed. starting new mlflow\")\n",
    "    register_mlflow(current_exp_params)\n",
    "elif not(mlflow_proc_state['proc'].is_alive()):\n",
    "    print(\"mlflow stopped. starting new mlflow with new step\")\n",
    "    register_mlflow(current_exp_params)\n",
    "else:\n",
    "    print(\"no parameters changed. New MLFLow experiment not needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7184a016-b7ef-4302-bee5-38ec30ed02f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Manually Kill Processes\n",
    "Uncomment below to kill desired processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79738332-5dec-4a1f-a0af-b2d9fc442150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# subprocess.run(f\"\"\". ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_arcion;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "# subprocess.run(f\"\"\". ./demo/sqlserver/run-ycsb-sqlserver-source.sh; kill_ycsb;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)\n",
    "# subprocess.run(f\"\"\"export SRCDB_ARC_USER={src_username.value}; . ./demo/sqlserver/run-ycsb-sqlserver-source.sh; drop_all_ycsb_tables;\"\"\",shell=True,executable=\"bash\",cwd=notebookpath)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Arcion License",
      "width": 184
     }
    ]
   },
   "notebookName": "sqlserver-arcion-dbx",
   "widgets": {
    "Access Token": {
     "currentValue": "dapi29ffee1b82da9b7ba3e11193abd842bf",
     "nuid": "a3fdf5f2-8290-4a28-b8ac-89bea1252303",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Access Token",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Arcion License": {
     "currentValue": "ewogICJsaWNlbnNlIiA6IHsKICAgICJ1dWlkIiA6ICI2MWQ0N2Y4Yi0zYmI4LTRhOTktYTAwYS1mMjA2OGEzMWM0MmYiLAogICAgIm93bmVyIiA6ICJEQiBTdXBwb3J0IFRyYWluaW5nIiwKICAgICJjcmVhdGVkIiA6ICIyMDIzLTEyLTA2VDAwOjAwWiIsCiAgICAiZXhwaXJlcyIgOiAiMjAyNC0wNi0wNlQwMDowMFoiLAogICAgInR5cGUiIDogIk9GRkxJTkUiLAogICAgImVkaXRpb24iIDogIkVOVEVSUFJJU0UiLAogICAgInNyYyIgOiBbICJBTEwiIF0sCiAgICAiZHN0IiA6IFsgIkFMTCIgXQogIH0sCiAgImtleSIgOiAiZXlKMWRXbGtJam9pTmpGa05EZG1PR0l0TTJKaU9DMDBZVGs1TFdFd01HRXRaakl3TmpoaE16RmpOREptSWl3aWIzZHVaWElpT2lKRVFpQlRkWEJ3YjNKMElGUnlZV2x1YVc1bklpd2lZM0psWVhSbFpDSTZJakl3TWpNdE1USXRNRFpVTURBNk1EQmFJaXdpWlhod2FYSmxjeUk2SWpJd01qUXRNRFl0TURaVU1EQTZNREJhSWl3aWRIbHdaU0k2SWs5R1JreEpUa1VpTENKbFpHbDBhVzl1SWpvaVJVNVVSVkpRVWtsVFJTSXNJbk55WXlJNld5SkJURXdpWFN3aVpITjBJanBiSWtGTVRDSmRmUT09Lk1HWUNNUUQ1SHVybWJrRm1ucURpR3lmY0J2dzVWQ2t5amJsTjFOM1pMc2pqdmlrRVdjOVVPZzZ3OEhUdHNCdmFFOWVBWjV3Q01RQ1lSNEtNTzZfYXhjMlF6RGZyTS1lenotTWFpcngyRnV3eEhSWkxQdjJzUEdaOVJ5UUl5cnpobldYZVRnVE1PdFk9Igp9",
     "nuid": "28b8555f-62d1-4e3f-bd83-80316798ef69",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Arcion License",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Config": {
     "currentValue": "",
     "nuid": "7d766068-6c1e-40d1-8af7-448fb3e04c1d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "Config",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Config",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Experiment ID": {
     "currentValue": "",
     "nuid": "730f4957-14a3-4127-bdb7-916f53691f22",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Experiment ID",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Package Source Dir": {
     "currentValue": "",
     "nuid": "f5e384ea-c669-45a4-9240-046fec8728e9",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "Package Source Dir",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
